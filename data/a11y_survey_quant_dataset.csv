DOI,venue,year,title,subtitle,abstract,authors,keywords,page_from,page_to
10.1145/2858036.2858245,CHI,2016,Linespace,A Sensemaking Platform for the Blind,"For visually impaired users, making sense of spatial information is difficult as they have to scan and memorize content before being able to analyze it. Even worse, any update to the displayed content invalidates their spatial memory, which can force them to manually rescan the entire display. Making display contents persist, we argue, is thus the highest priority in designing a sensemaking system for the visually impaired. We present a tactile display system designed with this goal in mind. The foundation of our system is a large tactile display (140x100cm, 23x larger than Hyperbraille), which we achieve by using a 3D printer to print raised lines of filament. The system's software then trades in this space in order to minimize screen updates. Instead of panning and zooming, for example, our system creates additional views, leaving display contents intact and thus supporting users in preserving their spatial memory. We illustrate our system and its design principles at the example of four spatial applications. We evaluated our system with six blind users. Participants responded favorably to the system and expressed, for example, that having multiple views at the same time was helpful. They also judged the increased expressiveness of lines over the more traditional dots as useful for encoding information.","Saiganesh Swaminathan, Thijs Roumen, Robert Kovacs, David Stangl, Stefanie Mueller, Patrick Baudisch","3D printing, accessibility",2175,2185
10.1145/3290605.3300544,CHI,2019,&#34;Occupational Therapy is Making&#34;,Clinical Rapid Prototyping and Digital Fabrication,"Consumer-fabrication technologies potentially improve the effectiveness and adoption of assistive technology (AT) by engaging AT users in AT creation. However, little is known about the role of clinicians in this revolution. We investigate clinical AT fabrication by working as expert fabricators for clinicians over a four-month period. We observed and co-designed AT with four occupational therapists at two clinics: a free clinic for uninsured clients, and a Veteran's Affairs Hospital. We find that existing fabrication processes, particularly with respect to rapid prototyping, do not align with clinical practice and its<i>do-no-harm</i> ethos. We recommend software solutions that would integrate into client care by: amplifying clinicians' expertise, revealing appropriate fabrication opportunities, and supporting adaptable fabrication.","Megan Hofmann, Kristin Williams, Toni Kaplan, Stephanie Valencia, Gabriella Hann, Scott Hudson, Jennifer Mankoff, Patrick Carrington","3d printing, adaptive design, digital fabrication, occupational therapy, rapid prototyping",1,13
10.1145/3290605.3300759,CHI,2019,Understanding and Designing for Deaf or Hard of Hearing Drivers on Uber, ,"We used content analysis of in-app driver survey responses, customer support tickets, and tweets, and face-to-face interviews of DHH Uber drivers to better understand the DHH driver experience. Here we describe challenges DHH drivers experience and how they address those difficulties via Uber's accessibility features and their own workarounds. We also identify and discuss design and product opportunities to improve the DHH driver experience on Uber.","Sooyeon Lee, Bjorn Hubert-Wallander, Molly Stevens, John Carroll","accessibility, communication, deaf or hard of hearing drivers, uber",1,12
10.1145/2470654.2481365,CHI,2013,Understanding the conflicting demands of family caregivers caring for depressed family members, ,"Depression is one of the most common disabilities in developed countries. Despite its often devastating impact on families, scant research has focused on how to facilitate the well-being of family caregivers. The aim of this paper is to uncover the challenges faced by family caregivers and support their well-being with the use of technologies. To understand the emotional and social burden of caregivers and how they handle their stress, we conducted in-depth interviews with 15 individuals who have cared for a depressed family member. Our findings reveal the multifaceted dilemma of caring for a depressed family member as well as the various strategies engaged in by caregivers to improve their own situations. Based on our findings, we suggest design implications for healthcare technologies to improve the wellness of caregivers who are looking after depressed family members.","Naomi Yamashita, Hideaki Kuzuoka, Keiji Hirata, Takashi Kudo","depression, family caregivers, healthcare technology, stress",2637,2646
10.1145/3173574.3173845,CHI,2018,Smart Kitchens for People with Cognitive Impairments,A Qualitative Study of Design Requirements,"Individuals with cognitive impairments currently leverage extensive human resources during their transitions from assisted living to independent living. In Western Europe, many government-supported volunteer organizations provide sheltered living facilities; supervised environments in which people with cognitive impairments collaboratively learn daily living skills. In this paper, we describe communal cooking practices in sheltered living facilities and identify opportunities for supporting these with interactive technology to reduce volunteer workload. We conducted two contextual observations of twelve people with cognitive impairments cooking in sheltered living facilities and supplemented this data through interviews with four employees and volunteers who supervise them. Through thematic analysis, we identified four themes to inform design requirements for communal cooking activities: Work organization, community, supervision, and practicalities. Based on these, we present five design implications for assistive systems in kitchens for people with cognitive deficiencies.","Thomas Kosch, Pawe&#322; Wo&#378;niak, Erin Brady, Albrecht Schmidt","accessibility, assistive systems, people with cognitive impairments, sheltered housing, smart kitchens",1,12
10.1145/3173574.3173772,CHI,2018,Accessible Maps for the Blind,Comparing 3D Printed Models with Tactile Graphics,"Tactile maps are widely used in Orientation and Mobility (O&#38;M) training for people with blindness and severe vision impairment. Commodity 3D printers now offer an alternative way to present accessible graphics, however it is unclear if 3D models offer advantages over tactile equivalents for 2D graphics such as maps. In a controlled study with 16 touch readers, we found that 3D models were preferred, enabled the use of more easily understood icons, facilitated better short term recall and allowed relative height of map elements to be more easily understood. Analysis of hand movements revealed the use of novel strategies for systematic scanning of the 3D model and gaining an overview of the map. Finally, we explored how 3D printed maps can be augmented with interactive audio labels, replacing less practical braille labels. Our findings suggest that 3D printed maps do indeed offer advantages for O&#38;M training.","Leona Holloway, Kim Marriott, Matthew Butler","3d printing, accessibility, blindness, mapping, orientation and mobility training, vision impairment",1,13
10.1145/3290605.3300757,CHI,2019,#HandsOffMyADA,A Twitter Response to the ADA Education and Reform Act,"Twitter continues to be used increasingly for communication related advocacy, activism, and social change. This is also the case for the disability community. In light of the recently proposed ADA Education and Reform in the United States, we investigate factors for effectiveness of sharing or retweeting messages about topics affecting the rights of people with disabilities. We perform a multifaceted study of the #HandsOffMyADA campaign against the proposed H.R.620 bill to: (1) explore how communication via Twitter compares to previous disability rights movements; (2) characterize the campaign in terms of hashtags, user groups, and content such as accessible multimedia that contribute to dissemination of campaign messages; (3) identify major themes in tweets and responses, and their variation among user groups; and (4) understand how the disability community mobilized for this campaign compared to previous Twitter initiatives.","Brooke Auxier, Cody Buntain, Paul Jaeger, Jennifer Golbeck, Hernisa Kacorri","accessibility, activism, disability rights, social media",1,12
10.1145/1978942.1979302,CHI,2011,CHANTI,predictive text entry using non-verbal vocal input,"This paper introduces a text entry application for users with physical disabilities who cannot utilize a manual keyboard. The system allows the user to enter text hands-free, with the help of ""Non-verbal Vocal Input"" (e.g., humming or whistling). To keep the number of input sounds small, an ambiguous keyboard is used. As the user makes a sequence of sounds, each representing a subset of the alphabet, the program searches for matches in a dictionary. As a model for the system, the scanning-based application QANTI was redesigned and adapted to accept the alternative input signals. The usability of the software was investigated in an international longitudinal study done at locations in the Czech Republic, Germany, and the United States. Eight test users were recruited from the target community. The users differed in the level of speech impairment. Three users did not complete the study due to the severity of their impairment. By the end of the experiment, the users were able to enter text at rates between 10 and 15 characters per minute.","Adam Sporka, Torsten Felzer, Sri Kurniawan, Ond&#345;ej Pol&#225;&#269;ek, Paul Haiduk, I. MacKenzie","ambiguous keyboard, assistive technology, human-computer interaction, non-verbal voice input, predictive text entry, scanning, user study",2463,2472
10.1145/2207676.2207734,CHI,2012,SpaceSense,representing geographical information to visually impaired people using spatial tactile feedback,"Learning an environment can be challenging for people with visual impairments. Braille maps allow their users to understand the spatial relationship between a set of places. However, physical Braille maps are often costly, may not always cover an area of interest with sufficient detail, and might not present up-to-date information. We built a handheld system for representing geographical information called SpaceSense, which includes custom spatial tactile feedback hardware-multiple vibration motors attached to different locations on a mobile touch-screen device. It offers high-level information about the distance and direction towards a destination and bookmarked places through vibrotactile feedback to help the user maintain the spatial relationships between these points. SpaceSense also adapts a summarization technique for online user reviews of public and commercial venues. Our user study shows that participants could build and maintain the spatial relationships between places on a map more accurately with SpaceSense compared to a system without spatial tactile feedback. They pointed specifically to having spatial tactile feedback as the contributing factor in successfully building and maintaining their mental map.","Koji Yatani, Nikola Banovic, Khai Truong","assistive technology, geographical information representation, handheld devices, touch screens, users with visual impairments, vibrotactile feedback",415,424
10.1145/2858036.2858517,CHI,2016,AugKey,Increasing Foveal Throughput in Eye Typing with Augmented Keys,"Eye-typing is an important tool for people with physical disabilities and, for some, it is their main form of communication. By observing expert typists using physical keyboards, we notice that visual throughput is considerably reduced in current eye-typing solutions. We propose AugKey to improve throughput by augmenting keys with a prefix, to allow continuous text inspection, and suffixes to speed up typing with word prediction. AugKey limits the visual information to the foveal region to minimize eye movements (i.e., reduce eye work). We have applied AugKey to a dwell-time keyboard and compared its performance with two conditions with no augmented feedback: a keyboard with and one without word prediction. Results show that AugKey can be about 28% faster than no word prediction and 20% faster than traditional word prediction, with a smaller workload index.","Antonio Diaz-Tula, Carlos Morimoto","augmented feedback, eye typing, text-entry speed, user experience, word prediction",3533,3544
10.1145/2702123.2702591,CHI,2015,Tongue-in-Cheek,Using Wireless Signals to Enable Non-Intrusive and Flexible Facial Gestures Detection,"Serious brain injuries, spinal injuries, and motor neuron diseases often lead to severe paralysis. Individuals with such disabilities can benefit from interaction techniques that enable them to interact with the devices and thereby the world around them. While a number of systems have proposed tongue-based gesture detection systems, most of these systems require intrusive instrumentation of the user's body (e.g., tongue piercing, dental retainers, multiple electrodes on chin). In this paper, we propose a wireless, non-intrusive and non-contact facial gesture detection system using X-band Doppler. The system can accurately differentiate between 8 different facial gestures through non-contact sensing, with an average accuracy of 94.3%.","Mayank Goel, Chen Zhao, Ruth Vinisha, Shwetak Patel","accessibility, tongue gestures, tongue-computer interface, wireless signals",255,258
10.1145/3173574.3173867,CHI,2018,SpeechBubbles,Enhancing Captioning Experiences for Deaf and Hard-of-Hearing People in Group Conversations,"Deaf and hard-of-hearing (DHH) individuals encounter difficulties when engaged in group conversations with hearing individuals, due to factors such as simultaneous utterances from multiple speakers and speakers whom may be potentially out of view. We interviewed and co-designed with eight DHH participants to address the following challenges: 1) associating utterances with speakers, 2) ordering utterances from different speakers, 3) displaying optimal content length, and 4) visualizing utterances from out-of-view speakers. We evaluated multiple designs for each of the four challenges through a user study with twelve DHH participants. Our study results showed that participants significantly preferred speechbubble visualizations over traditional captions. These design preferences guided our development of SpeechBubbles, a real-time speech recognition interface prototype on an augmented reality head-mounted display. From our evaluations, we further demonstrated that DHH participants preferred our prototype over traditional captions for group conversations.","Yi-Hao Peng, Ming-Wei Hsi, Paul Taele, Ting-Yu Lin, Po-En Lai, Leon Hsu, Tzu-chuan Chen, Te-Yen Wu, Yu-An Chen, Hsien-Hui Tang, Mike Chen","accessibility, augmented reality, closed captions, deaf and hard of hearing, hololens, text bubbles, word balloons",1,10
10.1145/3290605.3300262,CHI,2019,Design Goals for Playful Technology to Support Physical Activity Among Wheelchair Users, ,"Playful technology has the potential to support physical activity (PA) among wheelchair users, but little is known about design considerations for this audience, who experience significant access barriers. In this paper, we lever-age the Integrated Behavioural Model (IBM) to understand wheelchair users' perspectives on PA, technology, and play.First, we present findings from an interview study with eight physically active wheelchair users. Second, we build on the interviews in a survey that received 44 responses from a broader group of wheelchair users. Results show that the anticipation of positive experiences was the strongest predictor of engagement with PA, and that accessibility concerns act as barriers both in terms of PA participation and technology use. We present four design goals - emphasizing enjoyment,involving others, building knowledge and enabling flexibility - to make our findings actionable for researchers and designers wishing to create accessible playful technology to support PA.","Liam Mason, Kathrin Gerling, Patrick Dickinson, Antonella De Angeli","accessibility, games, integrated behavioral model, wheelchair",1,12
10.1145/3025453.3025888,CHI,2017,ForgetMeNot,Active Reminder Entry Support for Adults with Acquired Brain Injury,"Smartphone reminding apps can compensate for memory impairment after acquired brain injury (ABI). In the absence of a caregiver, users must enter reminders themselves if the apps are going to help them. Poor memory and apathy associated with ABI can result in failure to initiate such configuration behaviour and the benefits of reminder apps are lost. ForgetMeNot takes a novel approach to address this problem by periodically encouraging the user to enter reminders with unsolicited prompts (UPs). An in situ case study investigated the experience of using a reminding app for people with ABI and tested UPs as a potential solution to initiating reminder entry. Three people with severe ABI living in a post-acute rehabilitation hospital used the app in their everyday lives for four weeks to collect real usage data. Field observations illustrated how difficulties with motivation, insight into memory difficulties and anxiety impact reminder app use in a rehabilitation setting. Results showed that when 6 UPs were presented throughout the day, reminder-setting increased, showing UPs are an important addition to reminder applications for people with ABI. This study demonstrates that barriers to technology use can be resolved in practice when software is developed with an understanding of the issues experienced by the user group.","Matthew Jamieson, Brian O'Neill, Breda Cullen, Marilyn Lennon, Stephen Brewster, Jonathan Evans","acquired brain injury, assistive technology, field study, in situ study, memory rehabilitation, smartphone reminding",6012,6023
10.1145/3173574.3174062,CHI,2018,Exploring Accessible Smartwatch Interactions for People with Upper Body Motor Impairments, ,"Smartwatches are always-available, provide quick access to information in a mobile setting, and can collect continuous health and fitness data. However, the small interaction space of these wearables may pose challenges for people with upper body motor impairments. To investigate accessible smartwatch interactions for this user group, we conducted two studies. First, we assessed the accessibility of existing smartwatch gestures with 10 participants with motor impairments. We found that not all participants were able to complete button, swipe and tap interactions. In a second study, we adopted a participatory approach to explore smartwatch gesture preferences and to gain insight into alternative, more accessible smartwatch interaction techniques. Eleven participants with motor impairments created gestures for 16 common smartwatch actions on both touchscreen and non-touchscreen (bezel, wristband) areas of the watch and the user's body. We present results from both studies and provide design recommendations.","Meethu Malu, Pramod Chundury, Leah Findlater","accessibility, elicitation study, interactions, motor impairments, smartwatches, wearables",1,12
10.1145/2858036.2858567,CHI,2016,Universal Design Ballot Interfaces on Voting Performance and Satisfaction of Voters with and without Vision Loss, ,"Voting is a glocalized event across countries, states and municipalities in which individuals of all abilities want to participate. To enable people with disabilities to participate accessible voting is typically implemented by adding assistive technologies to electronic voting machines to accommodate people with disabilities. To overcome the complexities and inequities in this practice, two interfaces, EZ Ballot, which uses a linear yes/no input system for all selections, and QUICK Ballot, which provides random access voting through direct selection, were designed to provide one system for all voters. This paper reports efficacy testing of both interfaces. The study demonstrated that voters with a range of visual abilities were able to use both ballots independently. While non-sighted voters made fewer errors on the linear ballot (EZ Ballot), partially-sighted and sighted voters completed the random access ballot (QUICK Ballot) in less time. In addition, a higher percentage of non-sighted participants preferred the linear ballot, and a higher percentage of sighted participants preferred the random ballot.","Seunghyun ""Tina"" Lee, Yilin Liu, Ljilja Ruzic, Jon Sanford","effectiveness, efficiency, subjective usability, universal design, voting",4861,4871
10.1145/3025453.3025814,CHI,2017,Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images, ,"Research advancements allow computational systems to automatically caption social media images. Often, these captions are evaluated with sighted humans using the image as a reference. Here, we explore how blind and visually impaired people experience these captions in two studies about social media images. Using a contextual inquiry approach (n=6 blind/visually impaired), we found that blind people place a lot of trust in automatically generated captions, filling in details to resolve differences between an image's context and an incongruent caption. We built on this in-person study with a second, larger online experiment (n=100 blind/visually impaired) to investigate the role of phrasing in encouraging trust or skepticism in captions. We found that captions emphasizing the probability of error, rather than correctness, encouraged people to attribute incongruence to an incorrect caption, rather than missing details. Where existing research has focused on encouraging trust in intelligent systems, we conclude by challenging this assumption and consider the benefits of encouraging appropriate skepticism.","Haley MacLeod, Cynthia Bennett, Meredith Morris, Edward Cutrell","accessibility, alt text, automatic image captioning, blindness, social media, twitter",5988,5999
10.1145/3290605.3300762,CHI,2019,Personalising the TV Experience using Augmented Reality,An Exploratory Study on Delivering Synchronised Sign Language Interpretation,"Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme - one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a 'half-body' version and a 'full-body' version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the 'half-body' version. We discuss our participants reasoning behind their preferences and implications for future research.","Vinoba Vinayagamoorthy, Maxine Glancy, Christoph Ziegler, Richard Sch&#228;ffer","accessibility, augmented reality, bsl, companion screen, connected experiences, dgs, hbbtv 2.0, hololens, interaction techniques, personalisation, second screen, sign language, sse, synchronisation, television",1,12
10.1145/3025453.3025846,CHI,2017,Interaction Proxies for Runtime Repair and Enhancement of Mobile Application Accessibility, ,"We introduce interaction proxies as a strategy for runtime repair and enhancement of the accessibility of mobile applications. Conceptually, interaction proxies are inserted between an application's original interface and the manifest interface that a person uses to perceive and manipulate the application. This strategy allows third-party developers and researchers to modify an interaction without an application's source code, without rooting the phone, without otherwise modifying an application, while retaining all capabilities of the system (e.g., Android's full implementation of the TalkBack screen reader). This paper introduces interaction proxies, defines a design space of interaction re-mappings, identifies necessary implementation abstractions, presents details of implementing those abstractions in Android, and demonstrates a set of Android implementations of interaction proxies from throughout our design space. We then present a set of interviews with blind and low-vision people interacting with our prototype interaction proxies, using these interviews to explore the seamlessness of interaction, the perceived usefulness and potential of interaction proxies, and visions of how such enhancements could gain broad usage. By allowing third-party developers and researchers to improve an interaction, interaction proxies offer a new approach to personalizing mobile application accessibility and a new approach to catalyzing development, deployment, and evaluation of mobile accessibility enhancements.","Xiaoyi Zhang, Anne Ross, Anat Caspi, James Fogarty, Jacob Wobbrock","accessibility, interaction proxies, runtime modification",6024,6037
10.1145/2556288.2557124,CHI,2014,Interface design for older adults with varying cultural attitudes toward uncertainty, ,"This work reports on the design and evaluation of culturally appropriate technology for older adults. Our design context was Cognitive Testing on a Computer (C-TOC): a self-administered computerized test under development, intended to screen older adults for cognitive impairments. Using theory triangulation of cultural attitudes toward uncertainty, we designed two interfaces (one minimal and one rich) for one C-TOC subtest and hypothesized they would be culturally appropriate for older adult Caucasians and East Asians respectively. We ran an experiment with 36 participants to investigate cultural differences in performance, preference and anxiety. We found that Caucasians preferred the interface with minimal elements (i.e. those essential for the primary task) or had no preference. By contrast, East Asians preferred the rich interface augmented with security and learning support and felt less anxious with it than the minimal.","Shathel Haddad, Joanna McGrenere, Claudia Jacova","computerized cognitive assessment, cultural design, experiment, older adults, uncertainty avoidance",1913,1922
10.1145/2470654.2481292,CHI,2013,Accessible photo album,enhancing the photo sharing experience for people with visual impairment,"While a photograph is a visual artifact, studies reveal that a number of people with visual impairments are also interested in being able to share their memories and experiences with their sighted counterparts in the form of a photograph. We conducted an online survey to better understand the challenges faced by people with visual impairments in sharing and organizing photos, and reviewed existing tools and their limitations. Based on our analysis, we developed an accessible mobile application that enables a visually impaired user to capture photos along with audio recordings for the ambient sound and memo description and to browse through them eyes-free. Five visually impaired participants took part in a study in which they used our app to take photographs in naturalistic settings and to share them later with a sighted viewer. The participants were able to use our app to identify each photograph on their own during the photo sharing session, and reported high satisfaction in having been able to take the initiative during the process.","Susumu Harada, Daisuke Sato, Dustin Adams, Sri Kurniawan, Hironobu Takagi, Chieko Asakawa","audiophotography, blind, photo sharing, visual impairment",2127,2136
10.1145/2207676.2208385,CHI,2012,The SoundsRight CAPTCHA,an improved approach to audio human interaction proofs for blind users,"In this paper we describe the development of a new audio CAPTCHA called the SoundsRight CAPTCHA, and the evaluation of the CAPTCHA with 20 blind users. Blind users cannot use visual CAPTCHAs, and it has been documented in the research literature that the existing audio CAPTCHAs have task success rates below 50% for blind users. The SoundsRight audio CAPTCHA presents a real-time audio-based challenge in which the user is asked to identify a specific sound (for example the sound of a bell or a piano) each time it occurs in a series of 10 sounds that are played through the computer's audio system. Evaluation results from three rounds of usability testing document that the task success rate was higher than 90% for blind users. Discussion, limitations, and suggestions for future research are also presented.","Jonathan Lazar, Jinjuan Feng, Tim Brooks, Genna Melamed, Brian Wentz, Jon Holman, Abiodun Olalere, Nnanna Ekedebe","accessibility, audio CAPTCHA, blind users, evaluation, human interaction proof, security, web accessibility",2267,2276
10.1145/3173574.3173920,CHI,2018,&#8220;Bursting the Assistance Bubble&#8221;,Designing Inclusive Technology with Children with Mixed Visual Abilities,"Children living with visual impairments (VIs) are increasingly educated in mainstream rather than special schools. But knowledge about the challenges they face in inclusive schooling environments and how to design technology to overcome them remains scarce. We report findings from a field study involving interviews and observations of educators and children with/without VIs in mainstream schools, in which we identified the ""teaching assistant bubble"" as a potential barrier to group learning, social play and independent mobility. We present co-design activities blending elements of future workshops, multisensory crafting, fictional inquiry and bodystorming, demonstrating that children with and without VIs can jointly lead design processes and explore design spaces reflective of mixed visual abilities and shared experiences. We extend previous research by characterising challenges and opportunities for improving inclusive education of children with VIs in mainstream schools, in terms of balancing assistance and independence, and reflect on the process and outcomes of co-designing with mixed-ability groups in this context.","Oussama Metatla, Clare Cullen","children, co-design, education, inclusion, mixed abilities",1,14
10.1145/3290605.3300605,CHI,2019,Examining Augmented Virtuality Impairment Simulation for Mobile App Accessibility Design, ,"With mobile apps rapidly permeating all aspects of daily living with use by all segments of the population, it is crucial to support the evaluation of app usability for specific impaired users to improve app accessibility. In this work, we examine the effects of using our augmented virtuality impairment simulation system--Empath-D--to support experienced designer-developers to redesign a mockup of commonly used mobile application for cataract-impaired users, comparing this with existing tools that aid designing for accessibility. We show that the use of augmented virtuality for assessing usability supports enhanced usability challenge identification, finding more defects and doing so more accurately than with existing methods. Through our user interviews, we also show that augmented virtuality impairment simulation supports realistic interaction and evaluation to provide a concrete understanding over the usability challenges that impaired users face, and complements the existing guidelines-based approaches meant for general accessibility.","Kenny Choo, Rajesh Balan, Youngki Lee","accessibility, augmented virtuality, empathetic design, mobile app design, virtual reality",1,11
10.1145/2207676.2208652,CHI,2012,Design of an exergaming station for children with cerebral palsy, ,"We report on the design of a novel station supporting the play of exercise video games (exergames) by children with cerebral palsy (CP). The station combines a physical platform allowing children with CP to provide pedaling input into a game, a standard Xbox 360 controller, and algorithms for interpreting the cycling input to improve smoothness and accuracy of gameplay. The station was designed through an iterative and incremental participatory design process involving medical professionals, game designers, computer scientists, kinesiologists, physical therapists, and eight children with CP. It has been tested through observation of its use, through gathering opinions from the children, and through small experimental studies. With our initial design, only three of eight children were capable of playing a cycling-based game; with the final design, seven of eight could cycle effectively, and six reached energy expenditure levels recommended by the American College of Sports Medicine while pedaling unassisted.","Hamilton Hernandez, T.C. Graham, Darcy Fehlings, Lauren Switzer, Zi Ye, Quentin Bellay, Md Ameer Hamza, Cheryl Savery, Tadeusz Stach","accessibility, exergame, exergaming station, exertion interface, video game design",2619,2628
10.1145/3290605.3300861,CHI,2019,Let's Play Together,Adaptation Guidelines of Board Games for Players with Visual Impairment,"Board games present accessibility barriers for players with visual impairment since they often employ visuals alone to communicate gameplay information. Our research focuses on board game accessibility for those with visual impairment. This paper describes a three-phase study conducted to develop board game accessibility adaptation guidelines. These guidelines were developed through a user-centered design approach that included in-depth interviews and a series of user studies using two adapted board games. Our findings indicate that participants with and without visual impairment were able to play the adapted games, exhibiting a balanced experience whereby participants had complete autonomy and were provided with equal chances of victory. Our paper also contributes to the game and accessibility communities through the development of adaptation guidelines that allow board games to become inclusive irrespective of a player's visual impairment.","Frederico da Rocha Tom&#233; Filho, Pejman Mirza-Babaei, Bill Kapralos, Glaudiney Moreira Mendon&#231;a Junior","accessibility, board games, guidelines, visual impairment",1,15
10.1145/3290605.3300341,CHI,2019,SeeingVR,A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision,"Current virtual reality applications do not support people who have low vision, i.e., vision loss that falls short of complete blindness but is not correctable by glasses. We present SeeingVR, a set of 14 tools that enhance a VR application for people with low vision by providing visual and audio augmentations. A user can select, adjust, and combine different tools based on their preferences. Nine of our tools modify an existing VR application post hoc via a plugin without developer effort. The rest require simple inputs from developers using a Unity toolkit we created that allows integrating all 14 of our low vision support tools during development. Our evaluation with 11 participants with low vision showed that SeeingVR enabled users to better enjoy VR and complete tasks more quickly and accurately. Developers also found our Unity toolkit easy and convenient to use.","Yuhang Zhao, Edward Cutrell, Christian Holz, Meredith Morris, Eyal Ofek, Andrew Wilson","accessibility, low vision, unity, virtual reality",1,14
10.1145/2207676.2207736,CHI,2012,Guidelines are only half of the story,accessibility problems encountered by blind users on the web,"This paper describes an empirical study of the problems encountered by 32 blind users on the Web. Task-based user evaluations were undertaken on 16 websites, yielding 1383 instances of user problems. The results showed that only 50.4% of the problems encountered by users were covered by Success Criteria in the Web Content Accessibility Guidelines 2.0 (WCAG 2.0). For user problems that were covered by WCAG 2.0, 16.7% of websites implemented techniques recommended in WCAG 2.0 but the techniques did not solve the problems. These results show that few developers are implementing the current version of WCAG, and even when the guidelines are implemented on websites there is little indication that people with disabilities will encounter fewer problems. The paper closes by discussing the implications of this study for future research and practice. In particular, it discusses the need to move away from a problem-based approach towards a design principle approach for web accessibility.","Christopher Power, Andr&#233; Freire, Helen Petrie, David Swallow","accessibility guidelines, blind users, user evaluation, web accessibility",433,442
10.1145/2702123.2702588,CHI,2015,CoFa&#231;ade,A Customizable Assistive Approach for Elders and Their Helpers,"We present CoFa&#231;ade, a novel approach to helping elders reach their goals with IT products by working collaboratively with helpers. In this approach, the elder uses an interface with a small number of triggers, where each trigger is a single button (or card) that can execute a procedure. The helper uses a customization interface to link triggers to procedures that accomplish frequently-recurring high-level goals with IT products. Customization can be done either locally or remotely. We conducted an experiment to compare the CoFa&#231;ade approach with a baseline approach where helpers taught elders to perform IT tasks. Our results showed that CoFa&#231;ade can reduce helpers' time and effort, reduce elders' frustration, and improve elders' success rate in completing IT tasks.","Jason Chen Zhao, Richard Davis, Pin Sym Foong, Shengdong Zhao","accessibility design, cofa-ade, silver computing",1583,1592
10.1145/2858036.2858157,CHI,2016,Changing Family Practices with Assistive Technology,MOBERO Improves Morning and Bedtime Routines for Children with ADHD,"Families of children with Attention Deficit Hyperactivity Disorder (ADHD) often report morning and bedtime routines to be stressful and frustrating. Through a design process involving domain professionals and families we designed MOBERO, a smartphone-based system that assists families in establishing healthy morning and bedtime routines with the aim to assist the child in becoming independent and lowering the parents' frustration levels. In a two-week intervention with 13 children with ADHD and their families, MOBERO significantly improved children's independence and reduced parents' frustration levels. Additionally, use of MOBERO was associated with a 16.5% reduction in core ADHD symptoms and an 8.3% improvement in the child's sleep habits, both measured by standardized questionnaires. Our study highlights the potential of assistive technologies to change the everyday practices of families of children with ADHD.","Tobias Sonne, J&#246;rg M&#252;ller, Paul Marshall, Carsten Obel, Kaj Gr&#248;nb&#230;k","ADHD, assistive technology, attention deficit hyperactivity disorder, behavior change, children, mental health, mobile, routines, sleep",152,164
10.1145/2702123.2702589,CHI,2015,StructJumper,A Tool to Help Blind Programmers Navigate and Understand the Structure of Code,"It can be difficult for a blind developer to understand and navigate through a large amount of code quickly, as they are unable to skim as easily as their sighted counterparts. To help blind developers overcome this problem, we present StructJumper, an Eclipse plugin that creates a hierarchical tree based on the nesting structure of a Java class. The programmer can use the TreeView to get an overview of the code structure of the class (including all the methods and control flow statements) and can quickly switch between the TreeView and the Text Editor to get an idea of where they are within the nested structure. To evaluate StructJumper, we had seven blind programmers complete three tasks with and without our tool. We found that the users thought they would use StructJumper and there was a trend that they were faster completing the tasks with StructJumper.","Catherine Baker, Lauren Milne, Richard Ladner","accessibility, blind programmers, code structure, navigation, screen reader",3043,3052
10.1145/3173574.3173600,CHI,2018,MirrorMirror,A Mobile Application to Improve Speechreading Acquisition,"Many people around the world have difficulties in day-to-day conversation due to hearing loss. Hearing aids often fail to offer enough benefits and have low adoption rates. However, people with hearing loss find that speechreading can improve their understanding during conversation, but speechreading is a challenging skill to learn. Speechreading classes can improve acquisition, however there are a limited number of classes available and students can only practice effectively when attending class. To address this, we conducted a postal survey with 59 speechreading students to understand students' perspectives on practicing. Using our findings, we developed an Android application called MirrorMirror - a new Speechreading Acquisition Tool (SAT) that allows students to practice their speechreading by recording and watching videos of people they frequently speak with. We evaluated MirrorMirror through three case studies with speechreading students and found that they could effectively target their speechreading practice on people, words and situations they encounter during daily conversations.","Benjamin Gorman, David Flatla","accessibility, hearing loss, lipreading, speechreading",1,12
10.1145/1978942.1979001,CHI,2011,Usable gestures for blind people,understanding preference and performance,"Despite growing awareness of the accessibility issues surrounding touch screen use by blind people, designers still face challenges when creating accessible touch screen interfaces. One major stumbling block is a lack of understanding about how blind people actually use touch screens. We conducted two user studies that compared how blind people and sighted people use touch screen gestures. First, we conducted a gesture elicitation study in which 10 blind and 10 sighted people invented gestures to perform common computing tasks on a tablet PC. We found that blind people have different gesture preferences than sighted people, including preferences for edge-based gestures and gestures that involve tapping virtual keys on a keyboard. Second, we conducted a performance study in which the same participants performed a set of reference gestures. We found significant differences in the speed, size, and shape of gestures performed by blind people versus those performed by sighted people. Our results suggest new design guidelines for accessible touch screen interfaces.","Shaun Kane, Jacob Wobbrock, Richard Ladner","accessibility, blind, gesture recognition, gestures, touch screens",413,422
10.1145/3173574.3173964,CHI,2018,ChromaGlasses,Computational Glasses for Compensating Colour Blindness,"Prescription glasses are used by many people as a simple, and even fashionable way, to correct refractive problems of the eye. However, there are other visual impairments that cannot be treated with an optical lens in conventional glasses. In this work we present ChromaGlasses, Computational Glasses using optical head-mounted displays for compensating colour vision deficiency. Unlike prior work that required users to look at a screen in their visual periphery rather than at the environment directly, ChromaGlasses allow users to directly see the environment using a novel head-mounted displays design that analyzes the environment in real-time and changes the appearance of the environment with pixel precision to compensate the impairment of the user. In this work, we present first prototypes for ChromaGlasses and report on the results from several studies showing that ChromaGlasses are an effective method for managing colour blindness.","Tobias Langlotz, Jonathan Sutton, Stefanie Zollmann, Yuta Itoh, Holger Regenbrecht","augmented human, augmented reality, colour blindness, colour vision deficiency, computational glasses, head-mounted displays, near-eye displays, vision augmentation",1,12
10.1145/3173574.3174091,CHI,2018,Inclusive Computing in Special Needs Classrooms,Designing for All,"With a growing call for an increased emphasis on computing in school curricula, there is a need to make computing accessible to a diversity of learners. One potential approach is to extend the use of physical toolkits, which have been found to encourage collaboration, sustained engagement and effective learning in classrooms in general. However, little is known as to whether and how these benefits can be leveraged in special needs schools, where learners have a spectrum of distinct cognitive and social needs. Here, we investigate how introducing a physical toolkit can support learning about computing concepts for special education needs (SEN) students in their classroom. By tracing how the students' interactions-both with the physical toolkit and with each other-unfolded over time, we demonstrate how the design of both the form factor and the learning tasks embedded in a physical toolkit contribute to <i>collaboration, comprehension</i> and <i>engagement</i> when learning in mixed SEN classrooms.","Zuzanna Lechelt, Yvonne Rogers, Nicola Yuill, Lena Nagl, Grazia Ragone, Nicolai Marquardt","computational thinking, computer-supported learning, digital fluency, physical interfaces, special needs education",1,12
10.1145/3290605.3300933,CHI,2019,A Wee Bit More Interaction,Designing and Evaluating an Overactive Bladder App,"Overactive Bladder (OAB) is a widespread condition, affecting 20% of the population. Even though it is a treatable condition, people often do not seek treatment. In this paper, we describe how we co-designed and evaluated with 30 stakeholders (9 medical professionals and 21 end-users) an OAB mobile health application that aims to increase adherence to self-managed treatment. Our results support previous research that visualizing progress, setting goals, receiving reminders and feedback increases use. We discovered that games could be used successfully as a distraction technique for urge suppression. Contrary to the current research direction, automatically calculated features could be a detriment to app interaction. Regarding evaluation, we found that designers may not want to rely only on questionnaires when assessing the success of a game and its emotional impact on users.","Ana-Maria Salai, Lynne Baillie","assistive technology, co-design, interviews, mobile health applications, overactive bladder, usability",1,14
10.1145/3025453.3025790,CHI,2017,Smartphone-Based Gaze Gesture Communication for People with Motor Disabilities, ,"Current eye-tracking input systems for people with ALS or other motor impairments are expensive, not robust under sunlight, and require frequent re-calibration and substantial, relatively immobile setups. Eye-gaze transfer (e-tran) boards, a low-tech alternative, are challenging to master and offer slow communication rates. To mitigate the drawbacks of these two status quo approaches, we created GazeSpeak, an eye gesture communication system that runs on a smartphone, and is designed to be low-cost, robust, portable, and easy-to-learn, with a higher communication bandwidth than an e-tran board. GazeSpeak can interpret eye gestures in real time, decode these gestures into predicted utterances, and facilitate communication, with different user interfaces for speakers and interpreters. Our evaluations demonstrate that GazeSpeak is robust, has good user satisfaction, and provides a speed improvement with respect to an e-tran board; we also identify avenues for further improvement to low-cost, low-effort gaze-based communication technologies.","Xiaoyi Zhang, Harish Kulkarni, Meredith Morris","accessibility, amyotrophic lateral sclerosis (ALS), augmentative and alternative communication (AAC), eye gesture",2878,2889
10.1145/3025453.3025895,CHI,2017,Agency in Assistive Technology Adoption,Visual Impairment and Smartphone Use in Bangalore,"Studies on technology adoption typically assume that a user's perception of usability and usefulness of technology are central to its adoption. Specifically, in the case of accessibility and assistive technology, research has traditionally focused on the artifact rather than the individual, arguing that individual technologies fail or succeed based on their usability and fit for their users. Using a mixed-methods field study of smartphone adoption by 81 people with visual impairments in Bangalore, India, we argue that these positions are dated in the case of accessibility where a non-homogeneous population must adapt to technologies built for sighted people. We found that many users switch to smartphones despite their awareness of significant usability challenges with smartphones. We propose a nuanced understanding of perceived usefulness and actual usage based on need-related social and economic functions, which is an important step toward rethinking technology adoption for people with disabilities.","Joyojeet Pal, Anandhi Viswanathan, Priyank Chandra, Anisha Nazareth, Vaishnav Kameswaran, Hariharan Subramonyam, Aditya Johri, Mark Ackerman, Sile O'Modhrain","India, accessibility, android, bangalore, iOS, mobile phones",5929,5940
10.1145/3173574.3173630,CHI,2018,Environmental Factors in Indoor Navigation Based on Real-World Trajectories of Blind Users, ,"Indoor localization technologies can enhance quality of life for blind people by enabling them to independently explore and navigate indoor environments. Researchers typically evaluate their systems in terms of localization accuracy and user behavior along planned routes. We propose two measures of path-following behavior: deviation from optimal route and trajectory variability. Through regression analysis of real-world trajectories from blind users, we identify relationships between a) these measures and b) elements of the environment, route characteristics, localization error, and instructional cues that users receive. Our results provide insights into path-following behavior for turn-by-turn indoor navigation and have implications for the design of future interactions. Moreover, our findings highlight the importance of reporting these environmental factors and route properties in similar studies. We present automated and scalable methods for their calculation and to encourage their reporting for better interpretation and comparison of results across future studies.","Hernisa Kacorri, Eshed Ohn-Bar, Kris Kitani, Chieko Asakawa","accessibility, blind, indoor navigation, trajectory, turn-by-turn navigation",1,12
10.1145/3025453.3026032,CHI,2017,Leveraging Complementary Contributions of Different Workers for Efficient Crowdsourcing of Video Captions, ,"Hearing-impaired people and non-native speakers rely on captions for access to video content, yet most videos remain uncaptioned or have machine-generated captions with high error rates. In this paper, we present the design, implementation and evaluation of BandCaption, a system that combines automatic speech recognition with input from crowd workers to provide a cost-efficient captioning solution for accessible online videos. We consider four stakeholder groups as our source of crowd workers: (i) individuals with hearing impairments, (ii) second-language speakers with low proficiency, (iii) second-language speakers with high proficiency, and (iv) native speakers. Each group has different abilities and incentives, which our workflow leverages. Our findings show that BandCaption enables crowd workers who have different needs and strengths to accomplish micro-tasks and make complementary contributions. Based on our results, we outline opportunities for future research and provide design suggestions to deliver cost-efficient captioning solutions.","Yun Huang, Yifeng Huang, Na Xue, Jeffrey Bigham","complementary contributions, crowdsourcing, video caption",4617,4626
10.1145/2858036.2858070,CHI,2016,Designing Movement-based Play With Young People Using Powered Wheelchairs, ,"Young people using powered wheelchairs have limited access to engaging leisure activities. We address this issue through a two-stage project; 1) the participatory development of a set of wheelchair-controlled, movement-based games (with 9 participants at a school that provides education for young people who have special needs) and 2) three case studies (4 participants) exploring player perspectives on a set of three wheelchair-controlled casual games. Our results show that movement-based playful experiences are engaging for young people using powered wheelchairs. However, the participatory design process and case studies also reveal challenges for game accessibility regarding the integration of movement in games, diversity of abilities among young people using powered wheelchairs, and the representation of disability in games. In our paper, we explore how to address those challenges in the development of accessible, empowering movement-based games, which is crucial to the wider participation of young people using powered wheelchairs in play.","Kathrin Gerling, Kieran Hicks, Michael Kalyn, Adam Evans, Conor Linehan","accessibility, games, participatory design",4447,4458
10.1145/2470654.2466114,CHI,2013,Understanding palm-based imaginary interfaces,the role of visual and tactile cues when browsing,"Imaginary Interfaces are screen-less ultra-mobile interfaces. Previously we showed that even though they offer no visual feedback they allow users to interact spatially, e.g., by pointing at a location on their non-dominant hand. The primary goal of this paper is to provide a deeper understanding of palm-based imaginary interfaces, i.e., <i>why</i> they work. We perform our exploration using an interaction style inspired by interfaces for visually impaired users. We implemented a system that audibly announces target names as users scrub across their palm. Based on this interface, we conducted three studies. We found that (1) even though imaginary interfaces cannot display visual contents, users' visual sense remains the main mechanism that allows users to control the interface, as they watch their hands interact. (2) When we remove the visual sense by blindfolding, the tactile cues of both hands feeling each other in part replace the lacking visual cues, keeping imaginary interfaces usable. (3) While we initially expected the cues sensed by the pointing finger to be most important, we found instead that it is the tactile cues sensed by the <i>palm</i> that allow users to orient themselves most effectively. While these findings are primarily intended to deepen our understanding of Imaginary Interfaces, they also show that eyes-free interfaces located on skin outperform interfaces on physical devices. In particular, this suggests that palm-based imaginary interfaces may have benefits for visually impaired users, potentially outperforming the touchscreen-based devices they use today.","Sean Gustafson, Bernhard Rabe, Patrick Baudisch","imaginary interfaces, mobile, non-visual, visual feedback, tactile feedback, wearable",889,898
10.1145/1978942.1979030,CHI,2011,Cueing for drooling in Parkinson's disease, ,"We present the development of a socially acceptable cueing device for drooling in Parkinson's disease (PD). Sialorrhea, or drooling, is a significant problem associated with PD and has a strong negative emotional impact on those who experience it. Previous studies have shown the potential for managing drooling by using a cueing device. However, the devices used in these studies were deemed unacceptable by their users due to factors such as hearing impairment and social embarrassment. We conducted exploratory scoping work and high fidelity iterative prototyping with people with PD to get their input on the design of a cueing aid and this has given us an insight into challenges that confront users with PD and limit device usability and acceptability. The key finding from working with people with PD was the need for the device to be socially acceptable.","Roisin McNaney, Stephen Lindsay, Karim Ladha, Cassim Ladha, Guy Schofield, Thomas Ploetz, Nils Hammerla, Daniel Jackson, Richard Walker, Nick Miller, Patrick Olivier","Parkinson's disease, drooling, participatory design, swallowing",619,622
10.1145/2207676.2208324,CHI,2012,Full-body motion-based game interaction for older adults, ,"Older adults in nursing homes often lead sedentary lifestyles, which reduces their life expectancy. Full-body motion-control games provide an opportunity for these adults to remain active and engaged; these games are not designed with age-related impairments in mind, which prevents the games from being leveraged to increase the activity levels of older adults. In this paper, we present two studies aimed at developing game design guidelines for full-body motion controls for older adults experiencing age-related changes and impairments. Our studies also demonstrate how full-body motion-control games can accommodate a variety of user abilities, have a positive effect on mood and, by extension, the emotional well-being of older adults. Based on our studies, we present seven guidelines for the design of full-body interaction in games. The guidelines are designed to foster safe physical activity among older adults, thereby increasing their quality of life.","Kathrin Gerling, Ian Livingston, Lennart Nacke, Regan Mandryk","design, entertainment, games, older adults",1873,1882
10.1145/3290605.3300528,CHI,2019,The Promise of Empathy,"Design, Disability, and Knowing the ""Other""","This paper examines the promise of empathy, the name commonly given to the initial phase of the human-centered design process in which designers seek to understand their intended users in order to inform technology development. By analyzing popular empathy activities aimed at understanding people with disabilities, we examine the ways empathy works to both powerfully and problematically align designers with the values of people who may use their products. Drawing on disability studies and feminist theorizing, we describe how acts of empathy building may further distance people with disabilities from the processes designers intend to draw them into. We end by reimagining empathy as guided by the lived experiences of people with disabilities who are traditionally positioned as those to be empathized.","Cynthia Bennett, Daniela Rosner","design methods, disability, empathy",1,13
10.1145/2556288.2556963,CHI,2014,"Effects of balancing for physical abilities on player performance, experience and self-esteem in exergames", ,"Game balancing can help players with different skill levels play multiplayer games together; however, little is known about how the balancing approach affects performance, experience, and self-esteem'especially when differences in player strength result from given abilities, rather than learned skill. We explore three balancing approaches in a dance game and show that the explicit approach commonly used in commercial games reduces self-esteem and feelings of relatedness in dyads, whereas hidden balancing improves self-esteem and reduces score differential without affecting game outcome. We apply our results in a second study with dyads where one player had a mobility disability and used a wheelchair. By making motion-based games accessible for people with different physical abilities, and by enabling people with mobility disabilities to compete on a par with able-bodied peers, we show how to provide empowering experiences through enjoyable games that have the potential to increase physical activity and self-esteem.","Kathrin Gerling, Matthew Miller, Regan Mandryk, Max Birk, Jan Smeddinck","balancing, exergames, motion-based games, physical abilities, player experience",2201,2210
10.1145/2207676.2208649,CHI,2012,MOSOCO,a mobile assistive tool to support children with autism practicing social skills in real-life situations,"MOSOCO is a mobile assistive application that uses augmented reality and the visual supports of a validated curriculum, the Social Compass, to help children with autism practice social skills in real-life situations. In this paper, we present the results of a seven-week deployment study of MOSOCO in a public school in Southern California with both students with autism and neurotypical students. The results of our study demonstrate that MOSOCO facilitates practicing and learning social skills, increases both quantity and quality of social interactions, reduces social and behavioral missteps, and enables the integration of children with autism in social groups of neurotypical children. The findings from this study reveal emergent practices of the uses of mobile assistive technologies in real-life situations.","Lizbeth Escobedo, David Nguyen, LouAnne Boyd, Sen Hirano, Alejandro Rangel, Daniel Garcia-Rosas, Monica Tentori, Gillian Hayes","assistive technology, augmented reality, autism, child-computer interaction, mobile applications, social skills",2589,2598
10.1145/2470654.2481366,CHI,2013,Design to promote mindfulness practice and sense of self for vulnerable women in secure hospital services, ,"In the field of mental health care technologies, very limited attention has been given to the design of interventions for individuals who undergo treatment for severe mental health problems in intense care contexts. Exploring novel designs to engage vulnerable psychiatric patients in therapeutic skills practice and expanding on the potential of technology to promote mental health, the paper introduces the design concept of the <i>Spheres of Wellbeing</i>. A set of interactive artifacts is developed specifically for women with a dual diagnosis of a Learning Disability and Borderline Personality Disorder, living in the medium secure services of a forensic hospital in the UK. The women present a difficult to treat group due to extremely challenging behaviors and a fundamental lack of motivation to engage in therapy. The <i>Spheres</i> are designed to assist the women in practices of mindfulness, to help them tolerate emotional distress and to strengthen their sense of self, all of which are vital components of their specialist treatment Dialectical Behavioral Therapy (DBT). The Spheres are intended to supplement the therapy of the women and to contribute to our understanding of designing technology to enhance mental wellbeing and quality of life more generally.","Anja Thieme, Jayne Wallace, Paula Johnson, John McCarthy, Si&#226;n Lindley, Peter Wright, Patrick Olivier, Thomas Meyer","hospital, interaction design, materiality, mental health technology, mental wellbeing, mindfulness, sense of self",2647,2656
10.1145/3173574.3173801,CHI,2018,Design Opportunities for AAC and Children with Severe Speech and Physical Impairments, ,"Augmentative and alternative communication (AAC) technologies can support children with severe speech and physical impairments (SSPI) to express themselves. Yet, these seemingly 'enabling' technologies are often abandoned by this target group, suggesting a need to understand how they are used in communication. Little research has considered the interaction between people, interaction design and the material dimension of AAC. To address this, we report on a qualitative video study that examines the situated communication of five children using AAC in a special school. Our findings offer a new perspective on reconceptualising AAC design and use revealing four areas for future design: (1) incorporating an embodied view of communication, (2) designing to emphasise children's competence and agency, (3) regulating the presence, prominence and value of AAC, and (4) supporting a wider range of communicative functions that help address children's needs.","Seray Ibrahim, Asimina Vasalou, Michael Clarke","aac, accessibility, children, design, multimodal communication",1,13
10.1145/2858036.2858375,CHI,2016,MapSense,Multi-Sensory Interactive Maps for Children Living with Visual Impairments,"We report on the design process leading to the creation of MapSense, a multi-sensory interactive map for visually impaired children. We conducted a formative study in a specialized institute to understand children's educational needs, their context of care and their preferences regarding interactive technologies. The findings (1) outline the needs for tools and methods to help children to acquire spatial skills and (2) provide four design guidelines for educational assistive technologies. Based on these findings and an iterative process, we designed and deployed MapSense in the institute during two days. It enables collaborations between children with a broad range of impairments, proposes reflective and ludic scenarios and allows caretakers to customize it as they wish. A field experiment reveals that both children and caretakers considered the system successful and empowering.","Emeline Brule, Gilles Bailly, Anke Brock, Frederic Valentin, Gr&#233;goire Denis, Christophe Jouffrais","3D printing, DIY, accessibility, children, field-study, interactive map, tangible interaction, visual impairment",445,457
10.1145/3290605.3300740,CHI,2019,Exploring and Designing for Memory Impairments in Depression, ,"Depression is an affective disorder with distinctive autobiographical memory impairments, including negative bias, overgeneralization and reduced positivity. Several clinical therapies address these impairments, and there is an opportunity to develop new supports for treatment by considering depression-associated memory impairments within design. We report on interviews with ten experts in treating depression, with expertise in both neuropsychology and cognitive behavioral therapies. The interviews explore approaches for addressing each of these memory impairments. We found consistent use of positive memories for treating all memory impairments, the challenge of direct retrieval, and the need to support the experience of positive memories. We aim to sensitize HCI researchers to the limitations of memory technologies, broaden their awareness of memory impairments beyond episodic memory recall, and inspire them to engage with this less explored design space. Our findings open up new design opportunities for memory technologies for depression, including positive memory banks for active encoding and selective retrieval, novel cues for supporting generative retrieval, and novel interfaces to strengthen the reliving of positive memories.","Chengcheng Qu, Corina Sas, Gavin Doherty","cues, depression, memory impairment, memory technologies",1,15
10.1145/3173574.3174073,CHI,2018,"Values, Identity, and Social Translucence",Neurodiverse Student Teams in Higher Education,"To successfully function within a team, students must develop a range of skills for communication, organization, and conflict resolution. For students on the autism spectrum, these skills mirror the social, communicative, and cognitive experiences that can often be challenging for these learners. Since instructors and students collaborate using a mix of technology, we investigated the technology needs of neurodiverse teams comprised of autistic and non-autistic students. We interviewed seven autistic students and five employees of disability services in higher education. Our analysis focused on technology stakeholder values, stages of small-group development, and Social Translucence -- a model for online collaboration highlighting principles of visibility, awareness, and accountability. Despite motivation to succeed, neurodiverse students have difficulty expressing individual differences and addressing team conflict. To support future design of technology for neurodiverse teams, we propose: (1) a design space and design concepts including collaborative and affective computing tools, and (2) extending Social Translucence to account for student and group identities.","Annuska Zolyomi, Anne Ross, Arpita Bhattacharya, Lauren Milne, Sean Munson","autism, collaboration, neurodiversity, value-sensitive design",1,13
10.1145/3290605.3300371,CHI,2019,Virtual Showdown,An Accessible Virtual Reality Game with Scaffolds for Youth with Visual Impairments,"Virtual Reality (VR) is a growing source of entertainment, but people who are visually impaired have not been effectively included. Audio cues are motivated as a complement to visuals, making experiences more immersive, but are not a primary cue. To address this, we implemented a VR game called Virtual Showdown. We based Virtual Showdown on an accessible real-world game called Showdown, where people use their hearing to locate and hit a ball against an opponent. Further, we developed Verbal and Verbal/Vibration Scaffolds to teach people how to play Virtual Showdown. We assessed the acceptability of Virtual Showdown and compared our scaffolds in an empirical study with 34 youth who are visually impaired. Thirty-three participants wanted to play Virtual Showdown again, and we learned that participants scored higher with the Verbal Scaffold or if they had prior Showdown experience. Our empirical findings inform the design of future accessible VR experiences.","Ryan Wedoff, Lindsay Ball, Amelia Wang, Yi Xuan Khoo, Lauren Lieberman, Kyle Rector","blind, haptics, low vision, spatial audio, virtual reality, youth",1,15
10.1145/2858036.2858260,CHI,2016,Technological Caregiving,Supporting Online Activity for Adults with Cognitive Impairments,"With much of the population now online, the field of HCI faces new and pressing issues of how to help people sustain online activity throughout their lives, including through periods of disability. The onset of cognitive impairment later in life affects whether and how individuals are able to stay connected online and manage their digital information. While caregivers play a critical role in the offline lives of adults with cognitive impairments, less is known about how they support and enable online interaction. Using a constructivist grounded theory approach, data from focus groups with caregivers of adults with cognitive impairments reveal four forms of cooperative work caregivers perform in the context of supporting online activity. We find that staying active online is a way of empowering and engaging adults with cognitive impairments, yet this introduces new forms of risk, surrogacy, and cooperative technology use to the already demanding work of caregiving.","Anne Marie Piper, Raymundo Cornejo, Lisa Hurwitz, Caitlin Unumb","caregiving, social computing, vulnerable populations",5311,5323
10.1145/3173574.3173810,CHI,2018,Taking into Account Sensory Knowledge,The Case of Geo-techologies for Children with Visual Impairments,"This paper argues for designing geo-technologies supporting non-visual sensory knowledge. Sensory knowledge refers to the implicit and explicit knowledge guiding our uses of our senses to understand the world. To support our argument, we build on an 18 months field-study on geography classes for primary school children with visual impairments. Our findings show (1) a paradox in the use of non-visual sensory knowledge: described as fundamental to the geography curriculum, it is mostly kept out of school; (2) that accessible geo-technologies in the literature mainly focus on substituting vision with another modality, rather than enabling teachers to build on children's experiences; (3) the importance of the hearing sense in learning about space. We then introduce a probe, a wrist-worn device enabling children to record audio cues during field-trips. By giving importance to children's hearing skills, it modified existing practices and actors' opinions on non-visual sensory knowledge. We conclude by reflecting on design implications, and the role of technologies in valuing diverse ways of understanding the world.","Emeline Brul&#233;, Gilles Bailly","design, education, geography, maps, probe, sensory knowledge, visual impairments, wearable",1,14
10.1145/3290605.3300425,CHI,2019,"Understanding Trust, Transportation, and Accessibility through Ridesharing", ,"Relatively few studies of accessibility and transportation for people with vision impairments have investigated forms of transportation besides public transportation and walking. To develop a more nuanced understanding of this context, we turn to ridesharing, an increasingly used mode of transportation. We interviewed 16 visually-impaired individuals about their active use of ridesharing services like Uber and Lyft. Our findings show that, while people with vision impairments value independence, ridesharing involves building trust across a complex network of stakeholders and technologies. This data is used to start a discussion on how other systems can facilitate trust for people with vision impairments by considering the role of conversation, affordances of system incentives, and increased agency.","Robin Brewer, Vaishnav Kameswaran","accessibility, blind, ridesharing, transportation, trust, vision impairment",1,11
10.1145/2858036.2858564,CHI,2016,An Intimate Laboratory?,Prostheses as a Tool for Experimenting with Identity and Normalcy,"This paper is about the aspects of ability, selfhood, and normalcy embodied in people's relationships with prostheses. Drawing on interviews with 14 individuals with upper-limb loss and diverse experiences with prostheses, we find people not only choose to use and not use prosthesis throughout their lives but also form close and complex relationships with them. The design of ""assistive"" technology often focuses on enhancing function; however, we found that prostheses played important roles in people's development of identity and sense of normalcy. Even when a prosthesis failed functionally, such as was the case with 3D-printed prostheses created by an on-line open-source maker community (e-NABLE), we found people still praised the design and initiative because of the positive impacts on popular culture, identity, and community building. This work surfaces crucial questions about the role of design interventions in identity production, the promise of maker communities for accelerating innovation, and a broader definition of ""assistive"" technology.","Cynthia Bennett, Keting Cen, Katherine Steele, Daniela Rosner","ability, assistive technology, design interventions, identity, normalcy, prostheses",1745,1756
10.1145/2858036.2858146,CHI,2016,Technology and the Politics of Mobility,Evidence Generation in Accessible Transport Activism,"Digital technologies offer the possibility of community empowerment via the reconfiguration of public services. This potential relies on actively involved citizens engaging with decision makers to pursue civic goals. In this paper we study one such group of involved citizens, examining the evidencing practices of a rare disease charity campaigning for accessible public transport. Through fieldwork and interviews, we highlight the ways in which staff and volunteers assembled and presented different forms of evidence, in doing so reframing what is conceived as 'valid knowledge'. We note the challenges this group faced in capturing experiential knowledge around the accessibility barriers of public transport, and the trade-offs that are made when presenting evidence to policy and decision makers. We offer a number of design considerations for future HCI research, focusing on how digital technology might be configured more appropriately to support campaigning around the politics of mobility.","Sunil Rodger, John Vines, Janice McLaughlin","accessible transport, activism, disability, evidence, participation",2417,2429
10.1145/3025453.3025610,CHI,2017,Exploring the Design Space of AAC Awareness Displays, ,"Augmentative and alternative communication (AAC) devices are a critical technology for people with disabilities that affect their speech. One challenge with AAC systems is their inability to portray aspects of nonverbal communication that typically accent, complement, regulate, or substitute for verbal speech. In this paper, we explore the design space of awareness displays that can supplement AAC devices, considering their output features and their effects on the perceptions of interlocutors. Through designing prototypes and getting feedback on our designs from people with ALS, their primary caregivers, and other communication partners, we consider (1) the consistent tensions that arose between abstractness and clarity in meaning for these designs and (2) the ways in which these designs can further mark users as ""other."" Overall, we contribute a generative understanding of designing AAC awareness displays to augment and contextualize communication.","Kiley Sobel, Alexander Fiannaca, Jon Campbell, Harish Kulkarni, Ann Paradiso, Ed Cutrell, Meredith Morris","aac, accessibility, als, awareness displays, conversational awareness, conversational flow, disability., emotion expression, nonverbal communication",2890,2903
10.1145/2858036.2858507,CHI,2016,Tickers and Talker,An Accessible Labeling Toolkit for 3D Printed Models,"Three-dimensional models are important learning resources for blind people. With advances in 3D printing, 3D models are becoming more available. However, unlike visual or tactile graphics, there is no standard accessible way to label components in 3D models. We present a labeling toolkit that enables users to add and access audio labels to 3D printed models. The toolkit includes Tickers, small 3D printed percussion instruments added to 3D models, and Talker, a signal processing application that detects and classifies Ticker sounds. To use the toolkit, a model designer adds Tickers to a model using 3D modeling software. A user then prints the model with Tickers and records audio labels for each Ticker. Finally, users can strum the Tickers and Talker will play the corresponding labels. We evaluated Tickers and Talker with three models in a study with nine blind participants. Our toolkit achieved an accuracy of 93% across all participants and models. We discuss design implications and future work for accessible 3D printed models.","Lei Shi, Idan Zelzer, Catherine Feng, Shiri Azenkot","acoustic sensing, fabrication, labels, visual impairments",4896,4907
10.1145/3290605.3300746,CHI,2019,Design and Evaluation of a Social Media Writing Support Tool for People with Dyslexia, ,"People with dyslexia face challenges expressing themselves in writing on social networking sites (SNSs). Such challenges come from not only the technicality of writing, but also the self-representation aspect of sharing and communicating publicly on social networking sites such as Facebook. To empower people with dyslexia-style writing to express them-selves more confidently on SNSs, we designed and implemented Additional Writing Help(AWH) - a writing assistance tool to proofread text produced by users with dyslexia before they post on Facebook. AWH was powered by a neural machine translation (NMT) model that translates dyslexia style to non-dyslexia style writing. We evaluated the performance and the design of AWH through a week-long field study with 19 people with dyslexia and received highly positive feedback. Our field study demonstrated the value of providing better and more extensive writing support on SNSs, and the potential of AI for building a more inclusive Internet.","Shaomei Wu, Lindsay Reynolds, Xian Li, Francisco Guzm&#225;n","accessibility, artificial intelligence, dyslexia, social media",1,14
10.1145/3025453.3025845,CHI,2017,Facade,Auto-generating Tactile Interfaces to Appliances,"Common appliances have shifted toward flat interface panels, making them inaccessible to blind people. Although blind people can label appliances with Braille stickers, doing so generally requires sighted assistance to identify the original functions and apply the labels. We introduce <i>Facade</i> - a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel. Facade users capture a photo of the appliance with a readily available fiducial marker (a dollar bill) for recovering size information. This image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface. Facade then generates a 3D model for a layer of tactile and pressable buttons that fits over the original controls. Finally, a home 3D printer or commercial service fabricates the layer, which is then aligned and attached to the interface by the blind person. We demonstrate the viability of Facade in a study with 11 blind participants.","Anhong Guo, Jeeeun Kim, Xiang 'Anthony' Chen, Tom Yeh, Scott Hudson, Jennifer Mankoff, Jeffrey Bigham","3d printing, accessibility, blind, computer vision, crowdsourcing, fabrication, non-visual interfaces, visually impaired",5826,5838
10.1145/3290605.3300734,CHI,2019,ALAP,Accessible LaTeX Based Mathematical Document Authoring and Presentation,"Assistive technologies such as screen readers and text editors have been used in past to improve the accessibility and authoring of scientific and mathematical documents. However, most screens readers fail to narrate complex mathematical notations and expressions as they skip symbols and necessary information required for the accurate narration of mathematical content. This study aims at evaluating a new Accessible LaTeX Based Mathematical Document Authoring and Presentation (ALAP) tool, which assist people with visual impairments in reading and writing mathematical documents. ALAP includes features like, assistive debugging, Math Mode for reading and writing mathematical notations, and automatic generation of an accessible PDF document. These features aim to improve the LaTeX debugging experience and make it simple for blind users to author mathematical content by narrating it in natural language through the use of integrated text to speech (TTS) engine. We evaluated ALAP by conducting a study with 18 visually impaired LaTeX users. The results showed that users preferred ALAP over another comparable LaTeX based authoring tool and were relatively more comfortable in completing the tasks while using ALAP.","Ahtsham Manzoor, Safa Arooj, Shaban Zulfiqar, Murayyiam Parvez, Suleman Shahid, Asim Karim","accessible latex, accessible math, assistive debugging, computers, navigation, pdf accessibility, text to speech, visually impaired",1,12
10.1145/3290605.3300421,CHI,2019,Exploring the Opportunities for Technologies to Enhance Quality of Life with People who have Experienced Vision Loss, ,"Research predicts that 196 million people will be diagnosed with Age-Related Macular Degeneration (AMD) by 2020. People who experience AMD and other vision loss face barriers that affect their Quality of Life (QoL). People experience only modest improvement from technologies (e.g., screen readers, CCTV), tools (e.g., magnifying glasses, tactile buttons), and human help (e.g., friends, blindness organizations). Further, there are issues to accessing these resources based on one's place of residence. To explore these challenges and determine design implications to support people who have experienced vision loss (PVL), we conducted a qualitative semi-structured interview study exploring QoL with 10 PVL. We uncovered themes of supporting creative work, recognizing the impact of one's living in a non-urban setting on QoL, and increasing efficiency at accomplishing tasks. We motivate the inclusion of PVL in the design process because they learned skills while sighted and are now low vision or blind.","Rachel Bartlett, Yi Xuan Khoo, Juan Pablo Hourcade, Kyle Rector","accessibility, quality of life (qol), vision loss",1,8
10.1145/2858036.2858340,CHI,2016,Helping Hands,Requirements for a Prototyping Methodology for Upper-limb Prosthetics Users,"This paper presents a case study of three participants with upper-limb amputations working with researchers to design prosthetic devices for specific tasks: playing the cello, operating a hand-cycle, and using a table knife. Our goal was to identify requirements for a design process that can engage the assistive technology user in rapidly prototyping assistive devices that fill needs not easily met by traditional assistive technology. Our study made use of 3D printing and other playful and practical prototyping materials. We discuss materials that support on-the-spot design and iteration, dimensions along which in-person iteration is most important (such as length and angle) and the value of a supportive social network for users who prototype their own assistive technology. From these findings we argue for the importance of extensions in supporting modularity, community engagement, and relatable prototyping materials in the iterative design of prosthetics.","Megan Hofmann, Jeffrey Harris, Scott Hudson, Jennifer Mankoff","3D printing, assistive technology, design, disability, open-source, personal-scale fabrication, prototyping",1769,1780
10.1145/3173574.3173777,CHI,2018,"<i>""I can do everything but see!"" </i> -- How People with Vision Impairments Negotiate their Abilities in Social Contexts", ,"This research takes an orientation to visual impairment (VI) that does not regard it as fixed or determined alone in or through the body. Instead, we consider (dis)ability as produced through interactions with the environment and configured by the people and technology within it. Specifically, we explore how abilities become negotiated through video ethnography with six VI athletes and spectators during the Rio 2016 Paralympics. We use generated in-depth examples to identify how technology can be a meaningful part of ability negotiations, emphasizing how these embed into the social interactions and lives of people with VI. In contrast to treating technology as a solution to a 'sensory deficit', we understand it to support the triangulation process of sense-making through provision of appropriate additional information. Further, we suggest that technology should not try and replace human assistance, but instead enable people with VI to better identify and interact with other people in-situ.","Anja Thieme, Cynthia Bennett, Cecily Morrison, Edward Cutrell, Alex Taylor","ability, accessibility, assistive technology, blindness, collaboration, ethnography, social technology, vision impairment",1,14
10.1145/3290605.3300246,CHI,2019,Airport Accessibility and Navigation Assistance for People with Visual Impairments, ,"People with visual impairments often have to rely on the assistance of sighted guides in airports, which prevents them from having an independent travel experience. In order to learn about their perspectives on current airport accessibility, we conducted two focus groups that discussed their needs and experiences in-depth, as well as the potential role of assistive technologies. We found that independent navigation is a main challenge and severely impacts their overall experience. As a result, we equipped an airport with a Bluetooth Low Energy (BLE) beacon-based navigation system and performed a real-world study where users navigated routes relevant for their travel experience. We found that despite the challenging environment participants were able to complete their itinerary independently, presenting none to few navigation errors and reasonable timings. This study presents the first systematic evaluation posing BLE technology as a strong approach to increase the independence of visually impaired people in airports.","Jo&#227;o Guerreiro, Dragan Ahmetovic, Daisuke Sato, Kris Kitani, Chieko Asakawa","airport accessibility, assistive technologies, blind navigation, orientation and mobility, visual impairment",1,14
10.1145/2556288.2556962,CHI,2014,The effects of embodied persuasive games on player attitudes toward people using wheelchairs, ,"People using wheelchairs face barriers in their daily lives, many of which are created by people who surround them. Promoting positive attitudes towards persons with disabilities is an integral step in removing these barriers and improving their quality of life. In this context, persuasive games offer an opportunity of encouraging attitude change. We created a wheelchair-controlled persuasive game to study how embodied interaction can be applied to influence player attitudes over time. Our results show that the game intervention successfully raised awareness for challenges that people using wheelchairs face, and that embodied interaction is a more effective approach than traditional input in terms of retaining attitude change over time. Based on these findings, we provide design strategies for embodied interaction in persuasive games, and outline how our findings can be leveraged to help designers create effective persuasive experiences beyond games.","Kathrin Gerling, Regan Mandryk, Max Birk, Matthew Miller, Rita Orji","attitude change, disability, embodied interaction, persuasive games",3413,3422
10.1145/3290605.3300445,CHI,2019,Stroke-Gesture Input for People with Motor Impairments,Empirical Results & Research Roadmap,"We examine the articulation characteristics of stroke-gestures produced by people with upper body motor impairments on touchscreens as well as the accuracy rates of popular classification techniques, such as the $-family, to recognize those gestures. Our results on a dataset of 9,681 gestures collected from 70 participants reveal that stroke-gestures produced by people with motor impairments are recognized less accurately than the same gesture types produced by people without impairments, yet still accurately enough (93.0%) for practical purposes; are similar in terms of geometrical criteria to the gestures produced by people without impairments; but take considerably more time to produce (3.4s vs. 1.7s) and exhibit lower consistency (-49.7%). We outline a research roadmap for accessible gesture input on touchscreens for users with upper body motor impairments, and we make our large gesture dataset publicly available in the community.","Radu-Daniel Vatavu, Ovidiu-Ciprian Ungurean","dataset, gesture input, motor impairments, touchscreens",1,14
10.1145/3290605.3300608,CHI,2019,Voice User Interfaces in Schools,Co-designing for Inclusion with Visually-Impaired and Sighted Pupils,"Voice user interfaces (VUIs) are increasingly popular, particularly in homes. However, little research has investigated their potential in other settings, such as schools. We investigated how VUIs could support inclusive education, particularly for pupils with visual impairments (VIs). We organised focused discussions with educators at a school, with support staff from local authorities and, through bodystorming, with a class of 27 pupils. We then ran a series of co-design workshops with participants with mixed-visual abilities to design an educational VUI application. This provided insights into challenges faced by pupils with VIs in mainstream schools, and opened a space for educators, sighted and visually impaired pupils to reflect on and design for their shared learning experiences through VUIs. We present scenarios, a design space and an example application that show novel ways of using VUIs for inclusive education. We also reflect on co-designing with mixed-visual-ability groups in this space.","Oussama Metatla, Alison Oldfield, Taimur Ahmed, Antonis Vafeas, Sunny Miglani","co-design, education, inclusion, visual impairment, voice user interfaces",1,15
10.1145/2556288.2557415,CHI,2014,Visually impaired users on an online social network, ,"In this paper we present the first large-scale empirical study of how visually impaired people use online social networks, specifically Facebook. We identify a sample of 50K visually impaired users, and study the activities they perform, the content they produce, and the friendship networks they build on Facebook. We find that visually impaired users participate on Facebook (e.g. status updates, comments, likes) as much as the general population, and receive more feedback (i.e., comments and likes) on average on their content. By analyzing the content produced by visually impaired users, we find that they share their experience and issues related to vision impairment. We also identify distinctive patterns in their language and technology use. We also show that, compared to other users, visually impaired users have smaller social networks, but such differences have decreased over time. Our findings have implications for improving the utility and usability of online social networks for visually impaired users.","Shaomei Wu, Lada Adamic","facebook, social media, social networking sites, vision disability, visually impaired users",3133,3142
10.1145/3173574.3173609,CHI,2018,Understanding the Needs of Searchers with Dyslexia, ,"As many as 20% of English speakers have dyslexia, a language disability that impacts reading and spelling. Web search is an important modern literacy skill, yet the accessibility of this language-centric endeavor to people with dyslexia is largely unexplored. We interviewed ten adults with dyslexia and conducted an online survey with 81 dyslexic and 80 non-dyslexic adults, in which participants described challenges they face in various stages of web search (query formulation, search result triage, and information extraction). We also report the findings of an online study in which 174 adults with dyslexia and 172 without dyslexia rated the readability and relevance of sets of search query results. Our findings demonstrate differences in behaviors and preferences between dyslexic and non-dyslexic searchers, and indicate that factoring readability into search engine rankings and/or interfaces may benefit both dyslexic and non-dyslexic users.","Meredith Morris, Adam Fourney, Abdullah Ali, Laura Vonessen","dyslexia, learning disabilities, reading disabilities, search engine, web search",1,12
10.1145/1978942.1979029,CHI,2011,ACES,promoting empathy towards aphasia through language distortion emulation software,"Individuals with aphasia, an acquired communication disorder, constantly struggle against a world that does not understand them. This lack of empathy and understanding negatively impacts their quality of life. While aphasic individuals may appear to have lost cognitive functioning, their impairment relates to receptive and expressive language, not to thinking processes. We introduce a novel system and model, Aphasia Characteristics Emulation Software (ACES), enabling users (e.g., caregivers, speech therapists and family) to experience, firsthand, the communication-distorting effects of aphasia. By allowing neurologically typical individuals to ""walk in another's shoes,"" we aim to increase patience, awareness and understanding. ACES was grounded in the communication science and psychological literature, and informed by an initial pilot study. Results from an evaluation of 64 participants indicate that ACES provides a rich experience that increases understanding and empathy for aphasia.","Joshua Hailpern, Marina Danilevsky, Andrew Harris, Karrie Karahalios, Gary Dell, Julie Hengst","aphasia, assistive technology, disabilities, empathy, emulation software, language, speech",609,618
10.1145/3290605.3300518,CHI,2019,A Place to Play,The (Dis)Abled Embodied Experience for Autistic Children in Online Spaces,"Play is the work of children-but access to play is not equal from child to child. Having access to a place to play is a challenge for marginalized children, such as children with disabilities. For autistic children, playing with other children in the physical world may be uncomfortable or even painful. Yet, having practice in the social skills play provides is essential for childhood development. In this ethnographic work, I explore how one community uses the sense of place and the digital embodied experience in a virtual world specifically to give autistic children access to play with their peers. The contribution of this work is twofold. First, I demonstrate how various physical and virtual spaces work together to make play possible. Second, I demonstrate these spaces, though some of them are digital, are no more or less ""real"" than the physical spaces making up a schoolyard or playground.",Kathryn Ringland,"autism, communication, disability, embodied experience, inclusion, social interaction, social media, virtual worlds",1,14
10.1145/3290605.3300427,CHI,2019,Designing Interactive 3D Printed Models with Teachers of the Visually Impaired, ,"Students with visual impairments struggle to learn various concepts in the academic curriculum because diagrams, images, and other visual are not accessible to them. To address this, researchers have design interactive 3D printed models (I3Ms) that provide audio descriptions when a user touches components of a model. In prior work, I3Ms were designed on an ad hoc basis, and it is currently unknown what general guidelines produce effective I3M designs. To address this gap, we conducted two studies with Teachers of the Visually Impaired (TVIs). First, we led two design workshops with 35 TVIs, who modified sample models and added interactive elements to them. Second, we worked with three TVIs to design three I3Ms in an iterative instructional design process. At the end of this process, the TVIs used the I3Ms we designed to teach their students. We conclude that I3Ms should (1) have effective tactile features (e.g., distinctive patterns between components), (2) contain both auditory and visual content (e.g., explanatory animations), and (3) consider pedagogical methods (e.g., overview before details).","Lei Shi, Holly Lawson, Zhuohao Zhang, Shiri Azenkot","interactive 3d printed models, user-centered design",1,14
10.1145/3173574.3173631,CHI,2018,BSpeak,An Accessible Voice-based Crowdsourcing Marketplace for Low-Income Blind People,"BSpeak is an accessible crowdsourcing marketplace that enables blind people in developing regions to earn money by transcribing audio files through speech. We examine accessibility and usability barriers that 15 first-time users, who are low-income and blind, experienced while completing transcription tasks on BSpeak and Mechanical Turk (MTurk). Our mixed-methods analysis revealed severe accessibility barriers in MTurk due to the absence of landmarks, unlabeled UI elements, and improper use of HTML headings. Compared to MTurk, participants found BSpeak significantly more accessible and usable, and completed tasks with higher accuracy in lesser time due to its voice-based implementation. In a two-week field deployment of BSpeak in India, 24 low-income blind users earned rupee 7,310 by completing over 16,000 transcription tasks to yield transcriptions with 87% accuracy. Through our analysis of BSpeak's strengths and weaknesses, we provide recommendations for designing crowdsourcing marketplaces for low-income blind people in resource-constrained settings.","Aditya Vashistha, Pooja Sethi, Richard Anderson","accessibility, crowdsourcing, hci4d, india, voice",1,13
10.1145/3290605.3300615,CHI,2019,Empowering Expression for Users with Aphasia through Constrained Creativity, ,"Creative activities allow people to express themselves in rich, nuanced ways. However, being creative does not always come easily. For example, people with speech and language impairments, such as aphasia, face challenges in creative activities that involve language. In this paper, we explore the concept of constrained creativity as a way of addressing this challenge and enabling creative writing. We report an app, MakeWrite, that supports the constrained creation of digital texts through automated redaction. The app was co-designed with and for people with aphasia and was subsequently explored in a workshop with a group of people with aphasia. Participants were not only successful in crafting novel language, but, importantly, self-reported that the app was crucial in enabling them to do so. We refect on the potential of technology-supported constrained creativity as a means of empowering expression amongst users with diverse needs.","Timothy Neate, Abi Roper, Stephanie Wilson, Jane Marshall","accessibility, aphasia, constrained creativity, content creation, creative writing, creativity, poetry, speech impairments",1,12
10.1145/2470654.2470664,CHI,2013,Accessible online content creation by end users, ,"Like most online content, user-generated content (UGC) poses accessibility barriers to users with disabilities. However, the accessibility difficulties pervasive in UGC warrant discussion and analysis distinct from other kinds of online content. Content authors, community culture, and the authoring tool itself all affect UGC accessibility. The choices, resources available, and strategies in use to ensure accessibility are different than for other types of online content. We contribute case studies of two UGC communities with accessible content: Wikipedia, where authors focus on access to visual materials and navigation, and an online health support forum where users moderate the cognitive accessibility of posts. Our data demonstrate real world moderation strategies and illuminate factors affecting success, such as community culture. We conclude with recommended strategies for creating a culture of accessibility around UGC.","Katie Kuksenok, Michael Brooks, Jennifer Mankoff","accessibility, user-generated content",59,68
10.1145/3290605.3300240,CHI,2019,DesignABILITY,Framework for the Design of Accessible Interactive Tools to Support Teaching to Children with Disabilities,"Developing educational tools aimed at children with disabilities is a challenging process for designers and developers because existing methodologies or frameworks do not provide any pedagogical information and/or do not take into account the particular needs of users with some type of impairment. In this study, we propose a framework for the design of tools to support teaching to children with disabilities. The framework provides the necessary stages for the development of tools (hardware-based or software-based) and must be adapted for a specific disability and educational goal. For this study, the framework was adapted to support literacy teaching and contributes to the design of educational/interactive technology for deaf people while making them part of the design process and taking into account their particular needs. The experts' evaluation of the framework shows that it is well structured and may be adapted for other types of disabilities.","Leandro Fl&#243;rez-Aristiz&#225;bal, Sandra Cano, C&#233;sar Collazos, Andr&#233;s Solano, Stephen Brewster","collaborative learning, deaf children, design, disability, literacy, storytelling",1,16
10.1145/3290605.3300250,CHI,2019,Making Sense of Art,Access for Gallery Visitors with Vision Impairments,"While there is widespread recognition of the need to provide people with vision impairments (PVI) equitable access to cultural institutions such as art galleries, this is not easy. We present the results of a collaboration with a regional art gallery who wished to open their collection to PVIs in the local community. We describe a novel model that provides three different ways of accessing the gallery, depending upon visual acuity and mobility: virtual tours, self-guided tours and guided tours. As far as possible the model supports autonomous exploration by PVIs. It was informed by a value sensitive design exploration of the values and value conflicts of the primary stakeholders.","Leona Holloway, Kim Marriott, Matthew Butler, Alan Borning","3d printing, accessibility, art, blindness, value sensitive design, vision impairment",1,12
10.1145/2858036.2858409,CHI,2016,An Archive of Their Own,A Case Study of Feminist HCI and Values in Design,"Rarely are computing systems developed entirely by members of the communities they serve, particularly when that community is underrepresented in computing. Archive of Our Own (AO3), a fan fiction archive with nearly 750,000 users and over 2 million individual works, was designed and coded primarily by women to meet the needs of the online fandom community. Their design decisions were informed by existing values and norms around issues such as accessibility, inclusivity, and identity. We conducted interviews with 28 users and developers, and with this data we detail the history and design of AO3 using the framework of feminist HCI and focusing on the successful incorporation of values into design. We conclude with considering examples of complexity in values in design work: the use of design to mitigate tensions in values and to influence value formation or change.","Casey Fiesler, Shannon Morrison, Amy Bruckman","accessibility, fan fiction, feminist hci, human-centered design, online communities, social norms, values in design",2574,2585
10.1145/2702123.2702430,CHI,2015,"""My Hand Doesn't Listen to Me!""",Adoption and Evaluation of a Communication Technology for the 'Oldest Old',"Adoption and use of novel technology by the institutionalized 'oldest old' (80+) is understudied. This population is the fastest growing demographic group in developed countries, providing design opportunities and challenges for HCI. Since the recruitment of oldest old people is challenging, research tends to focus on older adults (65+) and their use of and attitudes towards existing communication technologies, or on their caregivers and social ties. Our study deployed a novel communication appliance among five frail oldest old people living in a long-term care facility, which included field observations and usability and accessibility tests. Our findings suggest factors that facilitate and hinder the adoption of communication technologies, such as social, attitudinal, digital literacy, physical, and usability. We also discuss issues that arise in studying technology adoption by the oldest old, including usability and accessibility testing, and suggest solutions that may be helpful to HCI researchers working with this population.","Barbara Neves, Rachel Franz, Cosmin Munteanu, Ronald Baecker, Mags Ngo","accessibility, adoption, communication technologies, evaluation, methodology, oldest old, usability",1593,1602
10.1145/3290605.3300744,CHI,2019,&#34;It Broadens My Mind&#34;,Empowering People with Cognitive Disabilities through Computing Education,"Computer science education is widely viewed as a path to empowerment for young people, potentially leading to higher education, careers, and development of computational thinking skills. However, few resources exist for people with cognitive disabilities to learn computer science. In this paper, we document our observations of a successful program in which young adults with cognitive disabilities are trained in computing concepts. Through field observations and interviews, we identify instructional strategies used by this group, accessibility challenges encountered by this group, and how instructors and students leverage peer learning to support technical education. Our findings lead to guidelines for developing tools and curricula to support young adults with cognitive disabilities in learning computer science.","Varsha Koushik, Shaun Kane","accessibility, cognitive disability, computer science education",1,12
10.1145/2207676.2208388,CHI,2012,SSMRecolor,improving recoloring tools with situation-specific models of color differentiation,"Color is commonly used to convey information in digital environments, but colors can be difficult to distinguish for many users -- either because of a congenital color vision deficiency (CVD), or because of situation-induced CVDs such as wearing colored glasses or working in sunlight. Tools intended to improve color differentiability (recoloring tools) exist, but these all use abstract models of only a few types of congenital CVD; if the user's color problems have a different cause, existing recolorers can perform poorly. We have developed a recoloring tool (SSMRecolor) based on the idea of situation-specific modeling -- in which we build a performance-based model of a particular user in their specific environment, and use that model to drive the recoloring process. SSMRecolor covers a much wider range of CVDs, including acquired and situational deficiencies. We evaluated SSMRecolor and two existing tools in a controlled study of people's color-matching performance in several environmental conditions. The study included participants with and without congenital CVD. Our results show both accuracy and response time in color-matching tasks were significantly better with SSMRecolor. This work demonstrates the value of a situation-specific approach to recoloring, and shows that this technique can substantially improve the usability of color displays for users of all types.","David Flatla, Carl Gutwin","accessibility, color, color vision deficiency, recoloring",2297,2306
10.1145/2556288.2557278,CHI,2014,GaitAssist,a daily-life support and training system for parkinson's disease patients with freezing of gait,"Patients with Parkinson's disease often experience freezing of gait, which bears a high risk of falling, a prevalent cause for morbidity and mortality. In this work we present GaitAssist, a wearable system for freezing of gait support in daily life. The system provides real-time auditory cueing after the onset of freezing episodes. Furthermore, GaitAssist implements training exercises to learn how to handle freezing situations. GaitAssist is the result of a design process where we considered the input of engineers, clinicians and 18 Parkinson's disease patients, in order to find an optimal trade-off between system wearability and performance. We tested the final system in a user study with 5 additional patients. They reported a reduction in the freezing of gait duration as a result of the auditory stimulation provided, and that they feel the system enhanced their confidence during walking.","Sinziana Mazilu, Ulf Blanke, Michael Hardegger, Gerhard Tr&#246;ster, Eran Gazit, Jeffrey Hausdorff","freezing of gait, gait impairment, on-body sensors, user-centered, wearable support",2531,2540
10.1145/2207676.2208354,CHI,2012,Reducing compensatory motions in video games for stroke rehabilitation, ,"Stroke is the leading cause of long-term disability among adults in industrialized nations; approximately 80% of people who survive a stroke experience motor disabilities. Recovery requires hundreds of daily repetitions of therapeutic exercises, often without therapist supervision. When performing therapy alone, people with limited motion often compensate for the lack of motion in one joint by moving another one. This compensation can impede the recovery progress and create new health problems. In this work we contribute (1) a methodology to reliably sense compensatory torso motion in the context of shoulder exercises done by persons with stroke and (2) the design and experimental evaluation of operant-conditioning-based strategies for games that aim to reduce compensatory torso motion. Our results show that these strategies significantly reduce compensatory motions compared to alternatives.","Gazihan Alankus, Caitlin Kelleher","compensation, design, stroke rehabilitation, video games",2049,2058
10.1145/2470654.2481401,CHI,2013,Three tensions in participatory design for inclusion, ,"One ideal of Participatory Design (PD) is active involvement by all stakeholders as co-designers. However, when PD is applied to real projects, certain compromises are unavoidable, no matter what stakeholders are involved. With this paper we want to shed light on some of the challenges in implementing ""true"" PD in the case of designing with children, in particular children with severe disabilities. We do this work to better understand challenges in an ongoing project, RHYME, and by doing so we hope to provide insight and inspiration for others.","Harald Holone, Jo Herstad","familiarity, inclusion, participatory design, universal design",2903,2906
10.1145/3173574.3173778,CHI,2018,vrSocial,Toward Immersive Therapeutic VR Systems for Children with Autism,"Social communication frequently includes nuanced nonverbal communication cues, including eye contact, gestures, facial expressions, body language, and tone of voice. This type of communication is central to face-to-face interaction, but can be challenging for children and adults with autism. Innovative technologies can provide support by augmenting human-delivered cuing and automated prompting. Specifically, immersive virtual reality (VR) offers an option to generalize social skill interventions by concretizing nonverbal information in real-time social interactions. In this work, we explore the design and evaluation of three nonverbal communication applications in immersive VR. The results of this work indicate that delivering real-time visualizations of proximity, speaker volume, and duration of one's speech is feasible in immersive VR and effective for real-time support for proximity regulation for children with autism. We conclude with design considerations for therapeutic VR systems.","LouAnne Boyd, Saumya Gupta, Sagar Vikmani, Carlos Gutierrez, Junxiang Yang, Erik Linstead, Gillian Hayes","accessibility, assistive technology, autism, immersive vr, nonverbal communication, prosody, proximity, visualization",1,12
10.1145/3025453.3025528,CHI,2017,Embracing Errors,Examining How Context of Use Impacts Blind Individuals' Acceptance of Navigation Aid Errors,"Prevention of errors has been an orienting goal within the field of Human-Computer Interaction since its inception, with particular focus on minimizing <i>human</i> errors through appropriate technology design. However, there has been relatively little exploration into how designers can best support users of <i>technologies</i> that will inevitably make errors. We present a mixed-methods study in the domain of navigation technology for visually impaired individuals. We examined how users respond to device errors made in realistic scenarios of use. Contrary to conventional wisdom that usable systems must be error-free, we found that 42% of errors were acceptable to users. Acceptance of errors depends on error type, building feature, and environmental context. Further, even when a technical error is acceptable to the user, the misguided social responses of others nearby can negatively impact user experience. We conclude with design recommendations that embrace errors while also supporting user management of errors in technical systems.","Ali Abdolrahmani, William Easley, Michele Williams, Stacy Branham, Amy Hurst","assistive technology, blindness, device errors, disability, navigation, stigmatization, visual impairments",4158,4169
10.1145/3290605.3300670,CHI,2019,Mazi,Tangible Technologies as a Channel for Collaborative Play,"This paper investigates how haptic and auditory stimulation can be playfully implemented as an accessible and stimulating form of interaction for children. We present the design of Mazi, a sonic Tangible User Interface (TUI) designed to encourage spontaneous and collaborative play between children with high support needs autism. We report on a five week study of Mazi with five children aged between 6 and 9 years old at a Special Education Needs (SEN) school in London, UK. We found that collaborative play emerged from the interaction with the system especially in regards to socialization and engagement. Our study contributes to exploring the potential of user-centered TUI development as a channel to facilitate social interaction while providing sensory regulation for children with SENs.","Antonella Nonnis, Nick Bryan-Kinns","autism, children, play, sensory integration, smart textiles, social interaction, tangible user interfaces",1,13
10.1145/3173574.3174217,CHI,2018,Baang,A Viral Speech-based Social Platform for Under-Connected Populations,"Speech is more natural than text for a large part of the world including hard-to-reach populations (low-literate, poor, tech-novice, visually-impaired, marginalized) and oral cultures. Voice-based services over simple mobile phones are effective means to provide orality-driven social connectivity to such populations. We present Baang, a versatile and inclusive voice-based social platform that allows audio content creation and sharing among its open community of users. Within 8 months, Baang spread virally to 10,721 users (69% of them blind) who participated in 269,468 calls and shared their thoughts via 44,178 audio-posts, 343,542 votes, 124,389 audio-comments and 94,864 shares. We show that the ability to vote, comment and share leads to viral spread, deeper engagement, longer retention and emergence of true dialog among participants. Beyond connectivity, Baang provides its users with a voice and a social identity as well as means to share information and get community support.","Agha Ali Raza, Bilal Saleem, Shan Randhawa, Zain Tariq, Awais Athar, Umar Saif, Roni Rosenfeld","blind users, hci4d, ict4d, ivr, low-literate, mobile phone, pakistan, speech-based social networks, telephone",1,12
10.1145/3173574.3173594,CHI,2018,SteeringWheel,A Locality-Preserving Magnification Interface for Low Vision Web Browsing,"Low-vision users struggle to browse the web with screen magnifiers. Firstly, magnifiers occlude significant portions of the webpage, thereby making it cumbersome to get the webpage overview and quickly locate the desired content. Further, magnification causes loss of spatial locality and visual cues that commonly define semantic relationships in the page; reconstructing semantic relationships exclusively from narrow views dramatically increases the cognitive burden on the users. Secondly, low-vision users have widely varying needs requiring a range of interface customizations for different page sections; dynamic customization in extant magnifiers is disruptive to users' browsing. We present SteeringWheel, a magnification interface that leverages content semantics to preserve local context. In combination with a physical dial, supporting simple rotate and press gestures, users can quickly navigate different webpage sections, easily locate desired content, get a quick overview, and seamlessly customize the interface. A user study with 15 low-vision participants showed that their web-browsing efficiency improved by at least 20 percent with SteeringWheel compared to extant screen magnifiers.","Syed Masum Billah, Vikas Ashok, Donald Porter, I.V. Ramakrishnan","accessibility, locality, low-vision, magnifier, user-interface, visual impairments, web-browsing",1,13
10.1145/3173574.3173633,CHI,2018,Rich Representations of Visual Content for Screen Reader Users, ,"Alt text (short for ""alternative text"") is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings.","Meredith Morris, Jazette Johnson, Cynthia Bennett, Edward Cutrell","accessibility, alt text, alternative text, blindness, captions, screen readers, visual impairment",1,11
10.1145/2556288.2557328,CHI,2014,The last meter,blind visual guidance to a target,"Smartphone apps can use object recognition software to provide information to blind or low vision users about objects in the visual environment. A crucial challenge for these users is aiming the camera properly to take a well-framed picture of the desired target object. We investigate the effects of two fundamental constraints of object recognition -- frame rate and camera field of view -- on a blind person's ability to use an object recognition smartphone app. The app was used by 18 blind participants to find visual targets beyond arm's reach and approach them to within 30 cm. While we expected that a faster frame rate or wider camera field of view should always improve search performance, our experimental results show that in many cases increasing the field of view does not help, and may even hurt, performance. These results have important implications for the design of object recognition systems for blind users.","Roberto Manduchi, James Coughlan","assistive technology, blindness, camera-based access to information, wayfinding",3113,3122
10.1145/3173574.3173643,CHI,2018,Blocks4All,Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments,"Blocks-based programming environments are a popular tool to teach children to program, but they rely heavily on visual metaphors and are therefore not fully accessible for children with visual impairments. We evaluated existing blocks-based environments and identified five major accessibility barriers for visually impaired users. We explored techniques to overcome these barriers in an interview with a teacher of the visually impaired and formative studies on a touchscreen blocks-based environment with five children with visual impairments. We distill our findings on usable touchscreen interactions into guidelines for designers of blocks-based environments.","Lauren Milne, Richard Ladner","accessibility, blocks-based programming environments, computer science education, visual impairments",1,10
10.1145/2702123.2702373,CHI,2015,TabLETS Get Physical,Non-Visual Text Entry on Tablet Devices,"Tablet devices can display full-size QWERTY keyboards similar to the physical ones. Yet, the lack of tactile feedback and the inability to rest the fingers on the home keys result in a highly demanding and slow exploration task for blind users. We present SpatialTouch, an input system that leverages previous experience with physical QWERTY keyboards, by supporting two-handed interaction through multitouch exploration and spatial, simultaneous audio feedback. We conducted a user study, with 30 novice touchscreen participants entering text under one of two conditions: (1) SpatialTouch or (2) mainstream accessibility method Explore by Touch. We show that SpatialTouch enables blind users to leverage previous experience as they do a better use of home keys and perform more efficient exploration paths. Results suggest that although SpatialTouch did not result in faster input rates overall, it was indeed able to leverage previous QWERTY experience in contrast to Explore by Touch.","Jo&#227;o Guerreiro, Andr&#233; Rodrigues, Kyle Montague, Tiago Guerreiro, Hugo Nicolau, Daniel Gon&#231;alves","bi-manual interaction, blind, non-visual interaction, spatial audio, tablet, text-entry, touchscreen",39,42
10.1145/2470654.2466158,CHI,2013,Analyzing user-generated youtube videos to understand touchscreen use by people with motor impairments, ,"Most work on the usability of touchscreen interaction for people with motor impairments has focused on lab studies with relatively few participants and small cross-sections of the population. To develop a richer characterization of use, we turned to a previously untapped source of data: YouTube videos. We collected and analyzed 187 non-commercial videos uploaded to YouTube that depicted a person with a physical disability interacting with a mainstream mobile touchscreen device. We coded the videos along a range of dimensions to characterize the interaction, the challenges encountered, and the adaptations being adopted in daily use. To complement the video data, we also invited the video uploaders to complete a survey on their ongoing use of touchscreen technology. Our findings show that, while many people with motor impairments find these devices empowering, accessibility issues still exist. In addition to providing implications for more accessible touchscreen design, we reflect on the application of user-generated content to study user interface design.","Lisa Anthony, YooJin Kim, Leah Findlater","assistive technology, ipad, iphone, motor impairments, physical disabilities, touchscreen, youtube",1223,1232
10.1145/3025453.3026045,CHI,2017,Understanding Volunteer AT Fabricators,Opportunities and Challenges in DIY-AT for Others in e-NABLE,"We present the results of a study of e-NABLE, a distributed, collaborative volunteer effort to design and fabricate upper-limb assistive technology devices for limb-different users. Informed by interviews with 14 stakeholders in e-NABLE, including volunteers and clinicians, we discuss differences and synergies among each group with respect to motivations, skills, and perceptions of risks inherent in the project. We found that both groups are motivated to be involved in e-NABLE by the ability to use their skills to help others, and that their skill sets are complementary, but that their different perceptions of risk may result in uneven outcomes or missed expectations for end users. We offer four opportunities for design and technology to enhance the stakeholders' abilities to work together.","Jeremiah Parry-Hill, Patrick Shih, Jennifer Mankoff, Daniel Ashbrook","3d printing, accessibility, assistive technology, digital fabrication, diy, limb difference, making, prosthetics",6184,6194
10.1145/2702123.2702324,CHI,2015,Networked Empowerment on Facebook Groups for Parents of Children with Special Needs, ,"Theories of empowerment explain how people gain personal and political control to take action to improve their lives. However, empowerment theories were developed prior to the Internet and fail to account for the speed and scale that people can find one another online. One domain where empowerment is critical is caring for children with special needs, in which parents are required to navigate a complex maze of services and processes to access care for their child. We conducted 43 interviews with parents of children with special needs to investigate whether using social media sites helps them to perform this caregiving work. Critically, parents are able to do this through almost real-time access to other parents on Facebook. This work introduces the concept of networked empowerment, that describes how parents find other parents, access resources, and explore new ways for promoting health advocacy among caregivers at a local and national level. We conclude with design implications for facilitating faster and better access to information and support for caregivers.","Tawfiq Ammari, Sarita Schoenebeck","children, disabilities, empowerment, parents, social media, special needs",2805,2814
10.1145/3173574.3173690,CHI,2018,Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation, ,"Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.","Yuhang Zhao, Cynthia Bennett, Hrvoje Benko, Edward Cutrell, Christian Holz, Meredith Morris, Mike Sinclair","auditory feedback, blindness, haptic feedback, mobility, virtual reality, visual impairments, white cane",1,14
10.1145/3173574.3173789,CHI,2018,A Face Recognition Application for People with Visual Impairments,Understanding Use Beyond the Lab,"Recognizing others is a major challenge for people with visual impairments (VIPs) and can hinder engagement in social activities. We present Accessibility Bot, a research prototype bot on Facebook Messenger, that leverages state-of-the-art computer vision and a user's friends' tagged photos on Facebook to help people with visual impairments recognize their friends. Accessibility Bot provides users information about identity and facial expressions and attributes of friends captured by their phone's camera. To guide our design, we interviewed eight VIPs to understand their challenges and needs in social activities. After designing and implementing the bot, we conducted a diary study with six VIPs to study its use in everyday life. While most participants found the Bot helpful, their experience was undermined by perceived low recognition accuracy, difficulty aiming a camera, and lack of knowledge about the phone's status. We discuss these real-world challenges, identify suitable use cases for Accessibility Bot, and distill design implications for future face recognition applications.","Yuhang Zhao, Shaomei Wu, Lindsay Reynolds, Shiri Azenkot","face recognition, social activity, visual impairment",1,14
10.1145/1978942.1979447,CHI,2011,OldGen,mobile phone personalization for older adults,"Mobile devices are currently difficult to customize for the usability needs of elderly users. The elderly are instead referred to specially designed ""senior phones"" or software add-ons. These tend to compromise in functionality as they attempt to solve many disabilities in a single solution. We present OldGen, a prototype framework where a novel concept enables accessibility features on generic mobile devices, by decoupling the software user interface from the phone's physical form factor. This opens up for better customization of the user interface, its functionality and behavior, and makes it possible to adapt it to the specific needs of each individual. OldGen makes the user interface portable, such that it could be moved between different phone hardware, regardless of model and brand. Preliminary observations and evaluations with elderly users indicate that this concept could address individual user interface related accessibility issues on general-purpose devices.","Alex Olwal, Dimitris Lachanas, Ermioni Zacharouli","accessibility, elderly, mobile, older users, user interfaces",3393,3396
10.1145/2858036.2858304,CHI,2016,Haptic Wave,A Cross-Modal Interface for Visually Impaired Audio Producers,"We present the Haptic Wave, a device that allows cross-modal mapping of digital audio to the haptic domain, intended for use by audio producers/engineers with visual impairments. We describe a series of participatory design activities adapted to non-sighted users where the act of prototyping facilitates dialog. A series of workshops scoping user needs, and testing a technology mock up and lo-fidelity prototype fed into the design of a final high-spec prototype. The Haptic Wave was tested in the laboratory, then deployed in real world settings in recording studios and audio production facilities. The cross-modal mapping is kinesthetic and allows the direct manipulation of sound without the translation of an existing visual interface. The research gleans insight into working with users with visual impairments, and transforms perspective to think of them as experts in non-visual interfaces for all users.","Atau Tanaka, Adam Parkinson","cross modal mapping, digital audio production, haptic interfaces, multimodal interaction, user centered design",2150,2161
10.1145/2556288.2557085,CHI,2014,Current and future mobile and wearable device use by people with visual impairments, ,"With the increasing popularity of mainstream wearable devices, it is critical to assess the accessibility implications of such technologies. For people with visual impairments, who do not always need the visual display of a mobile phone, alternative means of eyes-free wearable interaction are particularly appealing. To explore the potential impacts of such technology, we conducted two studies. The first was an online survey that included 114 participants with visual impairments and 101 sighted participants; we compare the two groups in terms of current device use. The second was an interview and design probe study with 10 participants with visual impairments. Our findings expand on past work to characterize a range of trends in smartphone use and accessibility issues therein. Participants with visual impairments also responded positively to two eyes-free wearable device scenarios: a wristband or ring and a glasses-based device. Discussions on projected use of these devices suggest that small, easily accessible, and discreet wearable input could positively impact the ability of people with visual impairments to access information on the go and to participate in certain social interactions.","Hanlu Ye, Meethu Malu, Uran Oh, Leah Findlater","accessibility, visual impairments, wearable computing",3123,3132
10.1145/3290605.3300602,CHI,2019,&#34;I Bought This for Me to Look More Ordinary&#34;,A Study of Blind People Doing Online Shopping,"Online shopping, by reducing the needs for traveling, has become an essential part of lives for people with visual impairments. However, in HCI, research on online shopping for them has only been limited to the analysis of accessibility and usability issues. To develop a broader and better understanding of how visually impaired people shop online and design accordingly, we conducted a qualitative study with twenty blind people. Our study highlighted that blind people's desire of being treated as ordinary had significantly shaped their online shopping practices: very attentive to the visual appearance of the goods even they themselves could not see and taking great pain to find and learn what commodities are visually appropriate for them. This paper reports how their trying to appear ordinary is manifested in online shopping and suggests design implications to support these practices.","Guanhong Liu, Xianghua Ding, Chun Yu, Lan Gao, Xingyu Chi, Yuanchun Shi","blindness, online shopping, ordinariness, visual impairment",1,11
10.1145/3173574.3173591,CHI,2018,Easy Return,An App for Indoor Backtracking Assistance,"We present a system that, implemented as an iPhone app controllable from an Apple Watch, can help a blind person backtrack a route taken in a building. This system requires no maps of the building or environment modifications. While traversing a path from a starting location to a destination, the system builds and records a path representation in terms of a sequence of turns and of step counts between turns. If the user wants to backtrack the same path, the system can provide assistance by tracking the user's location in the recorded path, and producing directional information in speech form about the next turns and step counts to follow. The system was tested with six blind participants in a controlled indoor experiment.","German Flores, Roberto Manduchi","inertial sensing, spatial accessibility, step counting, wayfinding",1,12
10.1145/2470654.2470703,CHI,2013,Age-related differences in performance with touchscreens compared to traditional mouse input, ,"Despite the apparent popularity of touchscreens for older adults, little is known about the psychomotor performance of these devices. We compared performance between older adults and younger adults on four desktop and touchscreen tasks: pointing, dragging, crossing and steering. On the touchscreen, we also examined pinch-to-zoom. Our results show that while older adults were significantly slower than younger adults in general, the touchscreen reduced this performance gap relative to the desktop and mouse. Indeed, the touchscreen resulted in a significant movement time reduction of 35% over the mouse for older adults, compared to only 16% for younger adults. Error rates also decreased.","Leah Findlater, Jon Froehlich, Kays Fattal, Jacob Wobbrock, Tanya Dastyar","accessibility, input, older adults, touchscreens",343,346
10.1145/3290605.3300530,CHI,2019,Towards Enabling Blind People to Independently Write on Printed Forms, ,"Filling out printed forms (e.g., checks) independently is currently impossible for blind people, since they cannot pinpoint the locations of the form fields, and quite often, they cannot even figure out what fields (e.g., name) are present in the form. Hence, they always depend on sighted people to write on their behalf, and help them affix their signatures. Extant assistive technologies have exclusively focused on reading, with no support for writing. In this paper, we introduce WiYG, a Write-it-Yourself guide that directs a blind user to the different form fields, so that she can independently fill out these fields without seeking assistance from a sighted person. Specifically, WiYG uses a pocket-sized custom 3D printed smartphone attachment, and well-established computer vision algorithms to dynamically generate audio instructions that guide the user to the different form fields. A user study with 13 blind participants showed that with WiYG, users could correctly fill out the form fields at the right locations with an accuracy as high as 89.5%.","Shirin Feiz, Syed Masum Billah, Vikas Ashok, Roy Shilkrot, IV Ramakrishnan","3d print, blind, form, paper, wiy, write-it-yourself, writing",1,12
10.1145/3290605.3300573,CHI,2019,Upside and Downside Risk in Online Security for Older Adults with Mild Cognitive Impairment, ,"Older adults are rapidly increasing their use of online services such as banking, social media, and email - services that come with subtle and serious security and privacy risks. Older adults with mild cognitive impairment (MCI) are particularly vulnerable to these risks because MCI can reduce their ability to recognize scams such as email phishing, follow recommended password guidelines, and consider the implications of sharing personal information. Older adults with MCI often cope with their impairments with the help of caregivers, including partners, children, and professional health personnel, when using and managing online services. Yet, this too carries security and privacy risks: sharing personal information with caregivers can create issues of agency, autonomy, and even risk embarrassment and information leakage; caregivers also do not always act in their charges' best interest. Through a series of interviews conducted in the US, we identify a spectrum of safeguarding strategies used and consider them through the lens of 'upside and downside risk' where there are tradeoffs between reduced privacy and maintaining older adults' autonomy and access to online services.","Helena Mentis, Galina Madjaroff, Aaron Massey","cybersecurity, dementia, discussion, risk, scams",1,13
10.1145/3290605.3300531,CHI,2019,"Social, Cultural and Systematic Frustrations Motivating the Formation of a DIY Hearing Loss Hacking Community", ,"Research on attitudes to assistive technology (AT) has shown both the positive and negative impact of these technologies on quality of life. Building on this research, we examine the sociocultural and technological frustrations with hearing loss (HL) technologies that motivate personal approaches to solving these issues. Drawing on meet-up observations and contextual interview data, we detail participants' experiences of and attitudes towards hearing AT that influences hacking hearing loss. Hearing AT is misunderstood as a solution to the impairment, influencing one-to-one interactions, cultural norms, and systematic frustrations. Participants' exasperation with the slow development of top-down solutions has led some members to design and develop their own personalised solutions. Beyond capturing a segment of the growing DIY health and wellbeing phenomenon, our findings extend beyond implications for design to present recommendations for the hearing loss industry, policy makers, and importantly, for researchers engaging with grassroots DIY health movements.","Aisling Ann O'Kane, Abdinasir Aliomar, Rebecca Zheng, Britta Schulte, Gianluca Trombetta","assistive technology, diy, hacking, health, hearing loss, social",1,14
10.1145/3025453.3025941,CHI,2017,Improving Gesture Recognition Accuracy on Touch Screens for Users with Low Vision, ,"We contribute in this work on gesture recognition to improve the accessibility of touch screens for people with low vision. We examine the accuracy of popular recognizers for gestures produced by people with and without visual impairments, and we show that the user-independent accuracy of $P, the best recognizer among those evaluated, is small for people with low vision (83.8%), despite $P being very effective for gestures produced by people without visual impairments (95.9%). By carefully analyzing the gesture articulations produced by people with low vision, we inform key algorithmic revisions for the P recognizer, which we call P+. We show significant accuracy improvements of $P+ for gestures produced by people with low vision, from 83.8% to 94.7% on average and up to 98.2%, and 3x faster execution times compared to P.",Radu-Daniel Vatavu,"1, P, P+, algorithms, evaluation, gesture recognition, low vision, point clouds, recognition, recognition accuracy, touch gestures, touch screens, visual impairments",4667,4679
10.1145/1753326.1753494,CHI,2010,Pointassist for older adults,analyzing sub-movement characteristics to aid in pointing tasks,"Perceptual, cognitive and motor deficits cause many older adults to have difficulty conducting pointing tasks on computers. Many strategies have been discussed in the HCI community to aid older adults and others in pointing tasks. We present a different approach in PointAssist, software that aids in pointing tasks by analyzing the characteristics of sub-movements, detecting when users have difficulty pointing, and triggering a precision mode that slows the speed of the cursor in those cases. PointAssist is designed to help maintain pointing skills, runs as a background process working with existing software, is not vulnerable to clusters of targets or targets in the way, and does not modify the visual appearance or the feel of user interfaces. There is evidence from a prior study that PointAssist helps young children conduct pointing tasks. In this paper, we present a study evaluating PointAssist with twenty older adults (ages 66-88). The study participants benefited from greater accuracy when using PointAssist, when compared to using the ""enhance pointer precision"" option in Windows XP. In addition, we provide evidence of correlations between neuropsychological measures, pointing performance, and PointAssist detecting pointing difficulty.","Juan Hourcade, Christopher Nguyen, Keith Perry, Natalie Denburg","accuracy, assistive technologies, older adults, pointing",1115,1124
10.1145/2858036.2858215,CHI,2016,SayWAT,Augmenting Face-to-Face Conversations for Adults with Autism,"During face-to-face conversations, adults with autism frequently use atypical rhythms and sounds in their speech (prosody), which can result in misunderstandings and miscommunication. SayWAT is a Wearable Assistive Technology that provides feedback to wearers about their prosody during face-to-face conversations. In this paper, we describe the design process that led to five design guidelines that governed the development of SayWAT and present results from two studies involving our prototype solution. Our results indicate that wearable assistive technologies can automatically detect atypical prosody and deliver feedback in real time without disrupting the wearer or the conversation partner. Additionally, we provide suggestions for wearable assistive technologies for social support.","LouAnne Boyd, Alejandro Rangel, Helen Tomimbang, Andrea Conejo-Toledo, Kanika Patel, Monica Tentori, Gillian Hayes","assistive technology, autism, communication, prosody, social skills, wearable computing",4872,4883
10.1145/3290605.3300722,CHI,2019,StoryBlocks,A Tangible Programming Game To Create Accessible Audio Stories,"Block-based programming languages can support novice programmers through features such as simplified code syntax and user-friendly libraries. However, most block-based programming languages are highly visual, which makes them inaccessible to blind and visually impaired students. To address the inaccessibility of block-based languages, we introduce StoryBlocks, a tangible block-based game that enables blind programmers to learn basic programming concepts by creating audio stories. In this paper, we document the design of StoryBlocks and report on a series of design activities with groups of teachers, Braille experts, and students. Participants in our design sessions worked together to create accessible stories, and their feedback offers insights for the future development of accessible, tangible programming tools.","Varsha Koushik, Darren Guinness, Shaun Kane","audio interfaces, computer science education, cross-ability collaboration, storytelling, tangible user interfaces",1,12
10.1145/3173574.3173665,CHI,2018,Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels, ,"As Automatic Speech Recognition (ASR) improves in accuracy, it may become useful for transcribing spoken text in real-time for Deaf and Hard-of-Hearing (DHH) individuals. To quantify users' comprehension and opinion of automatic captions, which inevitably contain some errors, we must identify appropriate methodologies for evaluation studies with DHH users, including quantitative measurement instruments suitable to the various literacy levels among the DHH population. A literature review guided our selection of several probes (e.g. multiple-choice comprehension-question accuracy or response time, scalar-questions about user estimation of ASR errors or their impact, users' numerical estimation of accuracy), which we evaluated in a lab study with DHH users, wherein their literacy levels and the actual accuracy of each caption stimulus were factors. For some probes, participants with lower literacy had more positive subjective responses overall, and, for participants with particular literacy score ranges, some probes were insufficiently sensitive to distinguish between caption accuracy levels.","Larwan Berke, Sushant Kafle, Matt Huenerfauth","accessibility, captioning, evaluation methods, literacy, people who are deaf or hard-of-hearing",1,12
10.1145/3025453.3025518,CHI,2017,Audible Beacons and Wearables in Schools,Helping Young Visually Impaired Children Play and Move Independently,"Young children with visual impairments tend to engage less with their surroundings, limiting the benefits from activities at school. We investigated novel ways of using sound from a bracelet, such as speech or familiar noises, to tell children about nearby people, places and activities, to encourage them to engage more during play and help them move independently. We present a series of studies, the first two involving visual impairment educators, that give insight into challenges faced by visually impaired children at school and how sound might help them. We then present a focus group with visually impaired children that gives further insight into the effective use of sound. Our findings reveal novel ways of combining sounds from wearables with sounds from the environment, motivating audible beacons, devices for audio output and proximity estimation. We present scenarios, findings and a design space that show the novel ways such devices could be used alongside wearables to help visually impaired children at school.","Euan Freeman, Graham Wilson, Stephen Brewster, Gabriel Baud-Bovy, Charlotte Magnusson, Hector Caltenco","beacons, children, play, visual impairment, wearables",4146,4157
10.1145/2470654.2481291,CHI,2013,Visual challenges in the everyday lives of blind people, ,"The challenges faced by blind people in their everyday lives are not well understood. In this paper, we report on the findings of a large-scale study of the visual questions that blind people would like to have answered. As part of this year-long study, 5,329 blind users asked 40,748 questions about photographs that they took from their iPhones using an application called VizWiz Social. We present a taxonomy of the types of questions asked, report on a number of features of the questions and accompanying photographs, and discuss how individuals changed how they used VizWiz Social over time. These results improve our understanding of the problems blind people face, and may help motivate new projects more accurately targeted to help blind people live more independently in their everyday lives.","Erin Brady, Meredith Morris, Yu Zhong, Samuel White, Jeffrey Bigham","accessibility, blind users, crowdsourcing, mobile, q&#38;a",2117,2126
10.1145/1753326.1753649,CHI,2010,Towards customizable games for stroke rehabilitation, ,"Stroke is the leading cause of long term disability among adults in industrialized nations. The partial paralysis that stroke patients often experience can make independent living difficult or impossible. Research suggests that many of these patients could recover by performing hundreds of daily repetitions of motions with their affected limbs. Yet, only 31% of patients perform the exercises recommended by their therapists. Home-based stroke rehabilitation games may help motivate stroke patients to perform the necessary exercises to recover. In this paper, we describe a formative study in which we designed and user tested stroke rehabilitation games with both stroke patients and therapists. We describe the lessons we learned about what makes games useful from a therapeutic point of view.","Gazihan Alankus, Amanda Lazar, Matt May, Caitlin Kelleher","design, stroke rehabilitation, video games",2113,2122
10.1145/3290605.3300566,CHI,2019,Hands Holding Clues for Object Recognition in Teachable Machines, ,"Camera manipulation confounds the use of object recognition applications by blind people. This is exacerbated when photos from this population are also used to train models, as with teachable machines, where out-of-frame or partially included objects against cluttered backgrounds degrade performance. Leveraging prior evidence on the ability of blind people to coordinate hand movements using proprioception, we propose a deep learning system that jointly models hand segmentation and object localization for object classification. We investigate the utility of hands as a natural interface for including and indicating the object of interest in the camera frame. We confirm the potential of this approach by analyzing existing datasets from people with visual impairments for object recognition. With a new publicly available egocentric dataset and an extensive error analysis, we provide insights into this approach in the context of teachable recognizers.","Kyungjun Lee, Hernisa Kacorri","blind, egocentric, hand, k-shot learning, object recognition",1,12
10.1145/3290605.3300612,CHI,2019,Using Both Hands,Tangibles for Stroke Rehabilitation in the Home,"Stroke is one of the most common cause of long-term disability in the world, significantly reducing quality of life through impairing motor functions and cognitive abilities. Whilst rehabilitation exercises can help in the recovery of motor function impairments, stroke survivors rarely exercise enough, leading to far from optimal recovery. In this paper, we investigate how upper limb stroke rehabilitation can be supported using interactive tangible bimanual devices in the home. We customise the rehabilitation activities based on individual rehabilitation requirements and motivation of stroke survivors. Through evaluation with five stroke survivors, we uncovered insight into how tangible stroke rehabilitation systems for the home should be designed. The evaluation revealed the special importance of tailorable form factor and supporting self-awareness and grip exercises in order to increase the independency of stroke survivors to carry out activities of daily living.","Mikko Kyt&#246;, Laura Maye, David McGookin","bilateral, bimanual, home, rehabilitation, stroke, tangible interaction",1,14
10.1145/2858036.2858390,CHI,2016,Smart Touch,Improving Touch Accuracy for People with Motor Impairments with Template Matching,"We present two contributions toward improving the accessibility of touch screens for people with motor impairments. First, we provide an exploration of the touch behaviors of 10 people with motor impairments, e.g., we describe how touching with the back or sides of the hand, with multiple fingers, or with knuckles creates varied multi-point touches. Second, we introduce Smart Touch, a novel template-matching technique for touch input that maps any number of arbitrary contact-areas to a user's intended (x,y) target location. The result is that users with motor impairments can touch however their abilities allow, and Smart Touch will resolve their intended touch point. Smart Touch therefore allows users to touch targets in whichever ways are most comfortable and natural for them. In an experimental evaluation, we found that Smart Touch predicted (x,y) coordinates of the users' intended target locations over three times closer to the intended target than the native Land-on and Lift-off techniques reported by the built-in touch sensors found in the Microsoft PixelSense interactive tabletop. This result is an important step toward improving touch accuracy for people with motor impairments and others for whom touch screen operation was previously impossible.","Martez Mott, Radu-Daniel Vatavu, Shaun Kane, Jacob Wobbrock","ability-based design, accessibility, motor impairment, p recognizer, target acquisition, touch input, touch screens",1934,1946
10.1145/3290605.3300254,CHI,2019,EarTouch,Facilitating Smartphone Use for Visually Impaired People in Mobile and Public Scenarios,"Interacting with a smartphone using touch input and speech output is challenging for visually impaired people in mobile and public scenarios, where only one hand may be available for input (e.g., while holding a cane) and using the loudspeaker for speech output is constrained by environmental noise, privacy, and social concerns. To address these issues, we propose EarTouch, a one-handed interaction technique that allows the users to interact with a smartphone using the ear to perform gestures on the touchscreen. Users hold the phone to their ears and listen to speech output from the ear speaker privately. We report how the technique was designed, implemented, and evaluated through a series of studies. Results show that EarTouch is easy, efficient, fun and socially acceptable to use.","Ruolin Wang, Chun Yu, Xing-Dong Yang, Weijie He, Yuanchun Shi","accessibility, capacitive sensing, eartouch, one-handed interaction, smartphone, vision impairment",1,13
10.1145/3025453.3026014,CHI,2017,ProCom,Designing and Evaluating a Mobile and Wearable System to Support Proximity Awareness for People with Autism,"People with autism are at risk for social isolation due to differences in their perception and engagement with the social world. In this work, we aim to address one specific concern related to socialization the understanding, awareness, and use of interpersonal space. Over the course of a year, we iteratively designed and tested a series of concepts for supporting children with autism in perceiving, understanding, and responding to physical proximity with other people. During this process, we developed ProCom, a prototype system for measuring proximity without requiring instrumentation of the environment or another person. We used a variety of low and high fidelity prototypes, culminating in ProCom, to assess the feasibility, utility, and challenges of this approach. The results of these iterative design engagements indicate that wearable assistive technologies can support people in developing awareness of physical proximity in social settings. However, challenges related to both personal and collective use remain","LouAnne Boyd, Xinlong Jiang, Gillian Hayes","autism, children, parallel design, proximity, self-monitoring, social skills, wearable computing",2865,2877
10.1145/2470654.2466157,CHI,2013,PointAssist,assisting individuals with motor impairments,"We tested <i>PointAssist</i>, software that assists in pointing tasks by detecting difficulty through a sub-movement analysis and triggering help, with adjustments proposed to personalize the assistance provided to individuals with motor impairments. A within-subjects study with sixteen individuals with fine motor skills impairments resulted in statistically significant effects on accuracy using Friedman's test with (<sub>&#935;</sub><sup>2</sup>(1)=6.4, <i>p</i>=.011 in favor of personalized <i>PointAssist</i> compared to no assistance.","Guarionex Salivia, Juan Pablo Hourcade","human-computer interaction, motor impairments, older adults, pointing tasks, sub-movements",1213,1222
10.1145/3290605.3300282,CHI,2019,BBeep,A Sonic Collision Avoidance System for Blind Travellers and Nearby Pedestrians,"We present an assistive suitcase system, BBeep, for supporting blind people when walking through crowded environments. BBeep uses pre-emptive sound notifications to help clear a path by alerting both the user and nearby pedestrians about the potential risk of collision. BBeep triggers notifications by tracking pedestrians, predicting their future position in real-time, and provides sound notifications only when it anticipates a future collision. We investigate how different types and timings of sound affect nearby pedestrian behavior. In our experiments, we found that sound emission timing has a significant impact on nearby pedestrian trajectories when compared to different sound types. Based on these findings, we performed a real-world user study at an international airport, where blind participants navigated with the suitcase in crowded areas. We observed that the proposed system significantly reduces the number of imminent collisions.","Seita Kayukawa, Keita Higuchi, Jo&#227;o Guerreiro, Shigeo Morishima, Yoichi Sato, Kris Kitani, Chieko Asakawa","blind navigation, collision prediction, obstacle avoidance, path clearing, pedestrian detection, visual impairments",1,12
10.1145/2858036.2858114,CHI,2016,Incloodle,Evaluating an Interactive Application for Young Children with Mixed Abilities,"Every child should have an equal opportunity to learn, play, and participate in his or her life. In this work, we investigate how interactive technology design features support children with and without disabilities with inclusion during play. We developed four versions of Incloodle, a two-player picture-taking tablet application, designed to be inclusive of children with different abilities and needs. Each version of the application varied in (1) whether or not it enforced co-operation between children; and in (2) whether it prompted interactions through in-app characters or more basic instructions. A laboratory study revealed technology-enforced cooperation was helpful for child pairs who needed scaffolding, but character-based prompting had little effect on children's experiences. We provide an empirical evaluation of interactive technology for inclusive play and offer guidance for designing technology that facilitates inclusive play between young neurotypical and neurodiverse children.","Kiley Sobel, Kyle Rector, Susan Evans, Julie Kientz","child-computer interaction, children, inclusion, inclusive design, inclusive play, neurodiversity, play, universal design",165,176
10.1145/3173574.3174060,CHI,2018,Supporting Rhythm Activities of Deaf Children using Music-Sensory-Substitution Systems, ,"Rhythm is the first musical concept deaf people learn in music classes. However, hearing loss limits the amount of information that allows a deaf person to evaluate his or her performance and stay in sync with other musicians. In this paper, we investigated how a visual and vibrotactile music-sensory-substitution device, MuSS-Bits++, affects rhythm discrimination, reproduction, and expressivity of deaf people. We conducted a controlled study with 11 deaf children and found that most participants felt more confident wearing the device in vibration mode even when it did not objectively improve their accuracy. Furthermore, we studied how MuSS-Bits++ can be used in music classes at deaf schools and what challenges and opportunities arise in such a setting. Based on these studies, we discuss insights and future directions that support the design and development of music-sensory-substitution systems for music making.","Benjamin Petry, Thavishi Illandara, Don Samitha Elvitigala, Suranga Nanayakkara","assistive technology, deaf, design, music, sensory substitution",1,10
10.1145/1978942.1979031,CHI,2011,Evaluating swabbing,a touchscreen input method for elderly users with tremor,"Elderly users suffering from hand tremor have difficulties interacting with touchscreens because of finger oscillation. It has been previously observed that sliding one's finger across the screen may help reduce this oscillation. In this work, we empirically confirm this advantage by (1) measuring finger oscillation during different actions and (2) comparing error rate and user satisfaction between traditional tapping and swabbing in which the user slides his finger towards a target on a screen edge to select it. We found that oscillation is generally reduced during sliding. Also, compared to tapping, swabbing resulted in improved error rates and user satisfaction. We believe that swabbing will make touchscreens more accessible to senior users with tremor.","Chat Wacharamanotham, Jan Hurtmanns, Alexander Mertens, Martin Kronenbuerger, Christopher Schlick, Jan Borchers","accuracy, evaluation, input methods, older adults, swabbing, tapping, touchscreen, tremor",623,626
10.1145/3025453.3025522,CHI,2017,A Critical Lens on Dementia and Design in HCI, ,"Designing new technologies with and for individuals with dementia is a growing topic of interest within HCI. Yet, predominant societal views contribute to the positioning of individuals with dementia as deficient and declining, and treat technology as filling a gap left by impairment. We present the perspective of <i>critical dementia</i> as a way of reflecting on these views in the context of recent epistemological shifts in HCI. In addition to articulating how HCI can leverage the perspective of critical dementia, we present a case analysis of technology design in art therapy involving people with dementia aimed at challenging conventional narratives. This paper calls attention to and helps solidify an agenda for how the CHI community approaches dementia, design, and technology.","Amanda Lazar, Caroline Edasis, Anne Marie Piper","dementia, design, disability, paradigm, theory",2175,2188
10.1145/2702123.2702474,CHI,2015,Using Interactive Machine Learning to Support Interface Development Through Workshops with Disabled People, ,"We have applied interactive machine learning (IML) to the creation and customisation of gesturally controlled musical interfaces in six workshops with people with learning and physical disabilities. Our observations and discussions with participants demonstrate the utility of IML as a tool for participatory design of accessible interfaces. This work has also led to a better understanding of challenges in end-user training of learning models, of how people develop personalised interaction strategies with different types of pre-trained interfaces, and of how properties of control spaces and input devices influence people's customisation strategies and engagement with instruments. This work has also uncovered similarities between the musical goals and practices of disabled people and those of expert musicians.","Simon Katan, Mick Grierson, Rebecca Fiebrink","accessible interfaces, interactive machine learning, music.",251,254
10.1145/3173574.3174127,CHI,2018,Teaching Language to Deaf Infants with a Robot and a Virtual Human, ,"Children with insufficient exposure to language during critical developmental periods in infancy are at risk for cognitive, language, and social deficits [55]. This is especially difficult for deaf infants, as more than 90% are born to hearing parents with little sign language experience [48]. We created an integrated multi-agent system involving a robot and virtual human designed to augment language exposure for 6-12 month old infants. Human-machine design for infants is challenging, as most screen-based media are unlikely to support learning in [33]. While presently, robots are incapable of the dexterity and expressiveness required for signing, even if it existed, developmental questions remain about the capacity for language from artificial agents to engage infants. Here we engineered the robot and avatar to provide visual language to effect socially contingent human conversational exchange. We demonstrate the successful engagement of our technology through case studies of deaf and hearing infants.","Brian Scassellati, Jake Brawer, Katherine Tsui, Setareh Nasihati Gilani, Melissa Malzkuhn, Barbara Manini, Adam Stone, Geo Kartheiser, Arcangelo Merla, Ari Shapiro, David Traum, Laura-Ann Petitto","assistive technology, language development, sign language, social robots, virtual humans",1,13
10.1145/2702123.2702372,CHI,2015,The CADENCE Corpus,A New Resource for Inclusive Voice Interface Design,"Papers on voice interfaces for people with cognitive impairment or demenita only provide small snapshots of actual interactions, if at all. This is a major obstacle to the development of better interfaces. Transcripts of interactions between users and systems contain rich evidence of typical language patterns, indicate how users conceptualise their computer interlocutor, and highlight key design issues. In this paper, we introduce the CADENCE corpus and outline how it can be used to stimulate replicable research on inclusive voice interfaces. The CADENCE corpus is first data set of its kind to include rich data from people with cognitive impairment and free for research use. The corpus consists of transcribed spoken interactions between older people with and without cognitive impairment and a simulated Intelligent Cognitive Assistant and includes comprehensive data on users' cognitive abilities.","Maria Wolters, Jonathan Kilgour, Sarah MacPherson, Myroslava Dzikovska, Johanna Moore","assistive technology, cognitive impairment, dementia, replichi, spoken dialogue systems, voice interfaces",3963,3966
10.1145/2858036.2858315,CHI,2016,The Design of Assistive Location-based Technologies for People with Ambulatory Disabilities,A Formative Study,"In this paper, we investigate how people with mobility impairments assess and evaluate accessibility in the built environment and the role of current and emerging location-based technologies therein. We conducted a three-part formative study with 20 mobility impaired participants: a semi-structured interview (Part 1), a participatory design activity (Part 2), and a design probe activity (Part 3). Part 2 and 3 actively engaged our participants in exploring and designing the future of what we call assistive location-based technologies (ALTs) location-based technologies that specifically incorporate accessibility features to support navigating, searching, and exploring the physical world. Our Part 1 findings highlight how existing mapping tools provide accessibility benefits even though often not explicitly designed for such uses. Findings in Part 2 and 3 help identify and uncover useful features of future ALTs. In particular, we synthesize 10 key features and 6 key data qualities. We conclude with ALT design recommendations.","Kotaro Hara, Christine Chan, Jon Froehlich","accessibility, assistive technology, location-based technology, mobility impairment",1757,1768
10.1145/3290605.3300259,CHI,2019,Changing Perspective,A Co-Design Approach to Explore Future Possibilities of Divergent Hearing,"Conventional hearing aids frame hearing impairment almost exclusively as a problem. In the present paper, we took an alternative approach by focusing on positive future possibilities of 'divergent hearing'. To this end, we developed a method to speculate simultaneously about not-yet-experienced positive meanings and not-yet-existing technology. First, we gathered already existing activities in which divergent hearing was experienced as an advantage rather than as a burden. These activities were then condensed into 'Prompts of Positive Possibilities' (PPP), such as 'Creating a shelter to feel safe in"". In performative sessions, participants were given these PPPs and 'Open Probes' to enact novel everyday activities. This led to 26 possible meanings and according devices, such as ""Being able to listen back into the past with a rewinder"". The paper provides valuable insights into the interests and expectations of people with divergent hearing as well as a methodological contribution to a possibility-driven design.","Judith D&#246;rrenb&#228;cher, Marc Hassenzahl","enhancement, hearing impairment, participation, performative methods, positive design, possibility-driven design",1,12
10.1145/2207676.2207733,CHI,2012,CrossingGuard,exploring information content in navigation aids for visually impaired pedestrians,"Visually impaired pedestrians experience unique challenges when navigating an urban environment because many cues about orientation and traffic patterns are difficult to ascertain without the use of vision. Technological aids such as customized GPS navigation tools offer the chance to augment visually impaired pedestrians' sensory information with a richer depiction of an environment, but care must be taken to balance the need for more information with other demands on the senses. In this paper, we focus on the information needs of visually impaired pedestrians at intersections, which present a specific cause of stress when navigating in unfamiliar locations. We present a navigation application prototype called CrossingGuard that provides rich information to a user such as details about intersection geometry that are not available to visually impaired pedestrians today. A user study comparing content-rich information to a baseline condition shows that content-rich information raises the level of comfort that visually impaired pedestrians feel at unfamiliar intersections. In addition, we discuss the categories of information that are most useful. Finally, we introduce a micro-task approach to gather intersection data via Street View annotations that achieves 85.5% accuracy over the 9 categories of information used by CrossingGuard.","Richard Guy, Khai Truong","geographic data, navigation aids, visually impaired pedestrians, wayfinding",405,414
10.1145/3290605.3300692,CHI,2019,"Local Standards for Anonymization Practices in Health, Wellness, Accessibility, and Aging Research at CHI", ,"When studying technologies pertaining to health, wellness, accessibility, and aging, researchers are often required to perform a balancing act between controlling and sharing sensitive data of the people in their studies and protecting the privacy of these participants. If the data can be anonymized and shared, it can boost the impact of the research by facilitating replication and extension. Despite anonymization, data reporting and sharing may lead to re-identification of participants, which can be particularly problematic when the research deals with sensitive topics, such as health. We analyzed 509 CHI papers in the domains of health, wellness, accessibility, and aging to examine data reporting and sharing practices. Our analysis revealed notable patterns and trends regarding the reporting of age, gender, participant types, sample sizes, methodology, ethical considerations, anonymization techniques, and data sharing. Based on our findings, we propose several suggestions for community standards and practices that could facilitate data reporting and sharing while limiting the privacy risks for study participants.","Jacob Abbott, Haley MacLeod, Novia Nurain, Gustave Ekobe, Sameer Patil","anonymization, data sharing, meta-hci, methodology, privacy, research reporting",1,14
10.1145/1753326.1753620,CHI,2010,SoundNet,investigating a language composed of environmental sounds,"Auditory displays have been used in both human-machine and computer interfaces. However, the use of non-speech audio in assistive communication for people with language disabilities, or in other applications that employ visual representations, is still under-investigated. In this paper, we introduce SoundNet, a linguistic database that associates natural environmental sounds with words and concepts. A sound labeling study was carried out to verify SoundNet associations and to investigate how well the sounds evoke concepts. A second study was conducted using the verified SoundNet data to explore the power of environmental sounds to convey concepts in sentence contexts, compared with conventional icons and animations. Our results show that sounds can effectively illustrate (especially concrete) concepts and can be applied to assistive interfaces.","Xiaojuan Ma, Christiane Fellbaum, Perry Cook","assistive technologies, environmental sound, soundnet",1945,1954
10.1145/2702123.2702393,CHI,2015,Head-Mounted Display Visualizations to Support Sound Awareness for the Deaf and Hard of Hearing, ,"Persons with hearing loss use visual signals such as gestures and lip movement to interpret speech. While hearing aids and cochlear implants can improve sound recognition, they generally do not help the wearer localize sound necessary to leverage these visual cues. In this paper, we design and evaluate visualizations for spatially locating sound on a head-mounted display (HMD). To investigate this design space, we developed eight high-level visual sound feedback dimensions. For each dimension, we created 3-12 example visualizations and evaluated these as a design probe with 24 deaf and hard of hearing participants (Study 1). We then implemented a real-time proof-of-concept HMD prototype and solicited feedback from 4 new participants (Study 2). Study 1 findings reaffirm past work on challenges faced by persons with hearing loss in group conversations, provide support for the general idea of sound awareness visualizations on HMDs, and reveal preferences for specific design options. Although preliminary, Study 2 further contextualizes the design probe and uncovers directions for future work.","Dhruv Jain, Leah Findlater, Jamie Gilkeson, Benjamin Holland, Ramani Duraiswami, Dmitry Zotkin, Christian Vogler, Jon Froehlich","accessibility, conversation support, deaf, hard of hearing, head-mounted display, sound visualization, wearable",241,250
10.1145/2470654.2481371,CHI,2013,Consent for all,revealing the hidden complexity of terms and conditions,"Terms and conditions are central in acquiring user consent by service providers. Such documents are frequently highly complex and unreadable, placing doubts on the validity of so called 'informed consent'. While readability and web accessibility have been major themes for some time in HCI, the core principles have yet to be applied beyond webpage content and are absent from the underpinning terms and conditions. Our concern is that accessible web pages will encourage consent, masking the complexities of the terms of usage. Using the SMOG readability formula and UK Energy services as a case study, we observed that a series of supplier terms and conditions were far beyond what a functionally literate adult could be expected to understand. We also present a browser based plug-in which compares SMOG readability scores to popular books. The intention is to use this plug-in to assist in surfacing the hidden complexities underpinning online consent.","Ewa Luger, Stuart Moran, Tom Rodden","consent, energy, literacy, readability, smog, usability",2687,2696
10.1145/3025453.3025646,CHI,2017,FLight,A Low-Cost Reading and Writing System for Economically Less-Privileged Visually-Impaired People Exploiting Ink-based Braille System,"Reading printed documents and writing on a paper pose a great challenge for visually-impaired people. Existing studies that attempt to solve these challenges are expensive and not feasible in low-income context. Moreover, these studies solve reading and writing problems separately. On the contrary, in this study, we propose <i>FLight</i>, a low-cost reading and writing system for economically less-privileged people. <i>FLight</i> uses ink-based Braille characters as the medium of textual representation. This helps in keeping a compact spatial representation of texts, yet achieving a low-cost status. Additionally, <i>FLight</i> utilizes a low-cost wearable device to enhance ease of reading by visually-impaired people. We conduct a participatory design and iterative evaluation involving five visually-impaired children in Bangladesh for more than 18 months. Our user evaluation reveals that <i>FLight</i> is easy-to-use, and exhibits a potential low-cost solution for economically less-privileged visually-impaired people.","Tusher Chakraborty, Taslim Khan, A. B. M. Al Islam","assistive technology, braille, economically less-privileged people, wearable device",531,540
10.1145/2470654.2481290,CHI,2013,Listen to it yourself!,evaluating usability of what's around me? for the blind,"Although multiple GPS-based navigation applications exist for the visually impaired, these are typically poorly suited for in-situ exploration, require cumbersome hardware, lack support for widely accessible geographic databases, or do not take advantage of advanced functionality such as spatialized audio rendering. These shortcomings led to our development of a novel spatial awareness application that leverages the capabilities of a smartphone coupled with worldwide geographic databases and spatialized audio rendering to convey surrounding points of interest. This paper describes the usability evaluation of our system through a task-based study and a longer-term deployment, each conducted with six blind users in real settings. The findings highlight the importance of testing in ecologically valid contexts over sufficient periods to face real-world challenges, including balancing quality versus quantity for audio information, overcoming limitations imposed by sensor accuracy and quality of database information, and paying appropriate design attention to physical interaction with the device.","Sabrina Pan&#235;els, Adriana Olmos, Jeffrey Blum, Jeremy Cooperstock","audio feedback, mobile interface, visually impaired",2107,2116
10.1145/2702123.2702578,CHI,2015,ColourID,Improving Colour Identification for People with Impaired Colour Vision,"Being able to identify colours is a fundamental human activity; colour identification helps us work, get dressed, prepare food, and keep safe. But for the 5% of the world with impaired colour vision (ICV), colour identification is often a challenge, resulting in frustration and confusion with sometimes dangerous consequences. Colour namer tools have been proposed as a solution, however these are often slow to use and imprecise. To address these shortcomings, we developed three new colour identification techniques (ColourNames, ColourMeters, ColourPopper) using a new colour name dictionary based on the largest colour naming experiment to date. We compared our techniques to colour namers using participants with ICV in desktop and mobile conditions, and found that ColourNames and ColourPopper resulted in ~99% colour identification accuracy (10% higher than the colour namer), ColourMeters and ColourPopper were three times faster, and ColourPopper had lower perceived effort and was ranked significantly higher. With the benefits provided by our new colour identification techniques, people with ICV are one step closer to seeing the world like everyone else.","David Flatla, Alan Andrade, Ross Teviotdale, Dylan Knowles, Craig Stewart","colour identification, colour namers, colour vision deficiency, colourblindness, impaired colour vision",3543,3552
10.1145/3290605.3300257,CHI,2019,Cluster Touch,Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments,"We present Cluster Touch, a combined user-independent and user-specific touch offset model that improves the accuracy of touch input on smartphones for people with motor impairments, and for people experiencing situational impairments while walking. Cluster Touch combines touch examples from multiple users to create a shared user-independent touch model, which is then updated with touch examples provided by an individual user to make it user-specific. Owing to this combination, Cluster Touch allows people to quickly improve the accuracy of their smartphones by providing only 20 touch examples. In a user study with 12 people with motor impairments and 12 people without motor impairments, but who were walking, Cluster Touch improved touch accuracy by 14.65% for the former group and 6.81% for the latter group over the native touch sensor. Furthermore, in an offline analysis of existing mobile interfaces, Cluster Touch improved touch accuracy by 8.21% and 4.84% over the native touch sensor for the two user groups, respectively.","Martez Mott, Jacob Wobbrock","ability-based design, accessibility, motor impairments, situational impairments, smartphones, touch input, touch modeling",1,14
10.1145/2702123.2702437,CHI,2015,RegionSpeak,Quick Comprehensive Spatial Descriptions of Complex Images for Blind Users,"Blind people often seek answers to their visual questions from remote sources, however, the commonly adopted single-image, single-response model does not always guarantee enough bandwidth between users and sources. This is especially true when questions concern large sets of information, or spatial layout, e.g., where is there to sit in this area, what tools are on this work bench, or what do the buttons on this machine do? Our RegionSpeak system addresses this problem by providing an accessible way for blind users to (i) combine visual information across multiple photographs via image stitching, em (ii) quickly collect labels from the crowd for all relevant objects contained within the resulting large visual area in parallel, and (iii) then interactively explore the spatial layout of the objects that were labeled. The regions and descriptions are displayed on an accessible touchscreen interface, which allow blind users to interactively explore their spatial layout. We demonstrate that workers from Amazon Mechanical Turk are able to quickly and accurately identify relevant regions, and that asking them to describe only one region at a time results in more comprehensive descriptions of complex images. RegionSpeak can be used to explore the spatial layout of the regions identified. It also demonstrates broad potential for helping blind users to answer difficult spatial layout questions.","Yu Zhong, Walter Lasecki, Erin Brady, Jeffrey Bigham","accessibility, crowdsourcing, stitching, visual questions",2353,2362
10.1145/3025453.3025731,CHI,2017,Ubiquitous Accessibility for People with Visual Impairments,Are We There Yet?,"Ubiquitous access is an increasingly common vision of computing, wherein users can interact with any computing device or service from anywhere, at any time. In the era of personal computing, users with visual impairments required special-purpose, assistive technologies, such as screen readers, to interact with computers. This paper investigates whether technologies like screen readers have kept pace with, or have created a barrier to, the trend toward ubiquitous access, with a specific focus on desktop computing as this is still the primary way computers are used in education and employment. Towards that, the paper presents a user study with 21 visually-impaired participants, specifically involving the switching of screen readers within and across different computing platforms, and the use of screen readers in remote access scenarios. Among the findings, the study shows that, even for remote desktop access - an early forerunner of true ubiquitous access - screen readers are too limited, if not unusable. The study also identifies several accessibility needs, such as uniformity of navigational experience across devices, and recommends potential solutions. In summary, assistive technologies have not made the jump into the era of ubiquitous access, and multiple, inconsistent screen readers create new practical problems for users with visual impairments.","Syed Billah, Vikas Ashok, Donald Porter, I.V. Ramakrishnan","mobile computing, multiple screen readers, remote access, ubiquitous accessibility, visually impaired users",5862,5868
10.1145/2556288.2557237,CHI,2014,Wearables and chairables,inclusive design of mobile input and output techniques for power wheelchair users,"Power wheelchair users often use and carry multiple mobile computing devices. Many power wheelchair users have some upper body motor impairment that can make using these devices difficult. We believe that mobile device accessibility could be improved through designs that take into account users' functional abilities and take advantage of available space around the wheelchair itself. In this paper we present findings from multiple design sessions and interviews with 13 power wheelchair users and 30 clinicians, exploring the placement and form factor possibilities for input and output on a power wheelchair. We found that many power wheelchair users could benefit from <i>chairable</i> technology that is designed to work within the workspace of the wheelchair, whether worn on the body or mounted on he wheelchair frame. We present participants' preferences for chairable input and output devices, and identify possible design configurations for wearable and chairable devices.","Patrick Carrington, Amy Hurst, Shaun Kane","accessibility, input, mobile computing, natural user interface, output, participatory design, power wheelchair, wearable computers",3103,3112
10.1145/3290605.3300357,CHI,2019,An Autonomy-Perspective on the Design of Assistive Technology Experiences of People with Multiple Sclerosis, ,"In HCI and Assistive Technology design, autonomy is regularly equated with independence. This is a shortcut and leaves out design opportunities by omitting a more nuanced idea of autonomy. To improve our understanding of how people with severe physical disabilities experience autonomy, particularly in the context of Assistive Technologies, we engaged in in-depth fieldwork with 15 people with Multiple Sclerosis who were used to assistive devices. We constructed a grounded theory from a series of interviews, focus groups and observations, pointing to strategies in which participants sought autonomy either in the short-term (managing their daily energy reserve) or in the long-term (making future plans). The theory shows how factors like enabling technologies, capital (human, social, psychological resources), and compatibility with daily practices facilitated a sense of being in control for our participants. Moreover, we show how over-ambitious or bad design (e.g., paternalism) can lead to opposite results and restrict autonomy.","Florian G&#252;ldenpfennig, Peter Mayer, Paul Panek, Geraldine Fitzpatrick","active and assisted living, ambient assisted living, autonomy, grounded theory, multiple sclerosis, robotic toilet",1,14
10.1145/3173574.3173857,CHI,2018,Voicesetting,Voice Authoring UIs for Improved Expressivity in Augmentative Communication,"Alternative and augmentative communication (AAC) systems used by people with speech disabilities rely on text-to-speech (TTS) engines for synthesizing speech. Advances in TTS systems allowing for the rendering of speech with a range of emotions have yet to be incorporated into AAC systems, leaving AAC users with speech that is mostly devoid of emotion and expressivity. In this work, we describe voicesetting as the process of authoring the speech properties of text. We present the design and evaluation of two voicesetting user interfaces: the Expressive Keyboard, designed for rapid addition of expressivity to speech, and the Voicesetting Editor, designed for more careful crafting of the way text should be spoken. We evaluated the perceived output quality, requisite effort, and usability of both interfaces; the concept of voicesetting and our interfaces were highly valued by end-users as an enhancement to communication quality. We close by discussing design insights from our evaluations.","Alexander Fiannaca, Ann Paradiso, Jon Campbell, Meredith Morris","aac, als, amyotrophic lateral sclerosis, tts",1,12
10.1145/3025453.3025603,CHI,2017,Comparing Touchscreen and Mouse Input Performance by People With and Without Upper Body Motor Impairments, ,"Controlled studies of touchscreen input performance for users with upper body motor impairments remain relatively sparse. To address this gap, we present a controlled lab study of mouse vs. touchscreen performance with 32 participants (16 with upper body motor impairments and 16 without). Our study examines: (1) how touch input compares to an indirect pointing device (a mouse); (2) how performance compares across a range of standard interaction techniques; and (3) how these answers differ for users with and without motor impairments. While the touchscreen was faster than the mouse overall, only participants without motor impairments benefited from a lower error rate on the touchscreen. Indeed, participants <i>with</i> motor impairments had a <i>three-fold increase</i> in pointing (tapping) errors on the touchscreen compared to the mouse. Our findings also highlight the high frequency of spurious touches for users with motor impairments and update past accessibility recommendations for minimum touchscreen target sizes to at least 18mm.","Leah Findlater, Karyn Moffatt, Jon Froehlich, Meethu Malu, Joan Zhang","accessibility, human performance, input devices, motor impairments, mouse, touchscreen",6056,6061
10.1145/1978942.1978944,CHI,2011,Classroom-based assistive technology,collective use of interactive visual schedules by students with autism,"vSked is an interactive and collaborative assistive technology for students with autism, combining visual schedules, choice boards, and a token-based reward system into an integrated classroom system. In this paper, we present the results of a study of three deployments of vSked over the course of a year in two autism classrooms. The results of our study demonstrate that vSked can promote student independence, reduce the quantity of educator-initiated prompts, encourage consistency and predictability, reduce the time required to transition from one activity to another. The findings from this study reveal practices surrounding the use of assistive technologies in classrooms and highlight important considerations for both the design and the evaluation of assistive technologies in the future, especially those destined for classroom use.","Meg Cramer, Sen Hirano, Monica Tentori, Michael Yeganyan, Gillian Hayes","assistive technology, autism, visual schedules",1,10
10.1145/3290605.3300446,CHI,2019,Developing Accessible Services,Understanding Current Knowledge and Areas for Future Support,"When creating digital artefacts, it is important to ensure that the product being made is accessible to as much of the population as is possible. Many guidelines and supporting tools exist to assist reaching this goal. However, little is known about developers' understanding of accessible practice and the methods that are used to implement this. We present findings from an accessibility design workshop that was carried out with a mixture of 197 developers and digital technology students. We discuss perceptions of accessibility, techniques that are used when designing accessible products, and what areas of accessibility development participants believed were important. We show that there are gaps in the knowledge needed to develop accessible products despite the effort to promote accessible design. Our participants are themselves aware of where these gaps are and have suggested a number of areas where tools, techniques and guidance would improve their practice.","Michael Crabb, Michael Heron, Rhianne Jones, Mike Armstrong, Hayley Reid, Amy Wilson","accessibility, digital accessibility, web accessibility",1,12
10.1145/3290605.3300606,CHI,2019,Accessible Gesture Typing for Non-Visual Text Entry on Smartphones, ,"Gesture typing--entering a word by gliding the finger sequentially over letter to letter-- has been widely supported on smartphones for sighted users. However, this input paradigm is currently inaccessible to blind users: it is difficult to draw shape gestures on a virtual keyboard without access to key visuals. This paper describes the design of accessible gesture typing, to bring this input paradigm to blind users. To help blind users figure out key locations, the design incorporates the familiar screen-reader supported touch exploration that narrates the keys as the user drags the finger across the keyboard. The design allows users to seamlessly switch between exploration and gesture typing mode by simply lifting the finger. Continuous touch-exploration like audio feedback is provided during word shape construction that helps the user glide in the right direction of the key locations constituting the word. Exploration mode resumes once word shape is completed. Distinct earcons help distinguish gesture typing mode from touch exploration mode, and thereby avoid unintended mix-ups. A user study with 14 blind people shows 35% increment in their typing speed, indicative of the promise and potential of gesture typing technology for non-visual text entry.","Syed Masum Billah, Yu-Jung Ko, Vikas Ashok, Xiaojun Bi, IV Ramakrishnan","accessible text entry, blind, gesture-typing, non-visual",1,12
10.1145/3173574.3174203,CHI,2018,Towards a Multisensory Augmented Reality Map for Blind and Low Vision People,a Participatory Design Approach,"Current low-tech Orientation &#38; Mobility (O&#38;M) tools for visually impaired people, e.g. tactile maps, possess limitations. Interactive accessible maps have been developed to overcome these. However, most of them are limited to exploration of existing maps, and have remained in laboratories. Using a participatory design approach, we have worked closely with 15 visually impaired students and 3 O&#38;M instructors over 6 months. We iteratively designed and developed an augmented reality map destined at use in O&#38;M classes in special education centers. This prototype combines projection, audio output and use of tactile tokens, and thus allows both map exploration and construction by low vision and blind people. Our user study demonstrated that all students were able to successfully use the prototype, and showed a high user satisfaction. A second phase with 22 international special education teachers allowed us to gain more qualitative insights. This work shows that augmented reality has potential for improving the access to education for visually impaired people.","J&#233;r&#233;my Albouys-Perrois, J&#233;r&#233;my Laviole, Carine Briant, Anke Brock","accessibility, augmented reality, geographic maps, participatory design, visual impairment",1,14
10.1145/3025453.3025472,CHI,2017,Automated Detection of Facial Expressions during Computer-Assisted Instruction in Individuals on the Autism Spectrum, ,"It has been suggested that computer-assisted instruction (CAI) is a promising method for educating students on the autism spectrum. We aimed to determine whether automated recognition of facial expressions aided in predicting CAI engagement and learning performance. Seven youth with autism (mean age = 12.7, SD = 4.2) interacted with a CAI program, TeachTown Basics, for 15 consecutive sessions. Video recordings of the participants' faces were collected during these sessions and facial expressions from these videos were analyzed using CERT, an algorithm that automatically outputs intensity values for each facial action unit (AU). Using these data, we attempted to operationally define two engagement indices: (1) behavioral engagement, the proportion of time a participant had their face oriented to the computer screen; and (2) emotional engagement, the activation of AUs previously associated with CAI. Our results suggest that both indices strongly correlated with one another, but that emotional (not behavioral) engagement predicted test performance. CAI knowledge domain, participant sex, and developmental age also contributed to the prediction.","Alex Ahmed, Matthew Goodwin","computer vision, education/learning, emotion/affective computing, individuals with disabilities &#38; assistive technologies, quantitative methods",6050,6055
10.1145/3290605.3300452,CHI,2019,Patient Perspectives on Self-Management Technologies for Chronic Fatigue Syndrome, ,"Chronic Fatigue Syndrome (CFS) is a debilitating medical condition that is characterized by a range of physical, cognitive and social impairments. This paper investigates CFS patients' perspectives on the potential for technological support for self-management of their symptoms. We report findings from three studies in which people living with CFS 1) prioritized symptoms that they would like technologies to address, 2) articulated their current approaches to self-management alongside challenges they face, and 3) reflected on their experiences with three commercial smartphone apps related to symptom management. We contribute an understanding of the specific needs of the ME/CFS population and the ways in which they currently engage in self-management using technology. The paper ends by describing five high-level design recommendations for ME/CFS self-management technologies.","Tabby Davies, Simon Jones, Ryan Kelly","chronic fatigue syndrome, myalgic encephalomyelitis, self-management, self-tracking",1,13
10.1145/2702123.2702421,CHI,2015,FingerReader,A Wearable Device to Explore Printed Text on the Go,"Accessing printed text in a mobile context is a major challenge for the blind. A preliminary study with blind people reveals numerous difficulties with existing state-of-the-art technologies including problems with alignment, focus, accuracy, mobility and efficiency. In this paper, we present a finger-worn device, FingerReader, that assists blind users with reading printed text on the go. We introduce a novel computer vision algorithm for local-sequential text scanning that enables reading single lines, blocks of text or skimming the text with complementary, multimodal feedback. This system is implemented in a small finger-worn form factor, that enables a more manageable eyes-free operation with trivial setup. We offer findings from three studies performed to determine the usability of the FingerReader.","Roy Shilkrot, Jochen Huber, Wong Meng Ee, Pattie Maes, Suranga Nanayakkara","assistive technology, text reading, wearable interface",2363,2372
10.1145/3173574.3173799,CHI,2018,Slacktivists or Activists?,Identity Work in the Virtual Disability March,"Protests are important social forms of activism, but can be inaccessible to people with disabilities. Online activism, like the 2017 Disability March, has provided alternative venues for involvement in accessible protesting and social movements. In this study, we use identity theory as a lens to understand why and how disabled activists engaged in an online movement, and its impact on their self-concepts. We interviewed 18 disabled activists about their experiences with online protesting during the Disability March. Respondents' identities (as both disabled individuals and as activists) led them to organize or join the March, evolved alongside the group's actions, and were reprioritized or strained as a result of their involvement. Our findings describe the values and limitations of this activism to our respondents, highlight the tensions they perceived about their activist identities, and present opportunities to support further accessibility and identity changes by integrating technology into their activist experiences.","Hanlin Li, Disha Bora, Sagar Salvi, Erin Brady","accessibility, activism, identity theory, social media",1,13
10.1145/2702123.2702471,CHI,2015,Designing Autism Research for Maximum Impact, ,"In recent decades, rates of autism spectrum disorder (ASD) have risen dramatically, and research into assistive technologies for this population has similarly escalated. For technology to be adopted, technologists need to communicate with practitioners across fields and match methodological and evaluation standards. We provide a set of recommendations for researchers to bridge the gap between fields and maximize the impact of their research, including instructions on how to identify and describe research participants and how to avoid research confounds and challenges specific to this population. We also advocate that researchers in ASD maintain a nimble, adaptable approach when performing experiments.","Elizabeth Carter, Jennifer Hyde","asd, autism, methodology, user studies",2801,2804
10.1145/3290605.3300738,CHI,2019,Guideline-Based Evaluation of Web Readability, ,"Effortless reading remains an issue for many Web users, despite a large number of readability guidelines available to designers. This paper presents a study of manual and automatic use of 39 readability guidelines in webpage evaluation. The study collected the ground-truth readability for a set of 50 webpages using eye-tracking with average and dyslexic readers (n = 79). It then matched the ground truth against human-based (n = 35) and automatic evaluations. The results validated 22 guidelines as being connected to readability. The comparison between human-based and automatic results also revealed a complex framework: algorithms were better or as good as human experts at evaluating webpages on specific guidelines - particularly those about low-level features of webpage legibility and text formatting. However, multiple guidelines still required a human judgment related to understanding and interpreting webpage content. These results contribute a guideline categorization laying the ground for future design evaluation methods.","Aliaksei Miniukovich, Michele Scaltritti, Simone Sulpizio, Antonella De Angeli","accessibility, design guidelines, user experience, wcag 2.1, web design",1,12
10.1145/3290605.3300436,CHI,2019,Editing Spatial Layouts through Tactile Templates for People with Visual Impairments, ,"Spatial layout is a key component in graphic design. While people who are blind or visually impaired (BVI) can use screen readers or magnifiers to access digital content, these tools fail to fully communicate the content's graphic design information. Through semi-structured interviews and contextual inquiries, we identify the lack of this information and feedback as major challenges in understanding and editing layouts. Guided by these insights and a co-design process with a blind hobbyist web developer, we developed an interactive, multimodal authoring tool that lets blind people understand spatial relationships between elements and modify layout templates. Our tool automatically generates tactile print-outs of a web page's layout, which users overlay on top of a tablet that runs our self-voicing digital design tool. We conclude with design considerations grounded in user feedback for improving the accessibility of spatially encoded information and developing tools for BVI authors.","Jingyi Li, Son Kim, Joshua Miele, Maneesh Agrawala, Sean Follmer","accessibility, accessible design tools, accessible web design, blindness, layout design, multimodal interfaces, tactile overlays, templates, visual impairments",1,11
10.1145/2470654.2470744,CHI,2013,Combining crowdsourcing and google street view to identify street-level accessibility problems, ,"Poorly maintained sidewalks, missing curb ramps, and other obstacles pose considerable accessibility challenges; however, there are currently few, if any, mechanisms to determine accessible areas of a city <i>a priori</i>. In this paper, we investigate the feasibility of using untrained crowd workers from Amazon Mechanical Turk (turkers) to find, label, and assess sidewalk accessibility problems in Google Street View imagery. We report on two studies: Study 1 examines the feasibility of this labeling task with six dedicated labelers including three wheelchair users; Study 2 investigates the comparative performance of turkers. In all, we collected 13,379 labels and 19,189 verification labels from a total of 402 turkers. We show that turkers are capable of determining the presence of an accessibility problem with 81% accuracy. With simple quality control methods, this number increases to 93%. Our work demonstrates a promising new, highly scalable method for acquiring knowledge about sidewalk accessibility.","Kotaro Hara, Vicki Le, Jon Froehlich","accessible urban navigation, crowdsourcing accessibility, google street view, image labeling, mechanical turk",631,640
10.1145/2470654.2470704,CHI,2013,Access lens,a gesture-based screen reader for real-world documents,"Gesture-based touch screen user interfaces, when designed to be accessible to blind users, can be an effective mode of interaction for those users. However, current accessible touch screen interaction techniques suffer from one serious limitation: they are only usable on devices that have been explicitly designed to support them. Access Lens is a new interaction method that uses computer vision-based gesture tracking to enable blind people to use accessible gestures on paper documents and other physical objects, such as product packages, device screens, and home appliances. This paper describes the development of Access Lens hardware and software, the iterative design of Access Lens in collaboration with blind computer users, and opportunities for future development.","Shaun Kane, Brian Frey, Jacob Wobbrock","accessibility, augmented reality, blindness, computer vision, gestures",347,350
10.1145/3173574.3174092,CHI,2018,Caption Crawler,Enabling Reusable Alternative Text Descriptions using Reverse Image Search,"Accessing images online is often difficult for users with vision impairments. This population relies on text descriptions of images that vary based on website authors' accessibility practices. Where one author might provide a descriptive caption for an image, another might provide no caption for the same image, leading to inconsistent experiences. In this work, we present the Caption Crawler system, which uses reverse image search to find existing captions on the web and make them accessible to a user's screen reader. We report our system's performance on a set of 481 websites from alexa.com's list of most popular sites to estimate caption coverage and latency, and also report blind and sighted users' ratings of our system's output quality. Finally, we conducted a user study with fourteen screen reader users to examine how the system might be used for personal browsing.","Darren Guinness, Edward Cutrell, Meredith Morris","accessibility, alt text, alternative text, image captioning, screen readers, vision impairment",1,11
10.1145/3290605.3300880,CHI,2019,Co-Created Personas,Engaging and Empowering Users with Diverse Needs Within the Design Process,"Personas are powerful tools for designing technology and envisioning its usage. They are widely used to imagine archetypal users around whom to orient design work. We have been exploring co-created personas as a technique to use in co-design with users who have diverse needs. Our vision was that this would broaden the demographic and liberate co-designers of their personal relationship with a health condition. This paper reports three studies where we investigated using co-created personas with people who had Parkinson's disease, dementia or aphasia. Observational data of co-design sessions were collected and analysed. Findings revealed that the co-created personas encouraged users with diverse needs to engage with co-designing. Importantly, they also afforded additional benefits including empowering users within a more accessible design process. Reflecting on the outcomes from the different user groups, we conclude with a discussion of the potential for co-created personas to be applied more broadly.","Timothy Neate, Aikaterini Bourazeri, Abi Roper, Simone Stumpf, Stephanie Wilson","aphasia, co-created personas, co-design, dementia, design, healthcare, parkinson's disease, vulnerable users",1,12
10.1145/3290605.3300289,CHI,2019,An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound, ,"We compared four audio-based radar metaphors for providing directional stimuli to users of AR headsets. The metaphors are clock face, compass, white noise, and scale. Each metaphor, or method, signals the movement of a virtual arm in a radar sweep. In a user study, statistically significant differences were observed for accuracy and response time. Beat-based methods (clock face, compass) elicited responses biased to the left of the stimulus location, and non-beat-based methods (white noise, scale) produced responses biased to the right of the stimulus location. The beat methods were more accurate than the non-beat methods. However, the non-beat methods elicited quicker responses. We also discuss how response accuracy varies along the radar sweep between methods. These observations contribute design insights for non-verbal, non-visual directional prompting.","Brendan Cassidy, Janet Read, I. MacKenzie","accessibility, augmented reality, directional prompting, headset, radar, spatial audio, visual impairment",1,8
10.1145/3173574.3174018,CHI,2018,A Large Inclusive Study of Human Listening Rates, ,"As conversational agents and digital assistants become increasingly pervasive, understanding their synthetic speech becomes increasingly important. Simultaneously, speech synthesis is becoming more sophisticated and manipulable, providing the opportunity to optimize speech rate to save users time. However, little is known about people's abilities to understand fast speech. In this work, we provide the first large-scale study on human listening rates. Run on LabintheWild, it used volunteer participants, was screen reader accessible, and measured listening rate by accuracy at answering questions spoken by a screen reader at various rates. Our results show that blind and low-vision people, who often rely on audio cues and access text aurally, generally have higher listening rates than sighted people. The findings also suggest a need to expand the range of rates available on personal devices. These results demonstrate the potential for users to learn to listen to faster rates, expanding the possibilities for human-conversational agent interaction.","Danielle Bragg, Cynthia Bennett, Katharina Reinecke, Richard Ladner","accessibility, blind, crowdsourcing, human abilities, listening rate, synthetic speech, visually impaired",1,12
10.1145/3290605.3300860,CHI,2019,JourneyCam,Exploring Experiences of Accessibility and Mobility among Powered Wheelchair Users through Video and Data,"Recent HCI research has investigated how digital technologies might enable citizens to identify and express matters of civic concern. We extend this work by describing JourneyCam, a smartphone-based system that enables powered wheelchair users to capture video and sensor data about their experiences of mobility. Thirteen participants used JourneyCam to document journeys, after which the data they collected was used to support discussions around their experiences. Our findings highlight how the system facilitated the articulation of complex embodied experiences, and how the collected data might have particular value in surfacing these experiences to help inform urban design and policymaking. Participants valued the ways in which JourneyCam's moving image and sensor data made hard-to-express sensations apparent, as well as how it enabled them to surface previously unrecognised issues. We conclude by highlighting future opportunities for how such tools might enable citizens to inform and influence civic governance.","Sunil Rodger, Dan Jackson, John Vines, Janice McLaughlin, Peter Wright","accessibility, civic technology, community technology, digital civics, disability, mobility, place, powered wheelchair users",1,15
10.1145/3173574.3174094,CHI,2018,Understanding the Accessibility of Smartphone Photography for People with Motor Impairments, ,"We present the results of an exploration to understand the accessibility of smartphone photography for people with motor impairments. We surveyed forty-six people and interviewed twelve people about capturing, editing, and sharing photographs on smartphones. We found that people with motor impairments encounter many challenges with smartphone photography, resulting in users capturing fewer photographs than they would like. Participants described various strategies they used to overcome challenges in order to capture a quality photograph. We also found that photograph quality plays a large role in deciding which photographs users share and how often they share, with most participants rating their photographs as average or poor quality compared to photos shared on their social networks. Additionally, we created design probes of two novel photography interfaces and received feedback from our interview participants about their usefulness and functionality. Based on our findings, we propose design recommendations for how to improve the accessibility of mobile photoware for people with motor impairments.","Martez Mott, Jane E., Cynthia Bennett, Edward Cutrell, Meredith Morris","accessibility, camera phone, mobile, motor impairment, photo editing, photo sharing, photography, smartphone",1,12
10.1145/2702123.2702342,CHI,2015,Being Seen,Co-Interpreting Parkinson's Patient's Movement Ability in Deep Brain Stimulation Programming,"The purpose of this study is to address the use of movement assessment sensors for clinical diagnosis and treatment. Eleven patients with Parkinson's disease who had under-gone deep brain stimulation (DBS) surgery were observed during follow-up appointments for adjustments to the stimulation settings. We examine the ways in which the patients and clinicians assess movement ability together in the clinic and how these assessments relate to the treatment of functional disability through DBS. We have found that effective assessment of movement and treatment efficacy is a collaborative and interpretive process (co-interpretation) that relies on input from patients, clinicians, and caregivers. From these findings we describe the design directions for movement sensors to support co-interpretation of movement in a clinical context as opposed to simply movement definition.","Helena Mentis, Rita Shewbridge, Sharon Powell, Paul Fishman, Lisa Shulman","co-interpretation, communication, health, movement, wearable sensors",511,520
10.1145/3025453.3025517,CHI,2017,"Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times", ,"We present <i>cascading dwell gaze typing</i>, a novel approach to dwell-based eye typing that dynamically adjusts the dwell time of keys in an on-screen keyboard based on the likelihood that a key will be selected next, and the location of the key on the keyboard. Our approach makes unlikely keys more difficult to select and likely keys easier to select by increasing and decreasing their required dwell times, respectively. To maintain a smooth typing rhythm for the user, we <i>cascade</i> the dwell time of likely keys, slowly decreasing the minimum allowable dwell time as a user enters text. Cascading the dwell time affords users the benefits of faster dwell times while causing little disruption to users' typing cadence. Results from a longitudinal study with 17 non-disabled participants show that our dynamic cascading dwell technique was significantly faster than a static dwell approach. Participants were able to achieve typing speeds of 12.39 WPM on average with our cascading technique, whereas participants were able to achieve typing speeds of 10.62 WPM on average with a static dwell time approach. In a small evaluation conducted with five people with ALS, participants achieved average typing speeds of 9.51 WPM with our cascading dwell approach. These results show that our dynamic cascading dwell technique has the potential to improve gaze typing for users with and without disabilities.","Martez Mott, Shane Williams, Jacob Wobbrock, Meredith Morris","accessibility, eye typing, gaze typing, text entry",2558,2570
10.1145/3173574.3173892,CHI,2018,Speak Up,A Multi-Year Deployment of Games to Motivate Speech Therapy in India,"The ability to communicate is crucial to leading an independent life. Unfortunately, individuals from developing communities who are deaf and hard of hearing tend to encounter difficulty communicating, due to a lack of educational resources. We present findings from a two-year deployment of Speak Up, a suite of voice-powered games to motivate speech therapy, at a school for the deaf in India. Using ethnographic methods, we investigated the interplay between Speak Up and local educational practices. We found that teachers' speech therapy goals had evolved to differ from those encoded in the games, that the games influenced classroom dynamics, and that teachers had improved their computer literacy and developed creative uses for the games. We used these insights to further enhance Speak Up by creating an explicit teacher role in the games, making changes that encouraged teachers to build their computer literacy, and adding an embodied agent.","Amal Nanavati, M. Dias, Aaron Steinfeld","assistive technologies, capacity building, ethnography, ictd, speech therapy",1,12
10.1145/3025453.3025949,CHI,2017,Understanding Low Vision People's Visual Perception on Commercial Augmented Reality Glasses, ,"People with low vision have a visual impairment that affects their ability to perform daily activities. Unlike blind people, low vision people have functional vision and can potentially benefit from smart glasses that provide dynamic, always-available visual information. We sought to determine what low vision people could see on mainstream commercial augmented reality (AR) glasses, despite their visual limitations and the device's constraints. We conducted a study with 20 low vision participants and 18 sighted controls, asking them to identify virtual shapes and text in different sizes, colors, and thicknesses. We also evaluated their ability to see the virtual elements while walking. We found that low vision participants were able to identify basic shapes and read short phrases on the glasses while sitting and walking. Identifying virtual elements had a similar effect on low vision and sighted people's walking speed, slowing it down slightly. Our study yielded preliminary evidence that mainstream AR glasses can be powerful accessibility tools. We derive guidelines for presenting visual output for low vision people and discuss opportunities for accessibility applications on this platform.","Yuhang Zhao, Michele Hu, Shafeka Hashash, Shiri Azenkot","accessibility, augmented reality, low vision, user study",4170,4181
10.1145/2858036.2858198,CHI,2016,"""Why would anybody do this?""",Understanding Older Adults' Motivations and Challenges in Crowd Work,"Diversifying participation in crowd work can benefit the worker and requester. Increasing numbers of older adults are online, but little is known about their awareness of or how they engage in mainstream crowd work. Through an online survey with 505 seniors, we found that most have never heard of crowd work but would be motivated to complete tasks by earning money or working on interesting or stimulating tasks. We follow up results from the survey with interviews and observations of 14 older adults completing crowd work tasks. While our survey data suggests that financial incentives are encouraging, in-depth interviews reveal that a combination of personal and social incentives may be stronger drivers of participation, but only if older adults can overcome accessibility issues and understand the purpose of crowd work. This paper contributes insights into how crowdsourcing sites could better engage seniors and other users.","Robin Brewer, Meredith Morris, Anne Marie Piper","crowdsourcing, motivation, older adults, online work",2246,2257
10.1145/2858036.2858130,CHI,2016,The AT Effect,How Disability Affects the Perceived Social Acceptability of Head-Mounted Display Use,"Wearable computing devices offer new possibilities to increase accessibility and independence for individuals with disabilities. However, the adoption of such devices may be influenced by social factors, and useful devices may not be adopted if they are considered inappropriate to use. While public policy may adapt to support accommodations for assistive technology, emerging technologies may be unfamiliar or unaccepted by bystanders. We surveyed 1200 individuals about the use of a head-mounted display in a public setting, examining how information about the user's disability affected judgments of the social acceptability of the scenario. Our findings reveal that observers considered head-mounted display use more socially acceptable if the device was being used to support a person with a disability.","Halley Profita, Reem Albaghli, Leah Findlater, Paul Jaeger, Shaun Kane","assistive technology, social perceptions, wearable computing",4884,4895
10.1145/3025453.3025560,CHI,2017,A Framework for Speechreading Acquisition Tools, ,"At least 360 million people worldwide have disabling hearing loss that frequently causes difficulties in day-to-day conversations. Traditional technology (e.g., hearing aids) often fails to offer enough value, has low adoption rates, and can result in social stigma. Speechreading can dramatically improve conversational understanding, but speechreading is a skill that can be challenging to learn. To address this, we developed a novel speechreading acquisition framework that can be used to design Speechreading Acquisition Tools (SATs) - a new type of technology to improve speechreading acquisition. We interviewed seven speechreading tutors and used thematic analysis to identify and organise the key elements of our framework. We then evaluated our framework by using it to: 1) categorise every tutor-identified speechreading teaching technique, 2) critically evaluate existing conversational aids, and 3) design three new SATs. Through the use of SATs designed using our framework, the speechreading abilities of people with hearing loss around the world should be enhanced, thereby improving the conversational foundation of their day-to-day lives.","Benjamin Gorman, David Flatla","deafness, hearing loss, lipreading, speechreading",519,530
10.1145/2702123.2702525,CHI,2015,Sharing is Caring,Assistive Technology Designs on Thingiverse,"An increasing number of online communities support the open-source sharing of designs that can be built using rapid prototyping to construct physical objects. In this paper, we examine the designs and motivations for assistive technology found on Thingiverse.com, the largest of these communities at the time of this writing. We present results from a survey of all assistive technology that has been posted to Thingiverse since 2008 and a questionnaire distributed to the designers exploring their relationship with assistive technology and the motivation for creating these designs. The majority of these designs are intended to be manufactured on a 3D printer and include assistive devices and modifications for individuals with disabilities, older adults, and medication management. Many of these designs are created by the end-users themselves or on behalf of friends and loved ones. These designers frequently have no formal training or expertise in the creation of assistive technology. This paper discusses trends within this community as well as future opportunities and challenges.","Erin Buehler, Stacy Branham, Abdullah Ali, Jeremy Chang, Megan Hofmann, Amy Hurst, Shaun Kane","3d printing, assistive technology, design, disability, open-source, personal-scale fabrication, prototyping",525,534
10.1145/2470654.2466210,CHI,2013,Use of an agile bridge in the development of assistive technology, ,"Engaging with end users in the development of assistive technologies remains one of the major challenges for researchers and developers in the field of accessibility and HCI. Developing usable software systems for people with complex disabilities is problematic, software developers are wary of using user-centred design, one of the main methods by which usability can be improved, due to concerns about how best to work with adults with complex disabilities, in particular Severe Speech and Physical Impairments (SSPI) and how to involve them in research. This paper reports on how the adoption of an adapted agile approach involving the incorporation of a user advocate on the research team helped in meeting this challenge in one software project and offers suggestions for how this could be used by other development teams.","Suzanne Prior, Annalu Waller, Rolf Black, Thilo Kroll","agile methodology, assistive technology, severe speech and physical impairments, user centred design",1579,1588
10.1145/1753326.1753569,CHI,2010,vSked,evaluation of a system to support classroom activities for children with autism,"Visual schedules--the use of symbols to represent a series of activities or steps--have been successfully used by caregivers to help children with autism to understand, structure, and predict activities in their daily lives. Building from in-depth fieldwork and participatory design sessions, we developed vSked, an interactive and collaborative visual scheduling system designed for elementary school classrooms. We evaluated vSked in situ in one autism-specific classroom over three weeks. In this paper, we present the design principles, technical solution, and results from this successful deployment. Use of vSked resulted in reductions in staff effort required to use visual supports. vSked also resulted in improvements in the perceived quality and quantity of communication and social interactions in the classroom.","Sen Hirano, Michael Yeganyan, Gabriela Marcu, David Nguyen, Lou Anne Boyd, Gillian Hayes","assistive technology, autism, education, visual supports",1633,1642
10.1145/2858036.2858241,CHI,2016,"""I Always Wanted to See the Night Sky""",Blind User Preferences for Sensory Substitution Devices,"Sensory Substitution Devices (SSDs) convert visual information into another sensory channel (e.g. sound) to improve the everyday functioning of blind and visually impaired persons (BVIP). However, the range of possible functions and options for translating vision into sound is largely open-ended. To provide constraints on the design of this technology, we interviewed ten BVIPs who were briefly trained in the use of three novel devices that, collectively, showcase a large range of design permutations. The SSDs include the 'Depth-vOICe,' 'Synaestheatre' and 'Creole' that offer high spatial, temporal, and colour resolutions respectively via a variety of sound outputs (electronic tones, instruments, vocals). The participants identified a range of practical concerns in relation to the devices (e.g. curb detection, recognition, mental effort) but also highlighted experiential aspects. This included both curiosity about the visual world (e.g. understanding shades of colour, the shape of cars, seeing the night sky) and the desire for the substituting sound to be responsive to movement of the device and aesthetically engaging.","Giles Hamilton-Fletcher, Marianna Obrist, Phil Watten, Michele Mengucci, Jamie Ward","SSD, blind, colour, depth, design, experience, hearing, sensory substitution, sound, vision, visually impaired",2162,2174
10.1145/3290605.3300346,CHI,2019,Playing Blind,Revealing the World of Gamers with Visual Impairment,"Previous research on games for people with visual impairment (PVI) has focused on co-designing or evaluating specific games - mostly under controlled conditions. In this research, we follow a game-agnostic, ""in-the-wild"" approach, investigating the habits, opinions and concerns of PVI regarding digital games. To explore these issues, we conducted an online survey and follow-up interviews with gamers with VI (GVI). Dominant themes from our analysis include the particular appeal of digital games to GVI, the importance of social trajectories and histories of gameplay, the need to balance complexity and accessibility in both games targeted to PVI and mainstream games, opinions about the state of the gaming industry, and accessibility concerns around new and emerging technologies such as VR and AR. Our study gives voice to an underrepresented group in the gaming community. Understanding the practices, experiences and motivations of GVI provides a valuable foundation for informing development of more inclusive games.","Ronny Andrade, Melissa Rogerson, Jenny Waycott, Steven Baker, Frank Vetere","audiogames, digital games, empowerment, visual impairment",1,14
10.1145/3173574.3174090,CHI,2018,The RAD,Making Racing Games Equivalently Accessible to People Who Are Blind,"We introduce the <i>racing auditory display (RAD)</i>, an audio-based user interface that allows players who are blind to play the same types of racing games that sighted players can play with an efficiency and sense of control that are similar to what sighted players have. The RAD works with a standard pair of headphones and comprises two novel sonification techniques: the <i>sound slider</i> for understanding a car's speed and trajectory on a racetrack and the <i>turn indicator system</i> for alerting players of the direction, sharpness, length, and timing of upcoming turns. In a user study with 15 participants (3 blind; the rest blindfolded and analyzed separately), we found that players preferred the RAD's interface over that of <i>Mach 1</i>, a popular blind-accessible racing game. We also found that the RAD allows an avid gamer who is blind to race as well on a complex racetrack as casual sighted players can, without a significant difference between lap times or driving paths.","Brian Smith, Shree Nayar","accessibility, accessible games, audio games, sonification",1,12
10.1145/1978942.1979424,CHI,2011,Enhancing independence and safety for blind and deaf-blind public transit riders, ,"Blind and deaf-blind people often rely on public transit for everyday mobility, but using transit can be challenging for them. We conducted semi-structured interviews with 13 blind and deaf-blind people to understand how they use public transit and what human values were important to them in this domain. Two key values were identified: <i>independence</i> and <i>safety</i>. We developed <i>GoBraille</i>, two related Braille-based applications that provide information about buses and bus stops while supporting the key values. GoBraille is built on <i>MoBraille</i>, a novel framework that enables a Braille display to benefit from many features in a smartphone without knowledge of proprietary, device-specific protocols. Finally, we conducted user studies with blind people to demonstrate that GoBraille enables people to travel more independently and safely. We also conducted co-design with a deaf-blind person, finding that a minimalist interface, with short input and output messages, was most effective for this population.","Shiri Azenkot, Sanjana Prasain, Alan Borning, Emily Fortuna, Richard Ladner, Jacob Wobbrock","accessibility, autonomy, blind, deaf-blind, public transit usability, safety, value sensitive design",3247,3256
10.1145/3173574.3173924,CHI,2018,Assisting Students with Intellectual and Developmental Disabilities in Inclusive Education with Smartwatches, ,"Smartwatches have a large potential to support everyday activities. However, their potential as assistive technologies in inclusive academic environments is unclear. To investigate how smartwatches can support students with intellectual and developmental disabilities (IDDs) to perform activities that require emotional and behavioral skills and involve communication, collaboration and planning, we implemented WELI. WELI (Wearable Life) is a wearable application designed to assist young adults with IDDs attending a postsecondary education program. This paper reports on the user-centric design process adopted in the development of WELI, and describes how smartwatches can assist students with IDDs in special education. The results reported are drawn from 8 user studies with 58 participants in total. WELI features include behavioral intervention, mood regulation, reminders, checklists, surveys and rewards. Results indicate that several considerations must be taken into account when designing for students with IDD, and that overall the students are enthusiastic about adopting an innovative smartwatch application in class, as they reacted positively about the technology and features provided.","Hui Zheng, Vivian Genaro Motti","assistive technologies, intellectual and developmental disabilities(idds), mobile, smartwatches, wearables",1,12
10.1145/3025453.3025904,CHI,2017,"Designing for the ""Universe of One""",Personalized Interactive Media Systems for People with the Severe Cognitive Impairment Associated with Rett Syndrome,"The needs and capabilities of a person with severe disabilities are often so specific that designing for the person is like designing for a ""universe of one."" This project addresses this problem for women with Rett syndrome, a disorder accompanied by severe cognitive, communication, and motor impairment. The research team adapted participatory design techniques to work with five such women, and their families, to design and evaluate new assistive technology for these women. The process suggests a class of media-playing devices that would be generally useful to women with Rett syndrome: systems that can load multiple audio or video segments; be activated by many different switches; and respond instantly to switch-hits. As well, the systems should permit a caregiver to set the start and end time of each segment, and how the system advances through a sequence of segments. The paper also discusses patterns that were observed when collaborating with the families. For example, parents shared longstanding but untried ideas for new assistive technology; and expressed a strong interest in any device that would help their daughters do things for themselves.","Anthony Hornof, Haley Whitman, Marah Sutherland, Samuel Gerendasy, Joanna McGrenere","assistive technology, intellectual disability, participatory design, rett syndrome, severe cognitive impairment, user observation studies, user training, user-centered design",2137,2148
10.1145/3025453.3025582,CHI,2017,Identifying how Visually Impaired People Explore Raised-line Diagrams to Improve the Design of Touch Interfaces, ,"Raised-line diagrams are widely used by visually impaired (VI) people to read maps, drawings or graphs. While previous work has identified general exploration strategies for raised-line drawings, we have limited knowledge on how this exploration is performed in detail and how it extends to other types of diagrams such as maps or graphs, frequently used in specialized schools. Such information can be crucial for the design of accessible interfaces on touchscreens. We conducted a study in which participants were asked to explore five types of raised-line diagrams (common drawings, perspective drawings, mathematical graphs, neighborhood maps, and geographical maps) while tracking both hands fingers. Relying on a first set of results, we proposed a set of design guidelines for touch interfaces.","Sandra Bardot, Marcos Serrano, Bernard Oriola, Christophe Jouffrais","bimanual exploration, blind, finger tracking, raised-line diagram, tactile drawings, tactile exploration, tactile maps",550,555
10.1145/3173574.3174033,CHI,2018,"""Accessibility Came by Accident""",Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities,"From an accessibility perspective, voice-controlled, home-based intelligent personal assistants (IPAs) have the potential to greatly expand speech interaction beyond dictation and screen reader output. To examine the accessibility of off-the-shelf IPAs (e.g., Amazon Echo) and to understand how users with disabilities are making use of these devices, we conducted two exploratory studies. The first, broader study is a content analysis of 346 Amazon Echo reviews that include users with disabilities, while the second study more specifically focuses on users with visual impairments, through interviews with 16 current users of home-based IPAs. Findings show that, although some accessibility challenges exist, users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Richer voice-based applications and solutions to support discoverability would be particularly useful to users with visual impairments. These findings should inform future work on accessible voice-based IPAs.","Alisha Pradhan, Kanika Mehta, Leah Findlater","accessibility, conversational interfaces, disability, intelligent personal assistants, speech",1,13
10.1145/3290605.3300907,CHI,2019,Sound Forest,Evaluation of an Accessible Multisensory Music Installation,"Sound Forest is a music installation consisting of a room with light-emitting interactive strings, vibrating platforms and speakers, situated at the Swedish Museum of Performing Arts. In this paper we present an exploratory study focusing on evaluation of Sound Forest based on picture cards and interviews. Since Sound Forest should be accessible for everyone, regardless age or abilities, we invited children, teens and adults with physical and intellectual disabilities to take part in the evaluation. The main contribution of this work lies in its findings suggesting that multisensory platforms such as Sound Forest, providing whole-body vibrations, can be used to provide visitors of different ages and abilities with similar associations to musical experiences. Interviews also revealed positive responses to haptic feedback in this context. Participants of different ages used different strategies and bodily modes of interaction in Sound Forest, with activities ranging from running to synchronized music-making and collaborative play.","Emma Frid, Hans Lindetorp, Kjetil Hansen, Ludvig Elblaus, Roberto Bresin","accessible digital musical instruments, evaluation of music systems, haptic feedback, music installations, music production",1,12
10.1145/3173574.3174044,CHI,2018,Weaving Lighthouses and Stitching Stories,Blind and Visually Impaired People Designing E-textiles,"We describe our experience of working with blind and visually impaired people to create interactive art objects that are personal to them, through a participatory making process using electronic textiles (e-textiles) and hands-on crafting techniques. The research addresses both the practical considerations about how to structure hands-on making workshops in a way which is accessible to participants of varying experience and abilities, and how effective the approach was in enabling participants to tell their own stories and feel in control of the design and making process. The results of our analysis is the offering of insights in how to run e-textile making sessions in such a way for them to be more accessible and inclusive to a wider community of participants.","Emilie Giles, Janet van der Linden, Marian Petre","crafting, creativity, e-textiles, participatory making, story-telling, touch-based interaction, visual impairment",1,12
10.1145/1978942.1979237,CHI,2011,Handscope,enabling blind people to experience statistical graphics on websites through haptics,"Statistical graphics on the web such as a tag cloud visually represent statistical data which are generated by website users. While sighted people can scan the latest information through the dynamic changes of statistical graphics, blind people, who cannot perceive them, lose opportunities to keep up to date in this quickly-changing society. In order to enable blind people to experience socially-generated statistical graphics, we propose a new assistive device, namely, Handscope, which translates statistical graphics on websites into simple height changes of its haptic pole. We conducted a two-phase user study with blind people in order to test its usability and explore its effects on the quality of blind users' web experiences. The results show the meaningful contribution of Handscope in extending the area of blind people's web experiences.","Da-jung Kim, Youn-kyung Lim","blind users, haptic, statistical graphics, web accessibility",2039,2042
10.1145/1978942.1979044,CHI,2011,In the shadow of misperception,assistive technology use and social interactions,"Few research studies focus on how the use of assistive technologies is affected by social interaction among people. We present an interview study of 20 individuals to determine how assistive technology use is affected by social and professional contexts and interactions. We found that specific assistive devices sometimes marked their users as having disabilities; that functional access took priority over feeling self-conscious when using assistive technologies; and that two misperceptions pervaded assistive technology use: (1) that assistive devices could functionally eliminate a disability, and (2) that people with disabilities would be helpless without their devices. Our findings provide further evidence that accessibility should be built into mainstream technologies. When this is not feasible, assistive devices should incorporate cutting edge technologies and strive to be designed for social acceptability, a new design approach we propose here.","Kristen Shinohara, Jacob Wobbrock","accessibility, assistive devices, interface design, product design, social interactions, stigma",705,714
10.1145/1753326.1753461,CHI,2010,Exploring the accessibility and appeal of surface computing for older adult health care support, ,"This paper examines accessibility issues of surface computing with older adults and explores the appeal of surface computing for health care support. We present results from a study involving 20 older adults (age 60 to 88) performing gesture-based interactions on a multitouch surface. Older adults were able to successfully perform all actions on the surface computer, but some gestures that required two fingers (resize) and fine motor movement (rotate) were problematic. Ratings for ease of use and ease of performing each action as well as time required to figure out an action were similar to that of younger adults. Older adults reported that the surface computer was less intimidating, less frustrating, and less overwhelming than a traditional computer. The idea of using a surface computer for health care support was well-received by participants. We conclude with a discussion of design issues involving surface computing for older adults and use of this technology for health care.","Anne Piper, Ross Campbell, James Hollan","health care, multitouch, older adults, surface computing",907,916
10.1145/2702123.2702261,CHI,2015,Participatory Design of Therapeutic Video Games for Young People with Neurological Vision Impairment, ,"Neurological Vision Impairment (NVI) detrimentally impacts upon quality of life, as daily activities such as reading and crossing the road often become significantly impaired. Therapy strategies for NVI based on visual scanning of on-screen stimuli have recently been demonstrated as effective at improving functional vision. However, these strategies are repetitive, monotonous and unsuitable for use with children and young adults. This project explores the design of a game-based therapy programme that aims to support participant engagement and adherence. We first outline requirements for this software, before reporting on the iterative design process undertaken in collaboration with young people, therapists and teachers at a centre for vision impairment. Our work provides insights into the participatory design of games in collaboration with young people with special needs, and reflects upon the tension of balancing game challenge, therapy goals, and accessibility. Furthermore, it highlights the potential of games to empower special populations by providing a medium through which to communicate the subjective experience of specific impairments.","Jonathan Waddington, Conor Linehan, Kathrin Gerling, Kieran Hicks, Timothy Hodgson","games, rehabilitation, therapy, vision, young people",3533,3542
10.1145/2702123.2702484,CHI,2015,Designing Conversation Cues on a Head-Worn Display to Support Persons with Aphasia, ,"Symbol-based dictionaries of text, images and sound can help individuals with aphasia find the words they need, but are often seen as a last resort because they tend to replace rather than augment the user's natural speech. Through two design investigations, we explore head-worn displays as a means of providing unobtrusive, always-available, and glanceable vocabulary support. The first study used narrative storyboards as a design probe to explore the potential benefits and challenges of a head-worn approach over traditional augmented alternative communication (AAC) tools. The second study then evaluated a proof-of-concept prototype in both a lab setting with the researcher and in situ with unfamiliar conversation partners at a local market. Findings suggest that a head-worn approach could better allow wearers to maintain focus on the conversation, reduce reliance on the availability of external tools (e.g., paper and pen) or people, and minimize visibility of the support by others. These studies should motivate further investigation of head-worn conversational support.","Kristin Williams, Karyn Moffatt, Denise McCall, Leah Findlater","accessibility, aphasia, conversational support, head-worn display, wearable computing",231,240
10.1145/3290605.3300654,CHI,2019,Understanding the Impact of TVIs on Technology Use and Selection by Children with Visual Impairments, ,"The use of technology in educational settings is extremely common. For many visually impaired children, educational settings are the first place they are exposed to the assistive technology that they will need to access mainstream computing devices. Current laws provide support for students to receive training from Teachers of the Visually Impaired (TVIs) on these assistive devices. Therefore, TVIs play an important role in the selection and training of technology. Through our interviews with TVIs, we discovered the factors that impact which technologies they select, how they attempt to mitigate the stigma associated with certain technologies, and the challenges that students face in learning assistive technologies. Through this research, we identified three needs that future research on assistive technology should address: (1) increasing focus on built-in accessibility features, (2) providing support for independent learning and exploration, and (3) creating technologies that can support users with progressive vision loss.","Catherine Baker, Lauren Milne, Richard Ladner","assistive technology, children with visual impairments, teachers of the visually impaired",1,13
10.1145/3173574.3174192,CHI,2018,CodeTalk,Improving Programming Environment Accessibility for Visually Impaired Developers,"In recent times, programming environments like Visual Studio are widely used to enhance programmer productivity. However, inadequate accessibility prevents Visually Impaired (VI) developers from taking full advantage of these environments. In this paper, we focus on the accessibility challenges faced by the VI developers in using Graphical User Interface (GUI) based programming environments. Based on a survey of VI developers and based on two of the authors' personal experiences, we categorize the accessibility difficulties into Discoverability, Glanceability, Navigability, and Alertability. We propose solutions to some of these challenges and implement these in CodeTalk, a plugin for Visual Studio. We show how CodeTalk improves developer experience and share promising early feedback from VI developers who used our plugin.","Venkatesh Potluri, Priyan Vaithilingam, Suresh Iyengar, Y. Vidya, Manohar Swaminathan, Gopal Srinivasa","accessibility, audio debugging, programming environments, visually impaired",1,11
10.1145/2702123.2702511,CHI,2015,Collaborative Accessibility,How Blind and Sighted Companions Co-Create Accessible Home Spaces,"In recent decades, great technological strides have been made toward enabling people who are blind to live independent, successful lives. However, there has been relatively little progress towards understanding the social, collaborative needs of this population, particularly in the domestic setting. We conducted semi-structured interviews in the homes of 10 pairs of close companions in which one partner was blind and one was not. We found that partners engaged in collaborative accessibility by taking active roles in co-creating an accessible environment. Due to their different visual abilities, however, partners sometimes encountered difficulties managing divergent needs and engaging in shared experiences. We describe outstanding challenges to creating accessible shared home spaces and outline new research and technology opportunities for supporting collaborative accessibility in the home.","Stacy Branham, Shaun Kane","accessibility, blindness, collaboration, home, interpersonal relationships, vision impairment",2373,2382
10.1145/3025453.3025899,CHI,2017,People with Visual Impairment Training Personal Object Recognizers,Feasibility and Challenges,"Blind people often need to identify objects around them, from packages of food to items of clothing. Automatic object recognition continues to provide limited assistance in such tasks because models tend to be trained on images taken by sighted people with different background clutter, scale, viewpoints, occlusion, and image quality than in photos taken by blind users. We explore personal object recognizers, where visually impaired people train a mobile application with a few snapshots of objects of interest and provide custom labels. We adopt transfer learning with a deep learning system for user-defined multi-label k-instance classification. Experiments with blind participants demonstrate the feasibility of our approach, which reaches accuracies over 90% for some participants. We analyze user data and feedback to explore effects of sample size, photo-quality variance, and object shape; and contrast models trained on photos by blind participants to those by sighted participants and generic recognizers.","Hernisa Kacorri, Kris Kitani, Jeffrey Bigham, Chieko Asakawa","accessibility, blind, computer vision, object recognition, photographs",5839,5849
10.1145/3173574.3173972,CHI,2018,Pocket Skills,A Conversational Mobile Web App To Support Dialectical Behavioral Therapy,"Mental health disorders are a leading cause of disability worldwide. Although evidence-based psychotherapy is effective, engagement from such programs can be low. Mobile apps have the potential to help engage and support people in their therapy. We developed Pocket Skills, a mobile web app based on Dialectical Behavior Therapy (DBT). Pocket Skills teaches DBT via a conversational agent modeled on Marsha Linehan, who developed DBT. We examined the feasibility of Pocket Skills in a 4-week field study with 73 individuals enrolled in psychotherapy. After the study, participants reported decreased depression and anxiety and increased DBT skills use. We present a model based on qualitative findings of how Pocket Skills supported DBT. Pocket Skills helped participants engage in their DBT and practice and implement skills in their environmental context, which enabled them to see the results of using their DBT skills and increase their self-efficacy. We discuss the design implications of these findings for future mobile mental health systems.","Jessica Schroeder, Chelsey Wilkes, Kael Rowan, Arturo Toledo, Ann Paradiso, Mary Czerwinski, Gloria Mark, Marsha Linehan","behavioral therapy, dbt, health informatics, mental health",1,15
10.1145/2702123.2702144,CHI,2015,Toward 3D-Printed Movable Tactile Pictures for Children with Visual Impairments, ,"Many children's books contain movable pictures with elements that can be physically opened, closed, pushed, pulled, spun, flipped, or swung. But these tangible, interactive reading experiences are inaccessible to children with visual impairments. This paper presents a set of 3D-printable models designed as building blocks for creating movable tactile pictures that can be touched, moved, and understood by children with visual impairments. Examples of these models are canvases, connectors, hinges, spinners, sliders, lifts, walls, and cutouts. They can be used to compose movable tactile pictures to convey a range of spatial concepts, such as in/out, up/down, and high/low. The design and development of these models were informed by three formative studies including 1) a survey on popular moving mechanisms in children's books and 3D-printed parts to implement them, 2) two workshops on the process creating movable tactile pictures by hand (e.g., Lego, Play-Doh), and 3) creation of wood-based prototypes and an informal testing on sighted preschoolers. Also, we propose a design language based on XML and CSS for specifying the content and structure of a movable tactile picture. Given a specification, our system can generate a 3D-printable model. We evaluate our approach by 1) transcribing six children's books, and 2) conducting six interviews on domain experts including four teachers for the visually impaired, one blind adult, two publishers at the National Braille Press, a renowned tactile artist, and a librarian.","Jeeeun Kim, Tom Yeh","3d modeling, 3d printing, movables, tactile pictures",2815,2824
10.1145/1753326.1753715,CHI,2010,Individual models of color differentiation to improve interpretability of information visualization, ,"Color is commonly used to represent categories and values in many computer applications, but differentiating these colors can be difficult in many situations (e.g., for users with color vision deficiency (CVD), or in bright light). Current solutions to this problem can adapt colors based on standard simulations of CVD, but these models cover only a fraction of the ways in which color perception can vary. To improve the specificity and accuracy of these approaches, we have developed the first ever individualized model of color differentiation (ICD). The model is based on a short calibration performed by a particular user for a particular display, and so automatically covers all aspects of the user's ability to see and differentiate colors in an environment. In this paper we introduce the new model and the manner in which differentiability limits are predicted. We gathered empirical data from 16 users to assess the model's accuracy and robustness. We found that the model is highly effective at capturing individual differentiation abilities, works for users with and without CVD, can be tuned to balance accuracy and color availability, and can serve as the basis for improved color adaptation schemes.","David Flatla, Carl Gutwin","assistive technology, color blindness, color differentiation, color vision deficiency, visualization",2563,2572
10.1145/3025453.3025493,CHI,2017,"Understanding the Role Fluidity of Stakeholders During Assistive Technology Research ""In the Wild""", ,"Deploying novel technologies requires the coordinated efforts of the research team, research participants, and a variety of community members and project stakeholders. To ensure that the project is completed successfully, these disparate groups of people engage in articulation work, which is the meta-work that supports the use of collaborative systems. In this paper, we examine the articulation work surrounding the deployment of systems that have found limited long-term adoption: assistive technology. Specifically, we examine three research deployments of a collaborative game for children with autism. Analysis of the articulation work performed during these studies demonstrates how research deployments of technologies create conditions in which stakeholders must take on additional roles to make the deployment work. By understanding the articulation work surrounding deployment studies engendered in this role fluidity, we can improve both research design and the analysis of data emergent from these studies.","LouAnne Boyd, Kyle Rector, Halley Profita, Abigale Stangl, Annuska Zolyomi, Shaun Kane, Gillian Hayes","articulation work, assistive technology, autism, collaboration, deployment, role fluidity",6147,6158
10.1145/2858036.2858058,CHI,2016,Tangible Reels,Construction and Exploration of Tangible Maps by Visually Impaired Users,"Maps are essential in everyday life, but inherently inaccessible to visually impaired users. They must be transcribed to non-editable tactile graphics, or rendered on very expensive shape changing displays. To tackle these issues, we developed a tangible tabletop interface that enables visually impaired users to build tangible maps on their own, using a new type of physical icon called Tangible Reels. Tangible Reels are composed of a sucker pad that ensures stability, with a retractable reel that renders digital lines tangible. In order to construct a map, audio instructions guide the user to precisely place Tangible Reels onto the table and create links between them. During subsequent exploration, the device provides the names of the points and lines that the user touches. A pre-study confirmed that Tangible Reels are stable and easy to manipulate, and that visually impaired users can understand maps that are built with them. A follow-up experiment validated that the designed system, including non-visual interactions, enables visually impaired participants to quickly build and explore maps of various complexities.","Julie Ducasse, Marc Mac&#233;, Marcos Serrano, Christophe Jouffrais","interactive graphics, non-visual tangible interfaces",2186,2197
10.1145/3025453.3025906,CHI,2017,Synthesizing Stroke Gestures Across User Populations,A Case for Users with Visual Impairments,"We introduce a new principled method grounded in the Kinematic Theory of Rapid Human Movements to automatically generate synthetic stroke gestures <i>across user populations</i> in order to support ability-based design of gesture user interfaces. Our method is especially useful when the target user population is difficult to sample adequately and, consequently, when there is not enough data to train gesture recognizers to deliver high levels of accuracy. To showcase the relevance and usefulness of our method, we collected gestures from people <i>without</i> visual impairments and successfully synthesized gestures with the articulation characteristics of people <i>with</i> visual impairments. We also show that gesture recognition accuracy improves significantly when using our synthetic gesture samples for training. Our contributions will benefit researchers and practitioners that wish to design gesture user interfaces for people with various abilities by helping them prototype, evaluate, and predict gesture recognition performance without having to expressly recruit and involve people with disabilities in long, time-consuming gesture collection experiments.","Luis Leiva, Daniel Mart&#237;n-Albo, Radu-Daniel Vatavu","bootstrapping, gesture synthesis, kinematic theory, rapid prototyping, sigma-lognormal model, touch gestures",4182,4193
10.1145/3173574.3173650,CHI,2018,"How Teens with Visual Impairments Take, Edit, and Share Photos on Social Media", ,"We contribute a qualitative investigation of how teens with visual impairments (VIP) access smartphone photography, from the time they take photos through editing and sharing them on social media. We observed that they largely want to engage with photos visually, similarly to their sighted peers, and have developed strategies around photo capture, editing, sharing, and consumption that attempt to mitigate usability limitations of current photography and social media apps. We demonstrate the need for more work examining how young people with low vision engage with smartphone photography and social media, as they are heavy users of such technologies and have challenges distinct from their totally blind counterparts. We conclude with design considerations to alleviate the usability barriers we uncovered and for making smartphone photography and social media more accessible and relevant for VIPs.","Cynthia Bennett, Jane E, Martez Mott, Edward Cutrell, Meredith Morris","accessibility, blindness, instagram, photography, snapchat, social media, visual impairment",1,12
10.1145/2702123.2702334,CHI,2015,Privacy Concerns and Behaviors of People with Visual Impairments, ,"Various technologies have been developed to help make the world more accessible to visually impaired people, and recent advances in low-cost wearable and mobile computing are likely to drive even moreadvances. However, the unique privacy and security needs of visually impaired people remain largely unaddressed. We conducted an exploratory user study with 14 visually impaired participants to understand the techniques they currently use for protecting privacy, their remaining privacy concerns,and how new technologies may be able to help. The interviews explored privacy not only in the physical world (e.g., bystanders overhearing private conversations) and the online world (e.g., determining if a URL is legitimate), but also in the interface between the two (e.g. bystanders `shoulder-surfing' data from screens). The study revealed serious concerns that are not adequately solved by current technology, and suggested new directions for improving the privacy of this significant fraction of the population.","Tousif Ahmed, Roberto Hoyle, Kay Connelly, David Crandall, Apu Kapadia","privacy, visually impaired people, wearable technology",3523,3532
10.1145/2470654.2466164,CHI,2013,Designing action-based exergames for children with cerebral palsy, ,"Children with cerebral palsy (CP) want to play fast-paced action-oriented videogames similar to those played by their peers without motor disabilities. This is particularly true of exergames, whose physically-active gameplay matches the fast pace of action games. But disabilities resulting from CP can make it difficult to play action games. Guidelines for developing games for people with motor disabilities steer away from high-paced action, including recommendations to avoid the need for time-sensitive actions and to keep game pace slow. Through a year-long participatory design process with children with CP, we have discovered that it is in fact possible to develop action-oriented exergames for children with CP at level III on the Gross Motor Function Classification Scale. We followed up the design process with an eight-week home trial, in which we found the games to be playable and enjoyable. In this paper, we discuss the design of these games, and present a set of design recommendations for how to achieve both action-orientation and playability.","Hamilton Hernandez, Zi Ye, T.C. Graham, Darcy Fehlings, Lauren Switzer","children with cerebral palsy., exergame, video game design",1261,1270
10.1145/3290605.3300313,CHI,2019,What Can Gestures Tell?,Detecting Motor Impairment in Early Parkinson's from Common Touch Gestural Interactions,"Parkinson's disease (PD) is a chronic neurological disorder causing progressive disability that severely affects patients' quality of life. Although early interventions can provide significant benefits, PD diagnosis is often delayed due to both the mildness of early signs and the high requirements imposed by traditional screening and diagnosis methods. In this paper, we explore the feasibility and accuracy of detecting motor impairment in early PD via sensing and analyzing users' common touch gestural interactions on smartphones. We investigate four types of common gestures, including flick, drag, pinch, and handwriting gestures, and propose a set of features to capture PD motor signs. Through a 102-subject (35 early PD subjects and 67 age-matched controls) study, our approach achieved an AUC of 0.95 and 0.89/0.88 sensitivity/specificity in discriminating early PD subjects from healthy controls. Our work constitutes an important step towards unobtrusive, implicit, and convenient early PD detection from routine smartphone interactions.","Feng Tian, Xiangmin Fan, Junjun Fan, Yicheng Zhu, Jing Gao, Dakuo Wang, Xiaojun Bi, Hongan Wang","parkinson's disease (pd), passive monitoring, touch gestures",1,14
10.1145/1978942.1979268,CHI,2011,Representing users in accessibility research, ,"The need to study representative users is widely accepted within the human-computer interaction (HCI) community. While exceptions exist, and alternative populations are sometimes studied, virtually any introduction to the process of designing user interfaces will discuss the importance of understanding the intended users as well as the significant impact individual differences can have on how effectively individuals can use various technologies. HCI researchers are expected to provide relevant demographics regarding study participants as well as information about experience using similar technologies. Yet, in the field of accessibility we continue to see studies that do not appropriately include representative users. Highlighting ways to remedy this multifaceted problem, we argue that expectations regarding how accessibility research is conducted and reported must be raised if this field is to have the desired impact with regard to inclusive design, the information technologies studied, and the lives of the individuals being studied.","Andrew Sears, Vicki Hanson","accessibility, accessible computing, disabilities, inclusion, older adults, representative users",2235,2238
10.1145/1753326.1753583,CHI,2010,Clutching at straws,using tangible interaction to provide non-visual access to graphs,"We present a tangible user interface (TUI) called Tangible Graph Builder, that has been designed to allow visually impaired users to access graph and chart-based data. We describe the current paper-based materials used to allow independent graph construction and browsing, before discussing how researchers have applied virtual haptic and non-speech audio techniques to provide more flexible access. We discuss why, although these technologies overcome many of the problems of non-visual graph access, they also introduce new issues and why the application of TUIs is important. An evaluation of Tangible Graph Builder with 12 participants (8 sight deprived, 4 blind) revealed key design requirements for non-visual TUIs, including phicon design and handling marker detection failure. We finish by presenting future work and improvements to our system.","David McGookin, Euan Robertson, Stephen Brewster","graphs, haptic interaction, tangible user interface, visual impairment",1715,1724
10.1145/3290605.3300659,CHI,2019,Feeling Fireworks,An Inclusive Tactile Firework Display,"This paper presents a novel design for a large-scale interactive tactile display. Fast dynamic tactile effects are created at high spatial resolution on a flexible screen, using directable nozzles that spray water jets onto the rear of the screen. The screen further has back-projected visual content and touch interaction. The technology is demonstrated in Feeling Fireworks, a tactile firework show. The goal is to make fireworks more inclusive for the Blind and Low-Vision (BLV) community. A BLV focus group provided input during the development process, and a user study with BLV users showed that Feeling Fireworks is an enjoyable and meaningful experience. A user study with sighted users showed that users could accurately label the correspondence between the designed tactile firework effects and corresponding visual fireworks. Beyond the Feeling Fireworks application, this is a novel approach for scalable tactile displays with potential for broader use.","Dorothea Reusser, Espen Knoop, Roland Siegwart, Paul Beardsley","accessibility, haptic device, large interactive screen",1,11
10.1145/3290605.3300292,CHI,2019,Project Sidewalk,A Web-based Crowdsourcing Tool for Collecting Sidewalk Accessibility Data At Scale,"We introduce Project Sidewalk, a new web-based tool that enables online crowdworkers to remotely label pedestrian-related accessibility problems by virtually walking through city streets in Google Street View. To train, engage, and sustain users, we apply basic game design principles such as interactive onboarding, mission-based tasks, and progress dashboards. In an 18-month deployment study, 797 online users contributed 205,385 labels and audited 2,941 miles of Washington DC streets. We compare behavioral and labeling quality differences between paid crowdworkers and volunteers, investigate the effects of label type, label severity, and majority vote on accuracy, and analyze common labeling errors. To complement these findings, we report on an interview study with three key stakeholder groups (N=14) soliciting reactions to our tool and methods. Our findings demonstrate the potential of virtually auditing urban accessibility and highlight tradeoffs between scalability and quality compared to traditional approaches.","Manaswi Saha, Michael Saugstad, Hanuma Teja Maddali, Aileen Zeng, Ryan Holland, Steven Bower, Aditya Dash, Sage Chen, Anthony Li, Kotaro Hara, Jon Froehlich","accessibility, crowdsourcing, gis, mobility impairments, urban informatics",1,14
10.1145/2702123.2702188,CHI,2015,"Personalized, Wearable Control of a Head-mounted Display for Users with Upper Body Motor Impairments", ,"Head-mounted displays provide relatively hands-free interaction that could improve mobile computing access for users with motor impairments. To investigate this largely unexplored area, we present two user studies. The first, smaller study evaluated the accessibility of Google Glass, a head-mounted display, with 6 participants. Findings revealed potential benefits of a head-mounted display yet demonstrated the need for alternative means of controlling Glass-3 of the 6 participants could not use it at all. We then conducted a second study with 12 participants to evaluate a potential alternative input mechanism that could allow for accessible control of a head-mounted display: switch-based wearable touchpads that can be affixed to the body or wheelchair. The study assessed input performance with three sizes of touchpad, investigated personalization patterns when participants were asked to place the touchpads on their body or wheelchair, and elicited subjective responses. All 12 participants were able to use the touchpads to control the display, and patterns of touchpad placement point to the value of personalization in providing support for each user's motor abilities.","Meethu Malu, Leah Findlater","mobile accessibility, motor impairments, wearables",221,230
10.1145/3290605.3300913,CHI,2019,PersonalTouch,Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction,"Modern touchscreen devices have recently introduced customizable touchscreen settings to improve accessibility for users with motor impairments. For example, iOS 10 introduced the following four Touch Accommodation settings: 1) Hold Duration, 2) Ignore Repeat, 3) Tap Assistance, and 4) Tap Assistance Gesture Delay. These four independent settings lead to a total of more than 1 million possible configurations, making it impractical to manually determine the optimal settings. We present PersonalTouch, which collects and analyzes touchscreen gestures performed by individual users, and recommends personalized, optimal touchscreen accessibility settings. Results from our user study show that PersonalTouch significantly improves touch input success rate for users with motor impairments (20.2%, N=12, p=.00054) and for users without motor impairments (1.28%, N=12, p=.032).","Yi-Hao Peng, Muh-Tarng Lin, Yi Chen, TzuChuan Chen, Pin Sung Ku, Paul Taele, Chin Guan Lim, Mike Chen","accessibility, motor impairment, personalization, touch-screen interaction",1,11
10.1145/3290605.3300747,CHI,2019,VIPBoard,Improving Screen-Reader Keyboard for Visually Impaired People with Character-Level Auto Correction,"Modern touchscreen keyboards are all powered by the word-level auto-correction ability to handle input errors. Unfortunately, visually impaired users are deprived of such benefit because a screen-reader keyboard offers only character-level input and provides no correction ability. In this paper, we present VIPBoard, a smart keyboard for visually impaired people, which aims at improving the underlying keyboard algorithm without altering the current input interaction. Upon each tap, VIPBoard predicts the probability of each key considering both touch location and language model, and reads the most likely key, which saves the calibration time when the touchdown point misses the target key. Meanwhile, the keyboard layout automatically scales according to users' touch point location, which enables them to select other keys easily. A user study shows that compared with the current keyboard technique, VIPBoard can reduce touch error rate by 63.0% and increase text entry speed by 12.6%.","Weinan Shi, Chun Yu, Shuyi Fan, Feng Wang, Tong Wang, Xin Yi, Xiaojun Bi, Yuanchun Shi","auto-correction, smartphone, text entry, visually impaired",1,12
10.1145/2858036.2858116,CHI,2016,"""With most of it being pictures now, I rarely use it""",Understanding Twitter's Evolving Accessibility to Blind Users,"Social media is an increasingly important part of modern life. We investigate the use of and usability of Twitter by blind users, via a combination of surveys of blind Twitter users, large-scale analysis of tweets from and Twitter profiles of blind and sighted users, and analysis of tweets containing embedded imagery. While Twitter has traditionally been thought of as the most accessible social media platform for blind users, Twitter's increasing integration of image content and users' diverse uses for images have presented emergent accessibility challenges. Our findings illuminate the importance of the ability to use social media for people who are blind, while also highlighting the many challenges such media currently present this user base, including difficulty in creating profiles, in awareness of available features and settings, in controlling revelations of one's disability status, and in dealing with the increasing pervasiveness of image-based content. We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments.","Meredith Morris, Annuska Zolyomi, Catherine Yao, Sina Bahram, Jeffrey Bigham, Shaun Kane","accessibility, blindness, social media, twitter",5506,5516
10.1145/223904.223918,CHI,1995,Developing dual user interfaces for integrating blind and sighted users,the HOMER UIMS,"Existing systems wliich  enable the accessibiiity of Grapliical User Interfaces to blind people follow an ""adaptation strategy"";  each system adopts its own fixed policy for reproducing visual dialogues to a non-visual form, without  knowledge about the application domain or particular dialogue characteristics. It is argued that non-visual  User Interfaces should be more than automatically generated adaptations of visual dialogues. Tools are  required to facilitate non-visual interface construction, which should allow iterative design and implementation  (not supported by adaptation methods). There is a need for ""integrated"" User Interfaces which are concurrently  accessible by both sighted and blind users in order to prevent segregation of blind people in their working  environment. The concept of Ducil User Interfaces is introduced as the most appropriate basis to address  this issue. A User Interface Management System has been developed, called HOMER, which facilitates the  development of Dual User Interfaces. HOMER supports the integration of visual and non-visual lexical  technologies. In this context, a simple toolkit has been also implemented for building non-visual User  Interfaces and has been incorporated in the HOMER system.","Anthony Savidis, Constantine Stephanidis",UIMS; Aids for the impaired; Programming environments,106,113
10.1145/223904.223919,CHI,1995,Improving GUI accessibility for people with low vision, ,"We present UnWindows VI,  a set of tools designed to assist low vision users of X Windows in effectively accomplishing two mundane  yet critical interaction tasks: selectively magÂ­nifying areas of the screen so that the contents can  be seen comfortably, and keeping track of the location of the mouse pointer. We describe our software  from both the end user's and implementor's points of view, with particular emphasis on issues related  to screen magnification techniques. We conÂ­clude with details regarding software availability and plans  for future extensions.","Richard Kline, Ephraim Glinert","workstation interfaces, assistive technology, low vision, screen magnification,  X Window System ",114,121
10.1145/238386.238406,CHI,1996,Audio enhanced 3D interfaces for visually impaired users, ," Three dimensional computer applications such as CAD  packages are often difficult to use because of inadequate depth feedback to the user. It has, however,  been shown that audio feedback can help improve a user's sense of depth perception. This paper describes  an experiment which evaluates the use of three different audio environ- ments in a 3D task undertakeh  by visually impaired users. The three audio environments map tonal, musical, and orchestral sounds to  an (x, y, z) position in a 3D environ- ment. In each environment the user's task is to locate a tar-  get in three dimensions as accurately and quickly as possible. This experiment has three important results:  that audio feedback improves performance in 3D applications for all users; that visually impaired users  can use 3D appli- cations with the accuracy of sighted users; and that visu- ally impaired users can  attain greater target accuracy than sighted users in a sound-only environment. Keywords User Interface,  Auditory Interface, Disability Access, 3D Interface ","Stephen Mereu, Rick Kazman","3D interface, auditory interface, disability access, user interface",72,78
10.1145/258549.258640,CHI,1997,Toward an HCI research and practice agenda based on human needs and social responsibility, ," We outline several  promising areas for improvements in research and practice in the field of Human Computer Interaction  (HCI). These topics show the richness and potential value of HCI work motivated by a combination of a  desire to improve practice and research, and a desire to meet human needs in a responsible manner. KEYWORDS  Future, HCI research and practice, disability access, information access, information retrieval, agents,  World Wide Web, UIMSS, architectures, information poverty, communication poverty, social issues, social  impact ","Michael Muller, Cathleen Wharton, William McIver, Lila Laux","HCI research and practice, UIMSs, World Wide Web, agents, architectures, communication poverty, disability access, future, informatin access, information poverty, information retrieval, social impact, social issues",155,161
10.1145/302979.303101,CHI,1999,Interactive 3D sound hyperstories for blind children, ,Interactive software is currently used for learning andentertainment purposes. This type of software is not very commonamong blind children because most computer games and electronictoys do not have appropriate interfaces to be accessible withoutvisual cues.This study introduces the idea of interactive hyperstoriescarried out in a 3D acoustic virtual world for blind children. Wehave conceptualized a model to design hyperstories. ThroughAudioDoom we have an application that enables testing cognitivetasks with blind children. The main research question underlyingthis work explores how audio- based entertainment and spatial soundnavigable experiences can create cognitive spatial structures inthe minds of blind children.AudioDoom presents first person experiences through explorationof interactive virtual worlds by using only 3D auralrepresentations of the space.,"Maruricio Lumbreras, Jaime S&#225;nchez","3D sound, audio interface, audio-based navigation, blind children, hyperstory, space representation, virtual acoustic environment",318,325
10.1145/302979.303105,CHI,1999,Visual profiles,a critical component of universal access,"This research focuses on characterizing visually impairedcomputer users performance on graphical user interfaces by linkingclinical assessments of low vision with visual icon identification.This was accomplished by evaluating user performance on basicidentification and selection tasks within a graphical userinterface, comparing partially sighted user performance with fullysighted user performance, and linking task performance to specificprofiles of visual impairment. Results indicate that visual acuity,contrast sensitivity, visual field and color perception weresignificant predictors of task performance. In addition, icon sizeand background color significantly influenced performance.Suggestions for future research are provided. Keywords","Julie Jacko, Max Dixon, Robert Rosa, Ingrid Scott, Charles Pappas","disabilities, low vision, universal access, visual icons",330,337
10.1145/365024.365038,CHI,2001,Improving the performance of the cyberlink mental interface with &#8220;yes / no program&#8221;, ,"We summarise the results of the first studies to investigate the Cyberlink brain body interface as an assistive technology. Three phases of studies and a contextual inquiry were performed with a range of users.  A focus group was formed from brain-injured users with locked-in syndrome who have no other method of communication or control of a computer than the Cyberlink. Versions of a Yes/No program were then created to allow communication and have achieved some success with the focus group.  The purpose of this paper is to discuss how this program has been improved and what steps need to be taken to create communication programs for persons with severe motor impairment.  As a result of our experiences, we have been able to develop a set of design guidelines for brain-body  interface operated Yes/No programs.  These are presented and justified on the basis of our experiences.  We also raise some general issues for assistive technologies of this nature.","Eamon Doherty, Gilbert Cockton, Chris Bloor, Dennis Benigno","assistive technology, cyberlink, locked in syndrome, mental interface, ody interface",69,76
10.1145/642611.642618,CHI,2003,Design and user evaluation of a joystick-operated full-screen magnifier, ,"The paper reports on two development cycles of a joystick-operated full-screen magnifier for visually impaired users. In the first cycle of evaluation, seven visually impaired computer users evaluated the system in comprehension-based sessions using text documents. After considering feedback from these evaluators, a second version of the system was produced and evaluated by a further six visually impaired users. The second evaluation was conducted using information-seeking tasks using Web pages. In both evaluations, the 'thinking aloud protocol' was used. This study makes several contributions to the field. First, it is perhaps the first published study investigating the use of a joystick as an absolute and relative pointing device to control a screen magnifier. Second, the present study revealed that for most of the visually impaired users who participated in the study the joystick had good spatial, cognitive and ergonomic attributes, even for those who had never before used a joystick.","Sri Kurniawan, Alasdair King, David Evans, Paul Blenkhorn","joystick, screen magnifier, visually impaired users",25,32
10.1145/642611.642619,CHI,2003,Older adults and visual impairment,what do exposure times and accuracy tell us about performance gains associated with multimodal feedback?,"This study examines the effects of multimodal feedback on the performance of older adults with different visual abilities. Older adults possessing normal vision (n=29) and those who have been diagnosed with Age-Related Macular Degeneration (n=30) performed a series of drag-and-drop tasks under varying forms of feedback. User performance was assessed with measures of feedback exposure times and accuracy. Results indicated that for some cases, non-visual (e.g. auditory or haptic) and multimodal (bi- and trimodal) feedback forms demonstrated significant performance gains over the visual feedback form, for both AMD and normally sighted users. In addition to visual acuity, effects of manual dexterity and computer experience are considered.","Julie Jacko, Ingrid Scott, Francois Sainfort, Leon Barnard, Paula Edwards, V. Emery, Thitima Kongnakorn, Kevin Moloney, Brynley Zorich","age related macular degeneration (AMD), multimodal feedback, multimodality, visual feedback, visually impaired users",33,40
10.1145/642611.642620,CHI,2003,Multiple haptic targets for motion-impaired computer users, ,"Although a number of studies have reported that force feedback gravity wells can improve performance in ""point-and-click"" tasks, there have been few studies addressing issues surrounding the use of gravity wells for multiple on-screen targets. This paper investigates the performance of users, both with and without motion-impairments, in a ""point-and-click"" task when an undesired haptic distractor is present. The importance of distractor location is studied explicitly. Results showed that gravity wells can still improve times and error rates, even on occasions when the cursor is pulled into a distractor. The greatest improvement is seen for the most impaired users. In addition to traditional measures such as time and errors, performance is studied in terms of measures of cursor movement along a path. Two cursor measures, angular distribution and temporal components, are proposed and their ability to explain performance differences is explored.","Faustina Hwang, Simeon Keates, Patrick Langdon, P. Clarkson","Wingman, cursor control, force feedback, motion-impaired",41,48
10.1145/642611.642641,CHI,2003,Can you see what i hear?,the design and evaluation of a peripheral sound display for the deaf,"We developed two visual displays for providing awareness of environmental audio to deaf individuals. Based on fieldwork with deaf and hearing participants, we focused on supporting awareness of non-speech audio sounds such as ringing phones and knocking in a work environment. Unlike past work, our designs support both monitoring and notification of sounds, support discovery of new sounds, and do not require a priori knowledge of sounds to be detected. Our Spectrograph design shows pitch and amplitude, while our Positional Ripples design shows amplitude and location of sounds. A controlled experiment involving deaf participants found neither display to be significantly distracting. However, users preferred the Positional Ripples display and found that display easier to monitor (notification sounds were detected with 90% success in a laboratory setting). The Spectrograph display also supported successful detection in most cases, and was well received when deployed in the field.","F. Ho-Ching, Jennifer Mankoff, James Landay","assistive technology, deaf, non-speech audio, peripheral and ambient and notification displays, sound visualization",161,168
10.1145/985692.985732,CHI,2004,Isolating the effects of visual impairment,exploring the effect of AMD on the utility of multimodal feedback,"This study examines the effects of multimodal feedback on the performance of older adults with an ocular disease, Age-Related Macular Degeneration (AMD), when completing a simple computer-based task. Visually healthy older users (n = 6) and older users with AMD (n = 6) performed a series of drag-and-drop tasks that incorporated a variety of different feedback modalities. The user groups were equivalent with respect to traditional visual function metrics and measured subject cofactors, aside from the presence or absence of AMD. Results indicate that users with AMD exhibited decreased performance, with respect to required feedback exposure time. Some non-visual and multimodal feedback forms show potential as solutions to enhance performance, for those with AMD as well as for visually healthy older adults.","Julie Jacko, Leon Barnard, Thitima Kongnakorn, Kevin Moloney, Paula Edwards, V. Emery, Francois Sainfort","age-related macular degeneration (AMD), multimodal feedback, multimodality, universal access, visual feedback, visual impairment, visually impaired users",311,318
10.1145/985692.985744,CHI,2004,The participatory design of a sound and image enhanced daily planner for people with aphasia, ,"Aphasia is a cognitive disorder that impairs speech and language. From interviews with aphasic individuals, their caregivers, and speech-language pathologists, the need was identified for a daily planner that allows aphasic users to independently manage their appointments. We used a participatory design approach to develop ESI Planner (the Enhanced with Sound and Images Planner) for use on a PDA and subsequently evaluated it in a lab study. This methodology was used in order to achieve both usable and adoptable technology. In addition to describing our experience in designing ESI Planner, two main contributions are provided: general guidelines for working with special populations in the development of technology, and design guidelines for accessible handheld technology.","Karyn Moffatt, Joanna McGrenere, Barbara Purves, Maria Klawe","assistive technology, cognitive disabilities, handheld devices, multi-modal interaction, participatory design, universal usability",407,414
10.1145/1054972.1054979,CHI,2005,Is your web page accessible?,a comparative study of methods for assessing web page accessibility for the blind,"Web access for users with disabilities is an important goal and challenging problem for web content developers and designers. This paper presents a comparison of different methods for finding accessibility problems affecting users who are blind. Our comparison focuses on techniques that might be of use to Web developers without accessibility experience, a large and important group that represents a major source of inaccessible pages. We compare a laboratory study with blind users to an automated tool, expert review by web designers with and without a screen reader, and remote testing by blind users. Multiple developers, using a screen reader, were most consistently successful at finding most classes of problems, and tended to find about 50% of known problems. Surprisingly, a remote study with blind users was one of the least effective methods. All of the techniques, however, had different, complementary strengths and weaknesses.","Jennifer Mankoff, Holly Fait, Tu Tran","assistive technologies, disability, evaluation, web accessibility",41,50
10.1145/1054972.1054995,CHI,2005,EyeDraw,enabling children with severe motor impairments to draw with their eyes,"EyeDraw is a software program that, when run on a computer with an eye tracking device, enables children with severe motor disabilities to draw pictures by just moving their eyes. This paper discusses the motivation for building the software, how the program works, the iterative development of two versions of the software, user testing of the two versions by people with and without disabilities, and modifications to the software based on user testing. Feedback from both children and adults with disabilities, and from their caregivers, was especially helpful in the design process. The project identifies challenges that are unique to controlling a computer with the eyes, and unique to writing software for children with severe motor impairments.","Anthony Hornof, Anna Cavender","art, children, drawing, eye tracking, input devices, interaction techniques, universal access",161,170
10.1145/1054972.1055009,CHI,2005,Discrete acceleration and personalised tiling as brain?body interface paradigms for neurorehabilitation, ,"We present two studies that have advanced the design of brain-body interfaces for use in the rehabilitation of individuals with severe neurological impairment due to traumatic brain injury. We first developed and evaluated an adaptive cursor acceleration algorithm based on screen areas. This improved the initial design, but was too inflexible to let users make the most of their highly varied abilities. Only some individuals were well served by this adaptive interface. We therefore developed and evaluated an approach based on personalized tile layouts. The rationales for both designs are presented, along with details of their implementation. Evaluation studies for each are reported, which show that we have extended the user population who can use our interfaces relative to previous studies. We have also extended the usable functionality for some of our user group. We thus claim that personalized tiling with discrete acceleration has allowed us to extend the usable functionality of brain-body interfaces to a wider population with traumatic brain injury, thus creating new options for neurorehabiliation.","Paul Gnanayutham, Chris Bloor, Gilbert Cockton","accessibility, assistive technology, brain-body interfaces, cyberlink&#228;, input devices, neurorehabiliation",261,270
10.1145/1054972.1055042,CHI,2005,A visual recipe book for persons with language impairments, ,"Cooking is a daily activity for many people. However, traditional text recipes are often prohibitively difficult to follow for people with language disorders, such as aphasia. We have developed a multi-modal application that leverages the retained ability of aphasic individuals to recognize image-based representations of objects, providing a presentation format that can be more easily followed than a traditional text recipe. Through a systematic approach to developing a visual language for cooking, and the subsequent case study evaluation of a prototype developed according to this language, we show that a combination of visual instructions and navigational structure can help individuals with relatively large language deficits to cook more independently.","Kimberly Tee, Karyn Moffatt, Leah Findlater, Eve MacGregor, Joanna McGrenere, Barbara Purves, Sidney Fels","aphasia, assistive technology, heuristics, multi-modal interfaces",501,510
10.1145/1054972.1055043,CHI,2005,Participatory design of an orientation aid for amnesics, ,"We present the participatory design and evaluation of an orientation aid for individuals who have anterograde amnesia. Our design team included six amnesics who have extreme difficulty storing new memories. We describe the methods we used to enable the participation of individuals with such severe cognitive impairments. Through this process, we have conceived, designed, and developed the OrientingTool, a software application for Personal Digital Assistants that can be used by amnesics to orient themselves when feeling lost or disoriented. Two complementary studies were conducted to evaluate the effectiveness of this tool in ecologically valid contexts. Our findings suggest that the OrientingTool can improve an amnesic's independence and confidence in managing situations when disoriented, and that participatory design may be productively used with participants who have significant cognitive disabilities.","Mike Wu, Ron Baecker, Brian Richards","anterograde amnesia, assistive technologies, cognitive prosthetics, orientation aids, participatory design, personal digital assistants, users with disabilities",511,520
10.1145/1124772.1124785,CHI,2006,Improving accessibility of the web with a computer game, ,"Images on the Web present a major accessibility issue for the visually impaired, mainly because the majority of them do not have proper captions. This paper addresses the problem of attaching proper explanatory text descriptions to arbitrary images on the Web. To this end, we introduce Phetch, an enjoyable computer game that collects explanatory descriptions of images. People play the game because it is fun, and as a side effect of game play we collect valuable information. Given any image from the World Wide Web, Phetch can output a correct annotation for it. The collected data can be applied towards significantly improving Web accessibility. In addition to improving accessibility, Phetch is an example of a new class of games that provide entertainment in exchange for human processing power. In essence, we solve a typical computer vision problem with HCI tools alone.","Luis von Ahn, Shiry Ginosar, Mihir Kedia, Ruoran Liu, Manuel Blum","accessibility, distributed knowledge acquisition, web-based games",79,82
10.1145/1124772.1124797,CHI,2006,Participatory design with proxies,developing a desktop-PDA system to support people with aphasia,"In this paper, we describe the design and preliminary evaluation of a hybrid desktop-handheld system developed to support individuals with aphasia, a disorder which impairs the ability to speak, read, write, or understand language. The system allows its users to develop speech communication through images and sound on a desktop computer and download this speech to a mobile device that can then support communication outside the home. Using a desktop computer for input addresses some of this population's difficulties interacting with handheld devices, while the mobile device addresses stigma and portability issues. A modified participatory design approach was used in which proxies, that is, speech-language pathologists who work with aphasic individuals, assumed the role normally filled by users. This was done because of the difficulties in communicating with the target population and the high variability in aphasic disorders. In addition, the paper presents a case study of the proxy-use participatory design process that illustrates how different interview techniques resulted in different user feedback.","Jordan Boyd-Graber, Sonya Nikolova, Karyn Moffatt, Kenrick Kin, Joshua Lee, Lester Mackey, Marilyn Tremaine, Maria Klawe","aphasia, assistive technology, multi-modal interfaces, participatory design",151,160
10.1145/1124772.1124845,CHI,2006,Trackball text entry for people with motor impairments, ,"We present a new gestural text entry method for trackballs. The method uses the mouse cursor and relies on crossing instead of pointing. A user writes in fluid Roman-like unistrokes by """"pulsing"""" the trackball in desired letter patterns. We examine this method both theoretically using the Steering Law and empirically in two studies. Our studies show that able-bodied users who were unfamiliar with trackballs could write at about 10 wpm with &lt;4% total errors after 45 minutes. In eight sessions, a motor-impaired trackball user peaked at 7.11 wpm with 0% uncorrected errors, compared to 5.95 wpm with 0% uncorrected errors with an on-screen keyboard. Over sessions, his speeds were significantly faster with our gestural method than with an on-screen keyboard. A former 15-year veteran of on-screen keyboards, he now uses our gestural method instead.","Jacob Wobbrock, Brad Myers","EdgeWrite, Fitts' Law, Steering Law, crossing, gestures, pointing, text entry, text input, trackballs, unistrokes",479,488
10.1145/1124772.1124911,CHI,2006,Tensions in designing capture technologies for an evidence-based care community, ,"Evidence-based care is an increasingly popular process for long term diagnosis and monitoring of education and healthcare disabilities. Because this evidence must also be collected in everyday life, it is a technique that can greatly benefit from automated capture technologies. These solutions, however, can raise significant concerns about privacy, control, and surveillance. In this paper, we present an analysis of these concerns with regard to evidence-based care. This analysis underscores the need to consider community-based risk and reward analyses in addition to the traditionally used analyses for individual users when designing socially appropriate technologies.","Gillian Hayes, Gregory Abowd","capture and access, ethnography, evidence-based care, privacy, ubicomp",937,946
10.1145/1124772.1124941,CHI,2006,Feeling what you hear,tactile feedback for navigation of audio graphs,"Access to digitally stored numerical data is currently very limited for sight impaired people. Graphs and visualizations are often used to analyze relationships between numerical data, but the current methods of accessing them are highly visually mediated. Representing data using audio feedback is a common method of making data more accessible, but methods of navigating and accessing the data are often serial in nature and laborious. Tactile or haptic displays could be used to provide additional feedback to support a point-and-click type interaction for the visually impaired. A requirements capture conducted with sight impaired computer users produced a review of current accessibility technologies, and guidelines were extracted for using tactile feedback to aid navigation. The results of a qualitative evaluation with a prototype interface are also presented. Providing an absolute position input device and tactile feedback allowed the users to explore the graph using tactile and proprioceptive cues in a manner analogous to point-and-click techniques.","Steven Wall, Stephen Brewster","accessibility, audio, blind, graph, guidelines, multimodal, navigation, tactile",1123,1132
10.1145/1124772.1124942,CHI,2006,Remote usability evaluations With disabled people, ,"Finding participants for evaluations with specific demographics can be a problem for usability and user experience specialists. In particular, finding participants with disabilities is especially problematic, yet testing with disabled people is becoming increasingly important. Two case studies are presented that explore using asynchronous remote evaluation techniques with disabled participants. These show that while quantitative data are comparable, the amount and richness of qualitative data are not likely to be comparable. The implications for formative and summative evaluations are discussed and a set of principles for local and remote evaluations with disabled users is presented.","Helen Petrie, Fraser Hamilton, Neil King, Pete Pavan","disabled users, remote evaluation, usability research, usability testing and evaluation",1133,1141
10.1145/1124772.1124943,CHI,2006,Desperately seeking simplicity,how young adults with cognitive disabilities and their families adopt assistive technologies,"A surprisingly high percentage of assistive technology devices (35% or more) are purchased, but not successfully adopted. Through semi-structured interviews with a dozen families, we have come to understand the role technology plays in the lives of families who have a young adult with cognitive disabilities, and how families find, acquire, and use these technologies. This study addresses gaps in existing research and informs future efforts in assistive technology design. Design implications include the importance of simplicity not only in technology function but in configuration, documentation, maintenance, and upgrade or replacement; as well as the need for designers to use methods that consider the multiple individuals and stages involved in the technology adoption process.",Melissa Dawe,"assistive technology, cell phones, cognitive disabilities, coordination, design, ethnography, family, home, independence, mobile technology, safety, semi-structured interviews, technology adoption",1143,1152
10.1145/1240624.1240670,CHI,2007,Approaches to web search and navigation for older computer novices, ,"A proof of concept web search and navigation system was developed for older people for whom the Internet is seen as an alien territory. A joint industry/academia team deployed User Sensitive Inclusive Design principles, focusing on the usability of the interface for this user group. The search and navigation system that was developed was significantly preferred by the user group to that provided by a standard commercial (Internet Service Provider) system; it scored highly for ease of use and the participants reported increased confidence in their ability to master the Internet. Recorded quantitative measures showed fewer task errors. The outcome of the development was a successful ""proof of concept"" search and navigation system for older novice computer users together with approaches to design and development for those who wish to design for this user group.","Anna Dickinson, Michael Smith, John Arnott, Alan Newell, Robin Hill","accessibility, human factors, interface layering, older people, usability, web browser, web portal, web search",281,290
10.1145/1240624.1240688,CHI,2007,The relationship between accessibility and usability of websites, ,"Accessibility and usability are well established concepts for user interfaces and websites. Usability is precisely defined, but there are different approaches to accessibility. In addition, different possible relationships could exist between problems encountered by disabled and non-disabled users, yet little empirical data have been gathered on this question. Guidelines for accessibility and usability of websites provide ratings of the importance of problems for users, yet little empirical data have been gathered to validate these ratings. A study investigated the accessibility of two websites with 6 disabled (blind) and 6 non-disabled (sighted) people. Problems encountered by the two groups comprised two intersecting sets, with approximately 15% overlap. For one of the two websites, blind people rated problems significantly more severely than sighted people. There was high agreement between participants as to the severity of problems, and agreement between participants and researchers. However, there was no significant agreement between either participants or researchers and the importance/priority ratings provided by accessibility and usability guidelines. Practical and theoretical implications of these results are discussed.","Helen Petrie, Omar Kheir","accessibility, guidelines, severity ratings, usability, user testing",397,406
10.1145/1240624.1240692,CHI,2007,EyePoint,practical pointing and selection using gaze and keyboard,"We present a practical technique for pointing and selection using a combination of eye gaze and keyboard triggers. EyePoint uses a two-step progressive refinement process fluidly stitched together in a look-press-look-release action, which makes it possible to compensate for the accuracy limitations of the current state-of-the-art eye gaze trackers. While research in gaze-based pointing has traditionally focused on disabled users, EyePoint makes gaze-based pointing effective and simple enough for even able-bodied users to use for their everyday computing tasks. As the cost of eye gaze tracking devices decreases, it will become possible for such gaze-based techniques to be used as a viable alternative for users who choose not to use a mouse depending on their abilities, tasks and preferences.","Manu Kumar, Andreas Paepcke, Terry Winograd","eye pointing, eye tracking, gaze-enhanced user interface design, pointing and selection",421,430
10.1145/1240624.1240759,CHI,2007,Password sharing,implications for security design based on social practice,"Current systems for banking authentication require that customers not reveal their access codes, even to members of the family. A study of banking and security in Australia shows that the practice of sharing passwords does not conform to this requirement. For married and de facto couples, password sharing is seen as a practical way of managing money and a demonstration of trust. Sharing Personal Identification Numbers (PINs) is a common practice among remote indigenous communities in Australia. In areas with poor banking access, this is the only way to access cash. People with certain disabilities have to share passwords with carers, and PIN numbers with retail clerks. In this paper we present the findings of a qualitative user study of banking and money management. We suggest design criteria for banking security systems, based on observed social and cultural practices of password and PIN number sharing.","Supriya Singh, Anuja Cabraal, Catherine Demosthenous, Gunela Astbrink, Michele Furlong","Australia, UCD, banking, security, sharing passwords, social and cultural centered design",895,904
10.1145/1240624.1240830,CHI,2007,Grow and know,understanding record-keeping needs for tracking the development of young children,"From birth through age five, children undergo rapid development and learn skills that will influence them their entire lives. Regular visits to the pediatrician and detailed record-keeping can ensure that children are progressing and can identify early warning signs of developmental delay or disability. However, new parents are often overwhelmed with new responsibilities, and we believe there is an opportunity for computing technology to assist in this process. In this paper, we present a qualitative study aimed at uncovering some specific needs for record-keeping and analysis for new parents and their network of caregivers. Through interviews and focus groups, we have confirmed assumptions about the rationales parents have and the functions required for using technology for record-keeping. We also identify new themes, potential prototypes, and design guidelines for this domain.","Julie Kientz, Rosa Arriaga, Marshini Chetty, Gillian Hayes, Jahmeilah Richardson, Shwetak Patel, Gregory Abowd","children, design requirements, developmental delay, healthcare, qualitative study",1351,1360
10.1145/1240624.1240854,CHI,2007,Towards developing assistive haptic feedback for visually impaired internet users, ,"Haptic technologies are thought to have the potential to help blind individuals overcome the challenges experienced when accessing the Web. This paper proposes a structured participatory-based approach for developing targeted haptic sensations for purposes of web page exploration, and reports preliminary results showing how HTML elements can be represented through the use of force-feedback. Findings are then compared with mappings from previous studies, demonstrating the need for providing tailored haptic sensations for blind Internet users. This research aims to culminate in a framework, encompassing a vocabulary of haptic sensations with accompanying recommendations for designers to reference when developing inclusive web solutions.","Ravi Kuber, Wai Yu, Graham McAllister","blind, design methodology, haptic, participatory design, scenarios, web accessibility",1525,1534
10.1145/1240624.1240856,CHI,2007,An adaptive & adaptable approach to enhance web graphics accessibility for visually impaired people, ,"To date, efforts have been made to enable visually impaired people to gain access to graphics on the Internet. However, these studies only offer a solution for a specific type of graphic by using a fixed set of hardware. To address this, a design approach of an adaptive and adaptable architecture is introduced which adapts to different graphical content, input/output devices (including assistive technologies) and user's profile and preferences. This system brings the opportunity to visually impaired people to gain access to graphics via different modalities by providing an adequate accessibility interface and interaction based on their profiles and needs.","Chui Tan, Wai Yu, Graham McAllister","adaptable, adaptive, context information, graphics, visually impaired",1539,1542
10.1145/1357054.1357119,CHI,2008,Multimodal collaborative handwriting training for visually-impaired people, ,"""McSig"" is a multimodal teaching and learning environ-ment for visually-impaired students to learn character shapes, handwriting and signatures collaboratively with their teachers. It combines haptic and audio output to realize the teacher's pen input in parallel non-visual modalities. McSig is intended for teaching visually-impaired children how to handwrite characters (and from that signatures), something that is very difficult without visual feedback. We conducted an evaluation with eight visually-impaired children with a pretest to assess their current skills with a set of character shapes, a training phase using McSig and then a post-test of the same character shapes to see if there were any improvements. The children could all use McSig and we saw significant improvements in the character shapes drawn, particularly by the completely blind children (many of whom could draw almost none of the characters before the test). In particular, the blind participants all expressed enjoyment and excitement about the system and using a computer to learn to handwrite.","Beryl Plimmer, Andrew Crossan, Stephen Brewster, Rachel Blagojevic","haptic trajectory playback, multimodal interface design, signature training, visually-impaired users",393,402
10.1145/1357054.1357151,CHI,2008,"Design, adoption, and assessment of a socio-technical environment supporting independence for persons with cognitive disabilities", ,"A significant fraction of persons with cognitive disabilities are potentially able to live more independently with the use of powerful tools embedded in their social environment. The Memory Aiding Prompting System (MAPS) provides an environment in which caregivers can create scripts that can be used by people with cognitive disabilities (""clients"") to support them in carrying out tasks that they would not be able to achieve by themselves. To account for the great diversity among clients, MAPS was developed as a meta-design environment, empowering the caregivers to develop personalized prompting systems for the specific needs of individual clients.","Stefan Carmien, Gerhard Fischer","assistive technology, cognitive disabilities, distributed intelligence, end-user development, ethnographic methods, independence, meta-design, socio-technical environments",597,606
10.1145/1357054.1357186,CHI,2008,Collaborating to remember,a distributed cognition account of families coping with memory impairments,"Individuals with cognitive deficits and their families are prime examples of collaborative ""systems"" that seek to perform everyday tasks together. Yet there has been little investigation into how these families communicate and coordinate in basic tasks like remembering appointments. In this paper we take a distributed cognition approach to studying ten families struggling with amnesia through nonparticipant observation and interviews. Our data show that the families work closely together as cognitive systems that must compensate for memory volatility in one of the members. We explore our participants' strategies for overcoming these difficulties and present lessons for the design of assistive technologies, highlighting the need for redundancy, easy and frequent synchronization, and awareness of updates. We conclude with implications for distributed cognition theory.","Mike Wu, Jeremy Birnholtz, Brian Richards, Ronald Baecker, Mike Massimi","amnesia, assistive technology, collaboration, design, distributed cognition, exploratory study, family, theory",825,834
10.1145/1357054.1357250,CHI,2008,"Improving the performance of motor-impaired users with automatically-generated, ability-based interfaces", ,"We evaluate two systems for automatically generating personalized interfaces adapted to the individual motor capabilities of users with motor impairments. The first system, SUPPLE, adapts to users' capabilities indirectly by first using the ARNAULD preference elicitation engine to model a user's preferences regarding how he or she likes the interfaces to be created. The second system, SUPPLE++, models a user's motor abilities directly from a set of one-time motor performance tests. In a study comparing these approaches to baseline interfaces, participants with motor impairments were 26.4% faster using ability-based user interfaces generated by SUPPLE++. They also made 73% fewer errors, strongly preferred those interfaces to the manufacturers' defaults, and found them more efficient, easier to use, and much less physically tiring. These findings indicate that rather than requiring some users with motor impairments to adapt themselves to software using separate assistive technologies, software can now adapt itself to the capabilities of its users.","Krzysztof Gajos, Jacob Wobbrock, Daniel Weld","ability-based user interfaces, arnauld, motor impairments, supple, supple++",1257,1266
10.1145/1357054.1357281,CHI,2008,Game over,learning by dying,"This paper presents the design and evaluation of ""Game Over!"", which is the world's first universally inaccessible game (i.e., a game that can be played by no one). The game is meant to be used as an educational tool for disseminating and teaching game accessibility guidelines. This is achieved by providing game developers a first-hand (frustrating) experience of how it feels interacting with a game that is not accessible, due to the fact that important design rules were not considered or applied during its design. Both the overall concept and the approach followed were evaluated and validated through: (a) an on-line survey; (b) ""live"" feedback from players and developers; and (c) public opinions and critique collected from numerous Web sites and blogs where ""Game Over!"" was presented and discussed. The evaluation outcomes strongly suggest that computer games and humor constitute a perfect match for reaching out, motivating and educating the game developers' community in the subject of game accessibility.",Dimitris Grammenos,"design guidelines, game accessibility, game-based learning",1443,1452
10.1145/1518701.1519032,CHI,2009,Designing with children with severe motor impairments, ,"Children with severe motor impairments such as with disabilities resulting from severe cerebral palsy benefit greatly from assistive technology, but very little guidance is available on how to collaborate with this population as partners in the design of such technology. To explore how to facilitate such collaborations, a field-based participant observation study, as well as structured and unstructured interviews, were conducted at a home for children with severe disabilities. Team-building collaborative design activities were pursued. Guidelines are proposed for how to collaborate with children with severe motor impairments.",Anthony Hornof,"accessibility, children, design, participant observation",2177,2180
10.1145/1518701.1518731,CHI,2009,The VoiceBot,a voice controlled robot arm,"We present a system whereby the human voice may specify continuous control signals to manipulate a simulated 2D robotic arm and a real 3D robotic arm. Our goal is to move towards making accessible the manipulation of everyday objects to individuals with motor impairments. Using our system, we performed several studies using control style variants for both the 2D and 3D arms. Results show that it is indeed possible for a user to learn to effectively manipulate real-world objects with a robotic arm using only non-verbal voice as a control mechanism. Our results provide strong evidence that the further development of non-verbal voice controlled robotics and prosthetic limbs will be successful.","Brandi House, Jonathan Malkin, Jeff Bilmes","motor impairment, robotics, speech recognition, voice-based interface",183,192
10.1145/1518701.1518756,CHI,2009,An enhanced musical experience for the deaf,design and evaluation of a music display and a haptic chair,"Music is a multi-dimensional experience informed by much more than hearing alone, and is thus accessible to people of all hearing abilities. In this paper we describe a prototype system designed to enrich the experience of music for the deaf by enhancing sensory input of information via channels other than in-air audio reception by the ear. The system has two main components-a vibrating 'Haptic Chair' and a computer display of informative visual effects that correspond to features of the music. The Haptic Chair provides sensory input of vibrations via touch. This system was developed based on an initial concept guided by information obtained from a background survey conducted with deaf people from multi-ethnic backgrounds and feedback received from two profoundly deaf musicians. A formal user study with 43 deaf participants suggested that the prototype system enhances the musical experience of a deaf person. All of the users preferred either the Haptic Chair alone (54%) or the Haptic Chair with the visual display (46%). The prototype system, especially the Haptic Chair was so enthusiastically received by our subjects that it is possible this system might significantly change the way the deaf community experiences music.","Suranga Nanayakkara, Elizabeth Taylor, Lonce Wyse, S Ong","assistive technology, deaf, haptic, music visualisation",337,346
10.1145/1518701.1518757,CHI,2009,Longitudinal study of people learning to use continuous voice-based cursor control, ,"We conducted a 2.5 week longitudinal study with five motor impaired (MI) and four non-impaired (NMI) participants, in which they learned to use the Vocal Joystick, a voice-based user interface control system. We found that the participants were able to learn the mapping between the vowel sounds and directions used by the Vocal Joystick, and showed marked improvement in their target acquisition performance. At the end of the ten session period, the NMI group reached the same level of performance as the previously measured ""expert"" Vocal Joystick performance, and the MI group was able to reach 70% of that level. Two of the MI participants were also able to approach the performance of their preferred device, a touchpad. We report on a number of issues that can inform the development of further enhancements in the realm of voice-driven computer control.","Susumu Harada, Jacob Wobbrock, Jonathan Malkin, Jeff Bilmes, James Landay","longitudinal study, motor impairment, pointer control, speech recognition, voice-based interface",347,356
10.1145/1518701.1518774,CHI,2009,Creating a spoken impact,encouraging vocalization through audio visual feedback in children with ASD,"One hallmark difficulty of children with Autism Spectrum Disorder (ASD) centers on communication and speech. Research into computer visualizations of voice has been shown to influence conversational patterns and allow users to reflect upon their speech. In this paper, we present the Spoken Impact Project (SIP), an effort to examine the effect of audio and visual feedback on vocalizations in low-functioning children with ASD by providing them with additional means of understanding and exploring their voice. This research spans over 12 months, including the creation of multiple software packages and detailed analysis of more than 20 hours of experimental video. SIP demonstrates the potential of computer generated audio and visual feedback to encourage vocalizations of children with ASD.","Joshua Hailpern, Karrie Karahalios, James Halle","accessibility, autism, children, speech, visualization, vocalization",453,462
10.1145/1518701.1518912,CHI,2009,The angle mouse,target-agnostic dynamic gain adjustment based on angular deviation,"We present a novel method of dynamic C-D gain adaptation that improves target acquisition for users with motor impairments. Our method, called the Angle Mouse, adjusts the mouse C-D gain based on the deviation of angles sampled during movement. When angular deviation is low, the gain is kept high. When angular deviation is high, the gain is dropped, making the target bigger in motor-space. A key feature of the Angle Mouse is that, unlike most pointing facilitation techniques, it is target-agnostic, requiring no knowledge of target locations or dimensions. This means that the problem of distractor targets is avoided because adaptation is based solely on the user's behavior. In a study of 16 people, 8 of which had motor impairments, we found that the Angle Mouse improved motor-impaired pointing throughput by 10.3% over the Windows default mouse and 11.0% over sticky icons. For able-bodied users, there was no significant difference among the three techniques, as Angle Mouse throughput was within 1.2% of the default. Thus, the Angle Mouse improved pointing performance for users with motor impairments while remaining unobtrusive for able-bodied users.","Jacob Wobbrock, James Fogarty, Shih-Yen (Sean) Liu, Shunichi Kimuro, Susumu Harada","control-display gain, cursor control, dynamic gain adjustment, mouse pointing, pointing facilitation, pointing techniques, target acquisition",1401,1410
10.1145/1518701.1518983,CHI,2009,Evaluating existing audio CAPTCHAs and an interface optimized for non-visual use, ,"Audio CAPTCHAs were introduced as an accessible alternative for those unable to use the more common visual CAPTCHAs, but anecdotal accounts have suggested that they may be more difficult to solve. This paper demonstrates in a large study of more than 150 participants that existing audio CAPTCHAs are clearly more difficult and time-consuming to complete as compared to visual CAPTCHAs for both blind and sighted users. In order to address this concern, we developed and evaluated a new interface for solving CAPTCHAs optimized for non-visual use that can be added in-place to existing audio CAPTCHAs. In a subsequent study, the optimized interface increased the success rate of blind participants by 59% on audio CAPTCHAs, illustrating a broadly applicable principle of accessible design: the most usable audio interfaces are often not direct translations of existing visual interfaces.","Jeffrey Bigham, Anna Cavender","audio captcha, blind users, non-visual interfaces",1829,1838
10.1145/1518701.1518984,CHI,2009,On the audio representation of distance for blind users, ,"This study examines methods for displaying distance information to blind travellers using sound, focussing on abstractions of methods currently used in commercial Electronic Travel Aids (ETAs). Ten blind participants assessed three sound encodings commonly used to convey distance information by ETAs: sound frequency (Pitch), Ecological Distance (ED), and temporal variation or Beat Rate (BR). Response time and response correctness were chosen for measures.","Martin Talbot, William Cowan","assistive technology, blind users, distance perception, evaluation, guidelines, sound visualization",1839,1848
10.1145/191028.191030,ASSETS,1994,Pattern recognition and synthesis for sign language translation system, ,"Sign language is one means of communication for hearing-impaired people. Words and sentences in sign language are mainly represented by hands' gestures. In this report, we show a sign language translation system which we are developing. The system translates Japanese sign language into Japanese and vice versa. In this system, hand shape and position data are inputted using DataGlove. Inputted hand motions are recognized and translated into Japanese sentences. Japanese text is translated into sign language represented as 3-D computer-graphic animation of sign language gestures.","M. Ohki, H. Sagawa, T. Sakiyama, E. Oohira, H. Ikeda, H. Fujisawa", ,1,8
10.1145/191028.191031,ASSETS,1994,Multimedia dictionary of American Sign Language, ,"The <italic>Multimedia Dictionary of American Sign Language</italic> (MM-DASL) is a Macintosh application designed to function as a bilingual (ASL-English) dictionary. It presents ASL signs in full-motion digital video using Apple's QuickTime technology. Major functions of the application include the capability to search for ASL signs by entering English words; the capability to search for ASL signs directly (by specifying formational features); and the capability to perform fuzzy searching (in both ASL and English search modes). For each ASL lexical entry, the dictionary contains definitions of the sign, grammatical information, usage notes, successful English translations, and other information. In addition to serving as the core engine for the MM-DASL, the application is capable of being localized to any signed language, thus allowing researchers and developers in other countries to use the MM-DASL to develop their own signed language dictionaries.","S. Wilcox, J. Scheibman, D. Wood, D. Cokely, W. Stokoe", ,9,16
10.1145/191028.191032,ASSETS,1994,A system for teaching speech to profoundly deaf children using synthesized acoustic and articulatory patterns, ,"This paper describes a computer assisted method of teaching profoundly deaf children to speak, which employs the unique feature of an integrated text-to-speech system (TTS). Our earlier speech training system presented a series of speech parameters, derived from articulatory instruments and acoustic analysis, in a visual form. In that system, teacher's speech is input to the system and used as a model for the children to follow, and the children's speech is monitored to provide feedback. As with other computer-aided speech training systems (e.g. [2]), the teacher-assisted trainer is limited by the time students have with speech teachers. Several computer-based systems for providing information as to the desired acoustic and articulatory patterns and feedback showing what the  children are doing already exist. In our system, we have developed an articulatory component which synthesizes tongue-palate contact patterns for the children to follow.","E. Keate, H. Javkin, N. Antonanzas-Barroso, R. Zou", ,17,22
10.1145/191028.191035,ASSETS,1994,Iconic language design for people with significant speech and multiple impairments, ,"We present an approach of iconic language design for people with significant speech and multiple impairments (SSMI), based upon the theory of Icon Algebra and the theory of Conceptual Dependency (CD) to derive the meaning of iconic sentences. An interactive design environment based upon this methodology is described.","P. Albacete, S.-K. Chang, G. Polese, B. Baker", ,23,30
10.1145/191028.191036,ASSETS,1994,The application of spatialization and spatial metaphor to augmentative and alternative communication, ,"The University of Delaware and the University of Dundee are collaborating on a project that is investigating the application of spatialization and spatial metaphors to interfaces for Augmentative and Alternative Communication. This paper outlines the project's motivation, goals, and methodological considerations. It presents a number of design principles obtained from a review of the HCI literature. Finally, it describes progress on the demonstration of this approach. This application called <italic>VAL</italic> provides a computer-based word board that retains spatial equivalence to the user's paper-based system. It also allows the user to access an extended lexicon through an interface to the WordNet lexical database.","P. Demasco, A. Newell, J. Arnott", ,31,38
10.1145/191028.191039,ASSETS,1994,Screen reader/2,access to OS/2 and the graphical user interface,"Screen Reader/2 is IBM's access system for OS/2, providing blind users access to the graphical user interface (GUI) of Presentation Manager, to Windows programs running under OS/2, and to text mode DOS and OS/2 programs. Screen Reader/2 is a completely redesigned and rewritten follow-on to IBM's Screen Reader Version 1.2 for DOS.There has been considerable discussion about the technical challenges, difficulties, and inherent obstacles presented by the GUI. Not enough time and energy has been devoted to the successes in GUI access, in part because the developers of GUI access software have had their hands full trying to solve very difficult problems.This paper will describe how IBM Screen Reader makes the GUI accessible.",J. Thatcher, ,39,46
10.1145/191028.191041,ASSETS,1994,Providing access to graphical user interfaces&#8212;not graphical screens, , ,"W. Edwards, E. Mynatt, K. Stockton", ,47,54
10.1145/191028.191043,ASSETS,1994,Increasing access to information for the print disabled through electronic documents in SGML, ,"There is a growing conviction that the Standard Generalized Markup Language, SGML, can play an important role as an enabling technology to increase access to information for blind and partially sighted people. This paper reports on mechanisms that have been devised to build in accessibility into SGML encoded electronic documents, concentrating on the work done in the CAPS Consortium&#8212;Communication and Access to Information for People with Special Needs, a European Union funded project in the Technology Initiative for Disabled and Elderly People (TIDE) Programme&#8212;and by ICADD, the International Committee on Accessible Document Design.","B. Bauwens, J. Engelen, F. Evenepoel, C. Tobin, T. Wesley", ,55,61
10.1145/191028.191045,ASSETS,1994,Interactive audio documents, ,"Communicating technical material orally is often hindered by the relentless linearity of audio; information flows <italic>actively</italic> past a passive listener. This is in stark contrast to communication through the printed medium, where we can actively peruse the visual display to access relevant information.A<subscrpt>S</subscrpt>T<subscrpt>E</subscrpt>R is an interactive computing system for <italic>audio formatting</italic> electronic documents (presently, documents written in (L<supscrpt>A</supscrpt>)T<subscrpt>E</subscrpt>X) to produce audio documents. A<subscrpt>S</subscrpt>T<subscrpt>E</subscrpt>R can speak both literary texts and highly technical documents that contain complex mathematics. In fact, the effective speaking and interactive browsing of mathematics  is a key goal of A<subscrpt>S</subscrpt>T<subscrpt>E</subscrpt>R. To this end, a listener can browse both complete documents and complex mathematical expressions. A<subscrpt>S</subscrpt>T<subscrpt>E</subscrpt>R thus enables <italic>active</italic> listening.This paper describes the browsing component of A<subscrpt>S</subscrpt>T<subscrpt>E</subscrpt>R. The design and implementation of A<subscrpt>S</subscrpt>T<subscrpt>E</subscrpt>R is beyond the scope of this paper. Here, we will focus on the browser, and refer to other parts of the system in passing for the sake of completeness.","T. Raman, D. Gries","Interactive Audio Renderings, Audio Browsing, Browsing Structure, In-context Rendering and Browsing, Spoken mathematics",62,68
10.1145/191028.191047,ASSETS,1994,An overview of programs and projects at the rehabilitation research and development center, ,"The mission of the Rehabilitation Research and Development Center is to improve the independence and quality of life for disabled veterans through the creation and application of emerging technologies. In support of this mission, the Center develops concepts, devices, and techniques for in-house testing, national evaluation, and technology transfer leading to commercial production. This presentation will detail the Center's design/development process and technology transfer strategies using examples drawn from its fifteen years of operation.",D. Jaffe, ,69,76
10.1145/191028.191049,ASSETS,1994,Using the Baby-Babble-Blanket for infants with motor problems,an empirical study,"Children with motor problems often develop to be passive, presumably because of an inability to communicate and to control the environment. The Baby-Babble-Blanket (BBB), a pad with pressure switches linked to a Macintosh computer, was developed to meet this need. Lying on the pad, infants use head-rolling, leg-lifting and kicking to produce digitized sound. Data is collected by the BBB software on the infant's switch activations. An empirical study was carried out on a five-month-old infant with club feet, hydrocephaly and poor muscle tone to determine what movements the infant could use to access the pad, whether movements would increase over a baseline in response to sound, and what level of cause and effect the infant would demonstrate. Videotapes and switch activation data suggest that the infant:<list><item>1) could activate the device by rolling his head and raising his legs.</item><item>2) increased switch activations, over a no-sound baseline, in response to the sound of his mother's voice.</item><item>3) was able to change from using his head to raising his legs in response to the reinforcer.</item></list>","H. Fell, H. Delta, R. Peterson, L. Ferrier, Z. Mooraj, M. Valleau","infants, communication and environmental control, sound, motor problems, single-case study, pad",77,84
10.1145/191028.191051,ASSETS,1994,Personal guidance system for the visually impaired, ,"We outline the design for a navigation system for the visually impaired and describe the progress we have made toward such a system. Our long-term goal is for a portable, self-contained system that will allow visually impaired individuals to travel through familiar and unfamiliar environments without the assistance of guides. The system, as it exists now, consists of the following functional components: (1) a means of determining the traveler's position and orientation in space, (2) a Geographic Information System comprising a detailed database of the surrounding environment and functions for automatic route planning and for selecting the database information desired by the user, and (3) the user interface.","J. Loomis, R. Golledge, R. Klatzky, J. Speigle, J. Tietz", ,85,91
10.1145/191028.191053,ASSETS,1994,Hyperbraille,a hypertext system for the blind,"Reading documents is a process which is strongly driven by visual impressions. This is even more the case when the document of interest is not only a linear text but rather a <italic>hypertext</italic> where links to other document parts are realized as highlighted or coloured text. Since blind people are unable to perceive this visual information there is a special need to enable them to navigate through such a non-linear document. In this paper we describe a set of new functions to enhance hypertext systems in order to ensure their accessibility by blind people. We figure out functions that are necessary to step through a hypertext document as well as some status report functions to give access to information that is usually presented visually in common hypertext systems.From our goal to set up an office workspace in a concrete application it becomes clear that we do not only want to enable a blind person to read hypertext-documents but moreover, it must be possible to edit hypertext documents in an easy-to-use on-line fashion. In addition to conventional text processing systems this means that we have to provide effective methods to build and to edit links. Furthermore, we integrate document analysis techniques to build a bridge between paper documents and braille output devices.","T. Kieninger, N. Kuhn", ,92,99
10.1145/191028.191055,ASSETS,1994,Automatic impact sound generation for using in nonvisual interfaces, ,"This paper describes work in progress on automatic generation of &#8220;impact sounds&#8221; based on <italic>purely</italic> physical modelling. These sounds can be used as non-speech audio presentation of objects and as interaction mechanisms to non visual interfaces. Different approaches for synthesizing impact sounds, the process of recording impact sounds and the analysis of impact sounds are introduced. A physical model for describing impact sounds &#8220;spherical objects hitting flat plates or beams&#8221; is presented. Some examples of impact sounds generated by mentioned physical model and comparison of spectra of real recorded sounds and model generated impact sounds (generated via physical modelling) are discussed. The objective of this research project (joint project University of Zurich and Swiss Federal Institute of Technology) is to develop a concept, methods and a prototype for an audio framework. This audio framework shall describe sounds on a highly abstract semantic level. Every sound is to be described as the result of one or several interactions between one or several objects at a certain place and in a certain environment.","A. Darvishi, E. Munteanu, V. Guggiana, H. Schauer, M. Motavalli, M. Rauterberg","non speech sound generation, visual impairment, auditory interfaces, physical modelling, auditive feedback, human computer interaction, software ergonomics, usability engineering, material properties",100,106
10.1145/191028.191058,ASSETS,1994,A communication tool for people with disabilities,lexical semantics for filling in the pieces,"The goal of this project is to provide a communication tool for people with severe speech and motor impairments (SSMI). The tool will facilitate the formation of syntactically correct sentences in the fewest number of keystrokes. Consider the situation where an individual is using a word-based augmentative communication system&#8212;each word is (basically) one keystroke and morphological endings etc. require additional keystrokes. Our prototype system is intended to reduce the burden of the user by allowing him/her to select only the uninflected content words of the desired sentence. The system is responsible for adding proper function words (e.g., articles, prepositions) and necessary morphological endings. In order to accomplish this task, the system attempts to generate a semantic  representation of an utterance under circumstances where syntactic (parse tree) information is not available because the input to the system is a compressed telegraphic message rather than a standard English sentence. The representation is used by the system to generate a full English sentence from the compressed input. The focus of the paper is on the knowledge and processing necessary to produce a semantic representation under these telegraphic constraints.","K. McCoy, P. Demasco, M. Jones, C. Pennington, P. Vanderheyden, W. Zickus", ,107,114
10.1145/191028.191061,ASSETS,1994,Validation of a keystroke-level model for a text entry system used by people with disabilities, ,"A keystroke-level model of user performance was developed to predict the improvement in text generation rate with a word prediction system relative to letters-only typing. Two sets of model simulations were tested against the actual performance of able-bodied and spinal cord injured subjects. For Model 1A, user parameter values were determined independently of subjects' actual performance. The percent improvements predicted by Model 1A differed from the actual improvements by 11 percentage points for able-bodied subjects and 53 percentage points for spinal cord injured subjects. Model 1B employed user parameter values derived from subjects' data and yielded more accurate simulations, with an average error of 6 percentage points across all subjects.","H. Koester, S. Levine", ,115,122
10.1145/191028.191063,ASSETS,1994,An experimental sound-based hierarchical menu navigation system for visually handicapped use of graphical user interfaces, ,"The use of modern computers by the visually handicapped has become more difficult over the past few years. In earlier systems the user interface was a simple character based environment. In those systems, simple devices like screen readers, braille output and speech synthesizers were effective. Current systems now run Graphical User Interfaces (GUIs) which have rendered these simple aids almost useless.In the current work we are developing a tonally based mechanism that allows the visually handicapped user to navigate through the same complex hierarchical menu structures used in the GUI. The software can be easily, and cheaply, incorporated in modern user interfaces, making them available for use by the visually handicapped. In the remainder of this paper we present a  description of the sound-based interfaces as well as the techniques we have developed to test them.","A. Karshmer, P. Brawner, G. Reiswig", ,123,128
10.1145/191028.191066,ASSETS,1994,A rule-based system that suggests computer adaptations for users with special needs, ,"AÃ¿rule-based program was written in Prolog to give advice about how to configure a computing system for users who have special needs. It employs a simple user model describing visual, cognitive, motor, and other abilities. Recommendations are made about appropriate input and output devices, including screens, keyboards, speech devices, and many others. The program was tested against professionals in this field and was shown to agree with them about as well as they agree with one another. Potential uses include advising those who configure computer systems, serving as a teaching tool, and driving intelligent human-computer interaction.","W. McMillan, M. Zeiger, L. Wisniewski", ,129,135
10.1145/191028.191069,ASSETS,1994,LVRS,the low vision research system,"The purpose of this paper is to describe the Low Vision Research System (LVRS). This is a computer-based research tool to be used by vision researchers to develop vision enhancement systems. The three components of the LVRS include warping software, interactive filtering software, and a digital video editing package.",M. Krell, ,136,140
10.1145/191028.191071,ASSETS,1994,EEG as a means of communication,preliminary experiments in EEG analysis using neural networks,"EEG analysis has played a key role in the modeling of the brain's cortical dynamics, but relatively little effort has been devoted to developing EEG as a limited means of communication. If several mental states can be reliably distinguished by recognizing patterns in EEG, then a paralyzed person could communicate to a device like a wheelchair by composing sequences of these mental states. EEG pattern recognition is a difficult problem and hinges on the success of finding representations of the EEG signals in which the patterns can be distinguished. In this article, we report on a study comparing three EEG representations, the raw signals, a reduced-dimensional representation using the K-L transform, and a frequency-based representation. Classification is performed with a two-layer  neural network implemented on a CNAPS server (128 processor, SIMD architecture) by Adaptive Solutions, Inc.. The best classification accuracy on untrained samples is 73% using the frequency-based representation.","C. Anderson, S. Devulapalli, E. Stolz", ,141,147
10.1145/191028.191073,ASSETS,1994,Audio formatting of a graph, ,The software package of Audio Formatting of a Graph (AFG) is primarily designed for people who are visually challenged to study graph theory. It is menu-driven so that a user can get information about a graph easily and conveniently.,"S. Zhang, M. Krishnamoorthy", ,148,152
10.1145/191028.191075,ASSETS,1994,"Disabilities, opportunities, internetworking and technology (DO-IT) on the electronic highway", ,"The United States needs citizens trained in science, engineering, and mathematics, including individuals from traditionally underrepresented groups such as women, racial minorities, and individuals with disabilities. The National Science Foundation has funded a project through the College of Engineering at the University of Washington whose purpose is to recruit and retain students with disabilities into science, engineering, and mathematics academic programs and careers. DO-IT (Disabilities, Opportunities, Internetworking and Technology) makes extensive use of computers, adaptive technology and the Internet network.","S. Burgstahler, D. Comden", ,153,156
10.1145/228347.228349,ASSETS,1996,Touching and hearing GUI's,design issues for the PC-Access system," PC-Access is a system which combines both hardware  and software in order to Â·provide multimodal feedback in a Microsoft Windows graphical interface and  within its applications. We propose two versions of PC-Access: one which offers sound feedback with an  enhanced drawing tablet. and another in which tactile stimuli are synthesized by a haptic pointing device.  When using the second version, the user will be able to perceive the interface objects (e.g, icons, menus,  windows) as well as actions (e.g, moving, re-sizing). Thus, PC-Access offers auditory information (non-verbal  sounds and voice synthesis), reinforced by the sense of touch which in turn helps to direct manipulation.   ","Christophe Ramstein, Odile Martial, Aude Dufresne, Michel Carignan, Patrick Chass&#233;, Philippe Mabilleau", ,2,9
10.1145/228347.228350,ASSETS,1996,Enhancing scanning input with non-speech sounds, ,"  This paper proposes the addition of non-speech sounds to aid people who use scanning as their method  of input. Scanning input is a temporal task; users have to press a switch when a cursor is over the required  target. However, it is usually presented as a spatial task with the items to be scanned laid-out in a  grid. Research has shown that for temporal tasks the auditory modality is often better than the visual.  This paper investigates this by adding non-speech sound to a visual scanning system. It also shows how  our natural abilities to perceive rhythms can be supported so that they can be used to aid the scanning  process. Structured audio messages called Earcons were used for the sound output. The results from a  preliminary investigation were favourable, indicating that the idea is feasible and further research  should be undertaken. KEYWORDS Non-speech sound, earcons, scanning input, multimodal interaction. ","Stephen Brewster, Veli-Pekka Raty, Atte Kortekangas","earcons, multimodal interaction, non-speech sound, scanning input",10,14
10.1145/228347.228351,ASSETS,1996,A study of input device manipulation difficulties, ," People  with a motor disability affecting their use of the keyÂ­board and/or mouse often tend to make unintentional  input errors. Little or no quantified data exists on physical errors in the use of standard computer  input devices, particularly with respect to motor disabilities. Such information, if available, could  be used to develop techÂ­niques for automatic recognition of specific difficulties. Once recognised, many  can be reduced or eliminated by appropriÂ­ate system and application configuration. This paper describes  the pilot study for an experiment inÂ­tended to gather detailed information about input errors made with  keyboards and mice. This work is a step towards proviÂ­sion of dynamic, automatic support for the configuration  of systems and applications to suit individual users. Some initial results from the pilot study are presented,  includÂ­ing an assessment of the experiment design and a summary of some interesting characteristics of  the data gathered so far. KEYWORDS: keyboard, mouse, errors, physical disability, input devices, input  logging. ",Shari Trewin,"errors, input devices, input logging, keyboard, mouse, physical disability",15,22
10.1145/228347.228352,ASSETS,1996,V-Lynx,bringing the World Wide Web to sight impaired users,"The World Wide Web (WWW) project merges the techniques of networked information and hypertext to make an easy but powerful global information system. A client program called a browser is used to access documents in the WWW system and present them all as parts of a seamless hypertext information space. However, today's browsers are primarily graphically or text oriented, which makes the whole system inaccessible to sight- impaired users. In this project we wanted to extend an existing browser with voice output. This extension would allow the sight- impaired to use, at least, textual data, which, at present, forms the bulk ofinformation available over the Web. Our browser should be able to read the document a line or paragraph at a time, read only the first sentence in a paragraph for quick scanning of the document, convey the document structure (headings, emphasized text,lists, hyperlinks), and allow for easy navigation while inside and between documents.","Mitchell Krell, Davor Cubranic","Lynx, URL, WWW, audio, http protocol, hypertext, voice, web browser, web navigation",23,26
10.1145/228347.228353,ASSETS,1996,Computer generated 3-dimensional models of manual alphabet handshapes for the World Wide Web, ," A teaching tool consisting of a collection of three dimensional computer  graphic models representing American Sign Language manual alphabet hand shapes in various locations and  orientations has been established. These computer graphic models have been recorded in the ""Virtual Reality  Modeling Language (VRML) [1] for display with World Wide Web browsers such as Netscape or Mosaic, in  conjunction with VRML browsers such as WebSpace or WorldView"". KEYWORDS: ASL, VRML, Virtual Reality,  World Wide Web ","Sarah Geitz, Timothy Hanson, Stephen Maher","ASL, VRML, World Wide Web, virtual reality",27,31
10.1145/228347.228354,ASSETS,1996,Emacspeak&#8212;direct speech access, ,"Emacspeak is a full-fledged speech output inter- face to Emacs, and is being used to provide direct speech access to a UNIX workstation. The kind of speech access provided by Emacspeak is qual- itatively different from what conventional screen- readers provide —emacspeak makes applications speak— as opposed to speaking the screen. Emacspeak is the first full-fledged speech output system that will allow someone who cannot see to work directly on a UNIX system (Until now, the only option available to visually impaired users has been to use a talking PC as a terminal.) Emacspeak is built on top of Emacs. Once Emacs is started, the user gets complete spoken feedback. I currently use Emacspeak at work on my SUN SparcStation and have also used it on a DECAL- PHA workstation under Digital UNIX while at Di- gital’s CRL􏰄 . I also use Emacspeak as the only speech output system on my laptop running Linux. Emacspeak is available on the Internet: FTP ftp://crl.dec.com/pub/digital/emacspeak/ WWW http://www.research.digital.com/CRL",T. Raman,"access to UNIX workstations, direct speech access",32,36
10.1145/228347.228355,ASSETS,1996,Combining haptic and braille technologies,design issues and pilot study," This article describes design issues for a bi-dimensional single cell braille display, called  Pantobraille, combining a standard braille cell with a force feedback device developed as part of the  Cffl's PC-Access project. The Pantobraille, with a 10xl6cm workspace, allows the user to place the pointer  on a graphical interface, to perceive forms and textures using the sense of touch, and to read braille  text on a bi-dimensional page. In order to determine the usability of such a device and to have a better  understanding of the issues that may arise when manipulating it for actual interactive tasks, two visually  handicapped persons were asked to use the device to follow reading patterns with one or two hands. Reading  performance and comfort with the Pantobraille remain inferior to standard braille displays but significant  improvments were observed while performing the complementary pointing and reading tasks using both hands.  Keywords Single cell braille display, haptic interface, force feedback device, braille display ",Christophe Ramstein,"braille display, force feedback device, haptic interface, single cell braille display",37,44
10.1145/228347.228356,ASSETS,1996,Interactive tactile display system,a support system for the visually disabled to recognize 3D objects,"We have developed an interactive tactile display system for the visually disabled to actively recognize three-dimensional objects or environments. The display presents visual patterns by tactile pins arranged in two-dimensional format. The pin height can be set to several levels to increase the touch information and to display a three-dimensional surface shape. Also, each pin has a tact switch in the bottom for the user to make the system know the position by pushing it. This paper describes the hardware and software of the system.","Yoshihiro Kawai, Fumiaki Tomita","3D, interactive interface, stereo vision, tactile display, the visually disabled",45,50
10.1145/228347.228357,ASSETS,1996,Audiograf,a diagram-reader for the blind," In technical  reports and papers interrelations are often represented as diagrams. With the aid of a touch panel and  auditory display AudioGraf enables blind and visually impaired people to read such diagrams. The diagram  is displayed on the touch panel where a part can be selected with a finger. The selected part will be  auditorally displayed. If the finger is moved, another part is selected and auditorally displayed. This  way the whole diagram can be explored in an audio-tactile way. A model of this audio-tactile exploration  is presented. Based on this model it is explained how AudioGraf supports the user. Usability tests have  shown that simple diagrams can be read by blind users within relative short time. KEYWORDS: Auditory  user interfaces, blind users, usability, diagram, reading-aid 1. ",Andrea Kennel,"auditory user interfaces, blind users, diagram, reading-aid, usability",51,56
10.1145/228347.228358,ASSETS,1996,"EVA, an early vocalization analyzer: an empirical validity study of computer categorization", ," Previous research indicates that infant vocalizations  are effective predictors of later articulation and language abilities (Locke, 1989, Menyuk, Liebergott,  Shultz, Chesnick &#38; Ferrier, 1991, Oller &#38; Seibert 1988, Jensen, Boggild-Andersen, Schmidt, Ankerhus,  Hansen, 1988). Intervention to encourage babbling activity in at-risk infants is frequently undertaken.  Research and clinical diagnosis of delayed or reduced babbling have so far relied on time-consuming and  unreliable perceptual analyses of recorded infant sounds. While acoustic analysis of infant sounds has  provided important information on the early characteristics of infant vocalizations (Bauer, 1988, Stark  1986) this information has still to be used to carry out automatic, real-time analysis. We are developing  a program, EVA, for the Macintosh computer that automatically analyzes digitized recordings of infant  vocalizations. We describe the prototype and report on validityÂ­testing of the first stage of development.  Our human judge and EVA had 92.8% agreement on the number of utterances in the 20 minutes of recordings,  commonly identifying 411 utterances. Their categorizations agreed 79.8% for duration and 87.3% for frequency,  better than human inter-judge agreement reported in the literature. The authors hope that the final version  of EVA will serve as a reliable standard for the analysis and evaluation of utterances of normal and  at-risk infants with a variety of etiologies. The acoustic information gained from such analysis will  allow us to develop a computer-based system to encourage early vocalization. KEYWORDS infants -pre-speech  vocalization -acoustic analysis -early intervention. Â· Permission to make digital/hard copies of all  or part ofthis material for personal or classroom use is granted without fee provided that the copies  a.re not ~ade or ~istributed for l?ro~t or co~~ercial advantage, the copyÂ­ nght nottce, the tttle of  the pubhcat10n and tis date appear, and notice is given tha.t copyright is by permission of the ACM,  Inc. To copy otherwise, to repubhsh, to post on servers or to redistribute to lists, requires specific  permission and/or fee. ASSETS '96, Vancouver, British Columbia Canada 0 1996 ACM 0-89791-776-6/96/04  .. $3.50  1 ","Harriet Fell, Linda Ferrier, Zehra Mooraj, Etienne Benson, Dale Schneider","acoustic analysis, early intervention, infants, pre-speech vocalization",57,63
10.1145/228347.228359,ASSETS,1996,An approach to the evaluation of assistive technology, ," A valid criticism of may innovations in assistive technology is that they have not heen evaluated.  However. there are obstacles which make this form of technology difficult to evaluate according to conventional  paradigms. The reasons behind this are discussed. A particular evaluation which enÂ­deavoured to circumvent  those problems is described. The item evaluated was Matllta/k, a program to make mathematics accessible  to blind people. KEYWORDS: Evaluation, auditory interfaces. earcons, blind people. mathematics. ","Robert Stevens, Alistair Edwards","auditory interfaces, blind people, earcons, mathematics",64,71
10.1145/228347.228360,ASSETS,1996,Designing interface toolkit with dynamic selectable modality, ," Incorporating  flexibility to select desirable modality into user interface systems is needed for people with disabilities,  since most modern applications use graphical user interfaces forcing fixed modality which is useful only  to sighted users. However, the requirement of user interfaces with flexible and selectable modality is  not a specific problem of disabled persons but a general problem of interfaces in the next genÂ­eration,  considering that environments, in which computers are used, are widening rapidly. This paper discusses  about an architecture of user interface toolkit to support flexibility required by both users with disability  and users in special environment, and proposes a model of semantic abstraction of user interaction, named  abstract widgets. The experimental implementation of such toolkit, named Fruit system, is also described.  KEYWORDS: Multi-modal interface, Graphical User InÂ­terface, User Interface Management System  ","Shiro Kawai, Hitoshi Aida, Tadao Saito","graphical user interface, multi-modal interface, user interface management system",72,79
10.1145/228347.228361,ASSETS,1996,Multimodal input for computer access and augmentative communication, ," This paper describes the overall goals of a project  that focuses on multimodal input for computer access and Augmentative and Alternative Communication (AAC)  systems. In particular the project explores the integration of speech recognition with headÂ­pointing.  The first part of this project addresses the use of speech and head-pointing to replace the traditional  keyboard and mouse. While either of these technologies can emulate both keyboard and mouse functions,  it is hypothesized that the most advantageous use of each technology will come from integration such  that each device's strength is utilized appropriately. To test this hypothesis, a series of experiments  are planned. The first experiment compares (quantitatively and qualitatively) each technology in the  context of text generation. The second experiÂ­ment looks at typical pointing tasks (e.g., dragging) for  each techÂ­nology. The third experiment will look at the technologies in an integrated context. Because  each of the technologies are themÂ­selves highly complex, significant time and effort has been devoted  to pilot testing. Those results and the implications on our research methodology are presented in this  paper.  ","Alice Smith, John Dunaway, Patrick Demasco, Denise Peischl","assistive technology, augmentative and alternative communication, computer access, head pointing, multimodal input, speech recognition",80,85
10.1145/228347.228362,ASSETS,1996,The keybowl,an ergonomically designed document processing device," This paper discloses preliminary findings and provides a discussion of a newly  designed alphanumeric input device called the Keybowl. The Keybowl was designed and developed primarily  as an alternative input device to allow users of various upper extremity disabilities to effectively  type, interact with, and navigate current computer interface designs. In addition, the Keybowl's unique  characteristics of adapting to the user's needs may provide a solution to the multi-million dollar a  year problem of carpal tunnel syndrome (CTS) as it relates to typing. The Keybowl totally eliminates  finger movement, minimizes wrist movement, and uses the concept of concurrent independent inputs (i.e.,  chording) in which two domes are moved laterally to type. Initial results indicated that users of the  Keybowl typed an average of 52% of their regular QWERTY flatboard keying speed in as little as five hours.  With regard to ergonomic advantage, Keybowl typists' flexion/extension wrist movements were reduced by  an average of 81.5% when compared to typists using the QWERTY keyboard. Movements in the ulnar/radial  plane were reduced by an average of 48%. KEYWORDS: keyboard, cumulative trauma, handicap, typing, carpal  tunnel syndrome ","Peter McAlindon, Kay Stanney","carpal tunnel syndrome, cumulative trauma, handicap, keyboard, typing",86,93
10.1145/228347.228363,ASSETS,1996,Designing the World Wide Web for people with disabilities,a user centered design approach," The emergence of the World Wide Web  has made it possible for individuals with appropriate computer and telecommunications equipment to interact  as never before. An explosion of next-generation information systems are flooding the commercial market.  This cyberspace convergence of data, computers, networks, and multimedia presents exciting challenges  to interface designers. However, this ""new technology frontier"" has also created enormous roadblocks  and barriers for people with disabilities. This panel will discuss specific issues, suggest potential  solutions and solicit contributions required to design an accessible Web interface that includes people  with disabilities. Permission to make digital/hard copies of all or part of this material for personal  or classroom use is granted without fee provided that the copies are not made or distributed for profit  or commercial advantage, the copyÂ­right notice, the title of the publication and ita date appear, and  notice is given that copyright is by permission of the ACM, Inc. To copy otherwise, to republish, to  post on servers or to redistribute to lists, requires specific permission and/or fee. ASSETS '96, Vancouver,  British Columbia, Canada o 1996 ACM 0-89791-776-6/96/04 .. $3.50 KEYWORDS: accessibility, blindness,  deaf, disabilities, hypermedia, mobility, people with disabilities, special needs, software development,  user interfaces, user requirements ","Lila Laux, Peter McNally, Michael Paciello, Gregg Vanderheiden","accessibility, blindness, deaf, disabilities, hypermedia, mobility, people with disabilities, software development, special needs, user interfaces, user requirements",94,101
10.1145/228347.228364,ASSETS,1996,A gesture recognition architecture for sign language, ," This paper presents a gesture recognition architecture  dedicated to Sign Languages. Sign Language gestures include five coÂ­occurring parameters, which convey  complementary independent information. Some signs belong to a predefined lexicon which can be learned  by the recognition system, but some other signs may be created during the discourses, depending on the  context. The proposed recognition system is able to recognise both kinds of signs, by using specific  classification tools, and a virtual scene for context storage. It is based on a study of French Sign  Language (LSF). KEYWORDS: Sign Language, Gesture Recognition, Gesture Interpretation, Data Glove ",Annelies Braffort,"data glove, gesture interpretation, gesture recognition, sign language",102,109
10.1145/228347.228365,ASSETS,1996,&#8220;Composability&#8221;,widening participation in music making for people with disabilities via music software and controller solutions," This paper discusses ways of enabling visually impaired and physically  disabled people to compose and perform music. The usage and adaptation of existing software-based composition  systems are described, in the context of education work undertaken by the Drake Music Project -a charity  which aims to facilitate disabled people in making music via technology. Some of the problems faced arc  discussed, and a custom system presented which aims to resolve some of these difficulties. KEYWORDS Music.  Physical Disability, Visual Impairment, Composition, Education, Software, MIDI, Adaptive Technology.  ","Tim Anderson, Clare Smith","MIDI, adaptive technology, composition, education, music, physical disability, visual impairment",110,116
10.1145/228347.228366,ASSETS,1996,A generic direct-manipulation 3D-auditory environment for hierarchical navigation in non-visual interaction, ," Auditory presentation  methods may significantly enhance the interaction quality during user-computer dialogue. The impact of  auditory interaction methods is important in the context of non-visual interaction, where audio is the  primary direct perception output modality. In a few cases, 3D-audio output techniques have been employed  for providing interaction for blind users. Unfortunately, such developments have been too specialized  and do not support re-usability of the implemented approaches and techniques in different contexts, where  non-visual interaction needs to be realized. A generic re-usable environment has been implemented, based  on 3D audio, 3D pointing, hand gestures and voice input, which is applicable in all cases that interactive  hierarchically structured selections from sets of alternatives must be handled. This environment has  been used to implement the hierarchical navigation dialogue in a multiÂ­media non-visual toolkit currently  under development. It is composed of a set of modules implementing re-usable functionality with which  interaction for non-visual hierarchical navigation can be realized within any non-visual interaction  toolkit. KEYWORDS: Non-visual toolkits, 3D-audio, re-usabi interaction, auditory lity. interfaces, ","Anthony Savidis, Constantine Stephanidis, Andreas Korte, Kai Crispien, Klaus Fellbaum","3D-audio, auditory interfaces, non-visual interaction, re-usability, toolkits",117,123
10.1145/228347.228367,ASSETS,1996,Improving the usability of speech-based interfaces for blind users, ," Adaptations using speech synthesis provide a basic  level of access to computer systems for blind users, but current systems pose a number of usability problems.  A study was carried out in order to assess the impact of certain issues on the usability of a typical  speech adaptation. The results suggest that much work needs to be done on the design of speech dialogues.  1. ","Ian Pitt, Alistair Edwards", ,124,130
10.1145/228347.228368,ASSETS,1996,TDraw,a computer-based tactile drawing tool for blind people,"Considerations about blind people?s relation to pictures of real world objects lead to the conclusion that blind and sighted people have very similar mental models of the 3D world. Because perception works completely differently, the mapping of the 3D world to a 2D picture differs significantly. A tool has been developed to allow blind people to draw pictures and at the same time study their drawing process. A first evaluation shows interesting results. These will eventually lead to a design of a rendering tool for (tactile) pictures for blind people.",Martin Kurze,"TDraw, mental model, tactile drawing tool, tactile drawings, tactile rendering",131,138
10.1145/228347.228369,ASSETS,1996,Development of dialogue systems for a mobility aid for blind people,initial design and usability testing,"  This paper presents a new travel aid to increase the independent mobility of blind and elderly travellers.  This aid builds on the technologies of geographical information systems (GIS) and the Global Positioning  System (GPS). The MoBIC Travel Aid (MoTA) consists of two interrelated components: the MoBIC Pre-journey  System (MoPS) to assist users in planning journeys and the MoBIC Outdoor System (MoODS) to execute these  plans by providing users with orientation and navigation assistance during journeys. The MoBIC travel  aid is complementary to primary mobility aids such as the long cane or guide dog. Results of a study  of user requirements, the user interface designs, and the first field trial, currently being conducted  in Berlin, are presented. KEYWORDS: visually disabled users; mobility and navigation; GPS; GIS; user  trials. ","Thomas Strothotte, Steffi Fritz, Rainer Michel, Andreas Raab, Helen Petrie, Valerie Johnson, Lars Reichert, Axel Schalt","GIS, GPS, mobility and navigation, user trials, visually disabled users",139,144
10.1145/274497.274499,ASSETS,1998,Comparing effects of navigational interface modalities on speaker prosodics, ,"Displayless interface technology must address issues similar to those of GUI access technology for users with visual impairments. Both must address the issue of providing nonvisual access to spatial data. This research examined the hypothesis that strictly verbal access to spatial datsi places a cognitive burden on the user, which in turn impacts the prosodies, i.e., nonverbal aspects, of the user’s speech. The hypothesis was tested through experiments in which subjects used speech-based, displayless interface followed by a multimodal interface to perform a series of navigational tasks. Their speech was recorded during the experiments and post-processed for prosodic content. Statistical analysis of the post-processed data showed significant differences in subjects’ prosodies when using the displayless versus the multimodal interface.",Julie Baca,"GUI access, displayless interfaces prosodies",3,10
10.1145/274497.274502,ASSETS,1998,Computer-based cognitive prosthetics,assistive technology for the treatment of cognitive disabilities,"Traumatic brain injury and stroke leave many individuals with cognitive disabilities even after much therapy. For over a decade, our multidisciplinary group has been conducting a research and clinical program. The focus of our efforts has been restoration of individual?s functioning through technology enabling them to perform some of their priority everyday activities. Our approach has been three-fold: 1) the application of theory and methods from computer science; 2) the design of one-of-a-kind prosthetic systems to bridge deficits, and 3) therapy integrated tightly with prosthetic technology. Research incorporated the single-subject case study approach - widely used in brain injury rehabilitation - with studies being partial replicates for grouping data. Results have been significant and substantial, with an increase of function being the rule rather than the exception. An important finding is that our evaluation techniques of patient abilities tends to show greater abilities than show in clinical testing These abilities can be used in participatory design to greatly enhance the clinical outcome. Also, the impact of small deficits on behavior seems to be significantly greater than one would expect. Resolving or bridging small deficits can have considerable behavioral impact.","Elliot Cole, Parto Dehdashti","cognitive disabilities, cognitive prosthetics, health care applications, learning disabilities, personal productivity tools, usability testing, user interfaces, user studies",11,18
10.1145/274497.274503,ASSETS,1998,Toward the use of speech and natural language technology in intervention for a language-disordered population, ,"We describe the design of Simone Says an interactive software environment for language remediation that brings together research in speech recognition, natural language processing and computer-aided instruction. The underlying technology for the implementation and the systems eventual evaluation are also discussed.",Jill Lehman, ,19,26
10.1145/274497.274504,ASSETS,1998,Lessons from developing audio HTML interface, ,"In this paper, we discuss the choice of specific sounds to use in an audio HTML interface, based on our previous research into developing principles for sound choice, called the AHA framework. AHA can be used along with the consideration of issues related to the target audience such as user tasks, goals, and interests to choose specific sounds for an interface. We describe two scenarios of potential users and interfaces that would seem to be appropriate for them.",Frankie James,"HTML, WWW, audio interfaces, blind, human-computer interaction",27,34
10.1145/274497.274505,ASSETS,1998,The use of gestures in multimodal input, ,"For users with motion impairments, the standard keyboard and mouse arrangement for computer access often presents problems. Other approaches have to be adopted to overcome this. In this paper, we will describe the development of a prototype multimodal input system based on two gestural input channels. Results from extensive user trials of this system are presented. These trials showed that the physical and cognitive loads on the user can quickly become excessive and detrimental to the interaction. Designers of multimodal input systems need to be aware of this and perform regular user trials to minimize the problem.","Simeon Keates, Peter Robinson","gesture recognition, multimodal input, user trials",35,42
10.1145/274497.274506,ASSETS,1998,VRML-based representations of ASL fingerspelling on the World Wide Web, ,"Virtual Reality Modeling Language (VRML) is an effective tool to document sign language on the World-Wide Web. In this paper, we present techniques to enlarge the vocabulary of encoded ASL signs in VRML 2.0 for educational purposes. In order to prove the concept of gesture making, a Web site is presented that demonstrates application of the hand model to fingerspell the ASL manual alphabet and numbers.","S. Su, Richard Furuta","American Sign Language, Virtual Reality Modeling Language, World Wide Web, hand gestures",43,45
10.1145/274497.274507,ASSETS,1998,Programming for usability in nonvisual user interfaces, ,Standard software engineering methods are not directly applicable to nonvisual user inter- faces due to the mismatch of user interfaces of developers and users. We have developed tools to visualize the nonvisual presentation and the nonvisual interaction. This requires to apply software interfaces as used by screen readers to collect data about widgets.,Gerhard Weber,"evaluation, nonvisual user interfaces, software engineering",46,48
10.1145/274497.274509,ASSETS,1998,Conversational gestures for direct manipulation on the audio desktop, ,We describe the speech-enabling approach to building auditory interfaces that treat speech as a first-class modality. The process of designing effective auditory interfaces is decomposed into identifying the atomic actions that make up the user interaction and the conversational gestures that enable these actions. The auditory interface is then synthesized by mapping these conversational gestures to appropriate primitives in the auditory environment. We illustrate this process with a concrete example by developing an auditory interface to the visually intensive task of playing tetris. Playing Tetris is a fun activity that has many of the same demands as day-to-day activities on the electronic desktop. Speech-enabling Tetris thus not only provides a fun way to exercise ones geometric reasoning abilities - it provides useful lessons in speech-enabling commonplace computing tasks.,T. Raman, ,51,58
10.1145/274497.274510,ASSETS,1998,Automatic babble recognition for early detection of speech related disorders, ,"We have developed a program, the Early Vocalization Analyzer (EVA), that automatically analyzes digitized recordings of infant vocalizations. The purpose of such a system is to automatically and reliably screen infants who may be at risk for later communication problems. Applying the landmark detection theory of Stevens et al., for the recognition of features in adult speech, EVA detects syllables in vocalizations produced by typically developing six to thirteen month old infants. We discuss the differences between adult-specific code and code written to analyze infant vocalizations and present the results of validity-testing.","Harriet Fell, Joel MacAuslan, Karen Chenausky, Linda Ferrier","acoustic analysis, early intervention, infants, pre-speech vocalization",59,66
10.1145/274497.274511,ASSETS,1998,A tool for creating eye-aware applications that adapt to changes in user behaviors, ,"A development tool is described that can be used to create eye-aware software applications that adapt in real-time to changes in a user's natural eye-movement behaviors and intentions. The research involved in developing this tool focuses on identifying patterns of eye-movement that describe three behaviors: Knowledgeable Movement, Searching, and Prolonged Searching. In the process of doing the research, two important features of eye-movement patterns were discovered-Revisits and Significant Fixations. Revisits and Significant Fixations complement the recognition of saccades, fixations, and blinks, and make easier the recognition of high-level patterns in users' natural eye-movements.",Greg Edwards,"eye interpretation engine, eye-aware, eyetracking, fixation duration, fixations, human-computer interaction, user intent, user-centered approach, visual search",67,74
10.1145/274497.274512,ASSETS,1998,Designing interfaces for an overlooked user group,considering the visual profiles of partially sighted users,"In this position paper we argue the importance of research focusing on the issues involved in designing computer systems for partially sighted computer users. Currently, there is a lack of data that explores how combinations of impaired visual processes affect preferences for, and performance with, graphical user interfaces. This lack of fundamental information about how an individual's visual profile determines the strategies and behaviors exhibited while using computers limits our ability to design effective user interfaces for partially sighted computer users. The objective of this position paper is to motivate research that addresses this deficiency in our knowledge base so that researchers can design enabling technologies in a systematic fashion for this unique user group as has been done for fully sighted users and blind users.","Julie Jacko, Andrew Sears","enabling technologies, human-computer interaction, partial vision, visually impaired",75,77
10.1145/274497.274513,ASSETS,1998,Modeling and generating sign language as animated line drawings, ,"This paper introduces an application for creating words and sentences of sign language as animated gesture sequences. A gesture is composed of the left and right hand sign, a body movement and a facial expression. We propose a technique for generating gestures as line drawings. Using line drawings allows us to run the application with simple 3D models without loss of essential information while achieving images which can be transferred very quickly over a network. Furthermore, the images resemble those used in printed teaching materials for sign language.","Frank Godenschweger, Thomas Strothotte", ,78,84
10.1145/274497.274514,ASSETS,1998,TGuide,a guidance system for tactile image exploration,We present a guidance system for blind people exploring tactile graphics. The system is composed of a new device using 8 vibrating elements to output directional information and a guidance software controlling the device. The evaluation of the system is also described.,Martin Kurze,"blind people, evaluation, graphics, guidance, tactile output device",85,91
10.1145/274497.274515,ASSETS,1998,Haptic virtual reality for blind computer users, ,"This paper describes a series of studies involving a haptic device which can display virtual textures and 3-D objects. The device has potential for simulating real world objects and assisting in the navigation of virtual environments. Three experiments investigated: (a) whether previous results from experiments using real textures could be replicated using virtual textures; (b) whether participants perceived virtual objects to have the intended size and angle; and (c) whether simulated real objects could be recognised. In all the experiments differences in perception by blind and sighted people were also explored. The results have implications for the future design of VEs in that it cannot be assumed that virtual textures and objects will feel to the user as the designer intends. However, they do show that a haptic interface has considerable potential for blind computer users.","Chetz Colwell, Helen Petrie, Diana Kornbrot, Andrew Hardwick, Stephen Furner","World Wide Web, blind users, haptic device, perception of virtual textures and objects, virtual environments",92,99
10.1145/274497.274516,ASSETS,1998,Auditory navigation in hyperspace,design and evaluation of a non-visual hypermedia system for blind users,"This paper presents the design and evaluation of a hypermedia system for blind users, making use of a non-visual interface, non-speech sounds, three input devices, and a 37-node hypermedia module. The important components of an effective auditory interface are discussed, together with the design of the auditory interface to hypermedia material. The evaluation is described, which was conducted over several weeks, and used a range of complementary objective and subjective measures to assess usability, performance and user preferences. The findings from evaluations with 9 visually impaired student participants are presented. The results from this research can be applied to the design and evaluation of other non-visual hypermedia systems, such as auditory WWW browsers and digital talking books.","Sarah Morley, Helen Petrie, Anne-Marie O'Neill, Peter McNally","auditory navigation of hypermedia, blind and visually impaired users, evaluation methodology, non-speech sounds, non-visual interface design",100,107
10.1145/274497.274517,ASSETS,1998,SUITEKeys,a speech understanding interface for the motor-control challenged,"This paper presents SUITEKeys, a continuous speech understanding interface for motor-control challenged computer users. This interface provides access to all available functionality of a computer by modeling interaction at the physical keyboard and mouse level. The paper briefly discusses the advantages and disadvantages of using speech at the user interface; it outlines the user- centered approach employed in developing the system; it introduces the formal model of the user interface in terms of its conceptual, semantic, syntactic, lexical and acoustic levels; it describes the SUITEKeys system architecture which consists of symbolic, statistical, and connectionist components; it presents a pilot study for assessing the effectiveness of speech as an alternate input modality for motor-control challenged users; and closes with directions for future research.","Bill Manaris, Alan Harkreader","accessibility, input devices, intelligent user interfaces, keyboard, motor-disabilities, mouse, natural language, selectable modalities, speech recognition",108,115
10.1145/274497.274518,ASSETS,1998,Adaptation of a cash dispenser to the needs of blind and visually impaired people, ,"An existing cash dispenser was implemented with speech output to give access to blind and visually impaired people. Additionally, the screen graphics and the function access were modified. The hardware was not changed. Blind and visually impaired subjects performed a usability-test, and experts in the field of human-computer-interaction evaluated the dispenser system?s usability heuristically. The results showed that the modifications help blind and visually impaired people to access such machines, but adaptations of the hardware are necessary to maintain usability. The two evaluation methods did not produce consistent results.",Jens Manzke,"ATM, application design, automatic teller machine, blind and visually impaired users, cash dispenser, heuristic evaluation, usability-test",116,123
10.1145/274497.274519,ASSETS,1998,Some thoughts on assistive technology for the blind, , ,Abraham Nemeth, ,124,125
10.1145/274497.274521,ASSETS,1998,An interactive method for accessing tables in HTML, ,"Although visually impaired people can access digital information by using computers, GUIs make it difficult for them to do so. One of the main obstacles preventing them from taking advantage of the almost unlimited information resources on the Web is the use of visual representations such as tables, image maps, and classified structures. This paper proposes a method for converting these visual representations into non-visual representations in HMTL. After describing a system that we developed to evaluate our method, we will discuss an interactive method for accessing tables in HTML files.","Toshiya Oogane, Chieko Asakawa","HTML, WWW, blind, conversion, table, visually disabled",126,128
10.1145/274497.274523,ASSETS,1998,Dual level intraframe coding for increased video telecommunication bandwidth, ,"While digital video transmission and video conferencing methods have improved significantly over the last few years, the transmission of sign language for individuals who are deaf via this medium still remains a problem. Desktop video teleconferencing systems accommodate the bandwidth limitations of both analog and digital (ISDN) telephone channels by reducing the frame rate while preserving voice quality and only minimally degrading image quality. Sign language transmission requires fidelity to movement (consistent and high frame rate), and requires reasonable image quality only in the areas around the hands and face. This paper presents a dual-level compression approach which uses a newly developed technique to identify the hands and face from the remainder of each video frame. This allows for a very lossy, high compression of most of each frame, while retaining the visual quality necessary to identify hand shapes and read facial expressions. By taking advantage of this compression, additional bandwidth is recaptured to allow an acceptable frame rate that maintains the fidelity of human movement necessary to represent sign language.","David Saxe, Richard Foulds, Arthur Joyce","disability access, gesture, hearing impairments, sign language",130,135
10.1145/274497.274524,ASSETS,1998,Reading and writing mathematics,the MAVIS project,"One of the greatest challenges to the visually impaired student in science and mathematics disciplines is the reading and writing of complex mathematical equations or have convenient access to information based tools such as the world wide web. In research currently underway at New Mexico State University, tools are being built using logic programming to facilitate access to complex information in a variety of formats. On top of the logic based tools, new interfaces are being designed to permit more convenient access to information by our visually impaired students.","A. Karshmer, G. Gupta, S. Geiiger, C. Weaver","LaTeX, Nemeth code, education, mathematics",136,143
10.1145/274497.274525,ASSETS,1998,Making VRML accessible for people with disabilities, ,Making VRML accessible for people with disabilities,"Sandy Ressler, Qiming Wang","VRML, accessibility, audio feedback, data access, navigational aids, speech input, user interfaces, virtual environments",144,148
10.1145/274497.274526,ASSETS,1998,User interface of a Home Page Reader, ,User interface of a Home Page Reader,"Chieko Asakawa, Takashi Itoh","GUI, WWW, blind, home page reader, numeric keypad, visually disabled",149,156
10.1145/274497.274527,ASSETS,1998,Digital talking books on a PC,a usability evaluation of the prototype DAISY playback software,"This paper describes the design and evaluation of the first system to play digital talking books on a PC: the DAISY Playback Software. The features of the software for navigating through structured digital audio are described. A detailed usability evaluation of this prototype software was designed and conducted to assess its current usability, in which 13 blind/partially sighted participants completed a series of realistic tasks and answered detailed usability questions on the system. Recommendations for improvements are presented which might inform designers of similar systems, such as other digital talking book systems or WWW browsers.",Sarah Morley,"auditory navigation, blind and visually impaired readers, digital talking books, evaluation methodology, structured information access",157,164
10.1145/274497.274528,ASSETS,1998,A phoneme probability display for individuals with hearing disabilities, ,We are building an aid for individuals with hearing impairments which converts continuous speech into an animated visual display. A speech analysis system continuously estimates phoneme probabilities from the input acoustic stream. Phoneme symbols are displayed graphically with brightness in proportion to estimated phoneme probabilities. We use an automated layout algorithm to design the display to group acoustically confusable phonemes together in the graphical display.,"Deb Roy, Alex Pentland", ,165,168
10.1145/274497.274529,ASSETS,1998,Augmenting home and office environments, ,"In this panel, we describe different techniques and applications of augmenting home and office environments. One application of augmented environments is to provide additional information associated with the environment via visual and / or auditory cues. Other applications assist users hi controlling aspects of their environment. Commercial opportunities in home automation allow people to more easily operate complex systems for temperature control, security, and maintenance. There are numerous research issues in designing augmented environments such as how multimodal input and output can be used effectively. Many of these systems need to assume some knowledge of the user?s intent and context. How to capture and interpret information about users in these environments is an open question. We will describe these issues during this panel as well as discuss with the ASSETS community how these efforts can be applied to the realm of assistive technology.","Elizabeth Mynatt, Douglas Blattner, Meera Blattner, Blair MacIntyre, Jennifer Mankoff","audio, augmented reality, home automation, multimodal, see-through displays, ubiquitous computing",169,172
10.1145/274497.274530,ASSETS,1998,A model of keyboard configuration requirements, ,"This paper presents a user model: a computer program which examines the behaviour of a real computer user. The model encompasses four aspects of keyboard use which can present difficulties for people with motor disabilities. Where relevant keyboard configuratibn options exist, the model chooses appropriate settings for these options. The model bases its recommendations on observation of users typing free English text. It is intended to form part of a dynamic configuration support tool. Empirical evaluation showed the model to be very accurate in identification of a given user?s difficulties. Where recommended configuration options were tried by the participants, high levels of error reduction and user satisfaction were found.","Shari Trewin, Helen Pain","Bounce Keys, Repeat Keys, Sticky Keys, empirical studies, keyboard configuration, keyboards, motor disabilities, user modelling",173,181
10.1145/274497.274531,ASSETS,1998,Head pointing and speech control as a hands-free interface to desktop computing, ,"A significant number of users are not able to use today's WIMP-style (Windows, Icons, Menus, and Pointers) Graphical User Interfaces (GUIs). This may be for different reasons including hands-busy situations (e.g., a mechanic at work), paralysis, or bad neural control of body movements. To overcome these difficulties in the practical use of existing software applications, solutions have to consider both technical and commercial aspects. The system introduced by this work addresses both goals, i.e., develops and customises existing technical solutions and keeps an eye on the end-user's costs.",Rainer Malkewitz,"disabled users, head gestures, pointing device, speech input",182,188
10.1145/274497.274532,ASSETS,1998,Factors leading to the successful use of voice recognition technology, ,"In this paper, results are presented from a field study of individuals with disabilities who used voice recognition technology (VRT). The perceived benefits of the VRT and the ability to use the VRT for a trial period were determined to be the major factors resulting in successful adoption of the technology.",Tanya Goette,"disability access, empirical studies, input/output devices, motor disabilities, speech and voice, user studies",189,196
10.1145/274497.274533,ASSETS,1998,Towards an EOG-based eye tracker for computer control, ,The authors are developing an eye tracking system for use with personal computers. The system is intended to provide a pointing device that could be useful to some people with physical disabilities. The basis for this system is the use of Bio-Electrical signals from the user's body. In particular the authors are investigating the use of the Electrooculogram and Visual Evoked Potentials. This paper describes an experiment to compare two algorithms for processing the signals and generate an effective output control.,"David Patmore, R. Knapp","EOG, VEP, electrooculogram, eye tracking, visual evoked potentials",197,203
10.1145/274497.274534,ASSETS,1998,A Web navigation tool for the blind, ,The aim of our work is to make the wealth of information on the World Wide Web more readily available to blind people. They must be able to search efficiently for relevant information and make quick and effective decisions about the usefulness of pages they retrieve. We have built a prototype application called BrookesTalk which we believe addresses this need more fully than ouier Web browsers. Information retrieval techniques are used to provide a set of complementary options which summarise a Web page and enable rapid decisions about its usefulness.,"Mary Zajicek, Chris Powell, Chris Reeves","HTML, World Wide Web, blind, browser, information retrieval",204,206
10.1145/354324.354327,ASSETS,2000,Low vision,the role of visual acuity in the efficiency of cursor movement,"  Graphical user interfaces are one of the more prevalent interface types which exist today. The popularity  of this interface type has caused problems for users with poor vision. Because usage strategies of low  vision users differ from blind users, existing research focusing on blind users is not sufficient in  describing the techniques employed by low vision users. The research presented here characterizes the  interaction strategies of a particular set of low vision users, those with Age-related Macular Degeneration,  using an analysis of cursor movement. The low vision users have been grouped according to the severity  of their vision loss and then compared to fully sighted individuals, with respect to cursor movement  efficiency. Results revealed that as the size of the icons on the computer screen increased, so did the  performance of the fully sighted participants as well as the participants with AMD. Keywords Low vision,  cursor movement, icon size, age-related macular degeneration, search strategy, graphical user interface,  GUI ","Julie Jacko, Armando Barreto, Gottlieb Marmet, Josey Chu, Holly Bautsch, Ingrid Scott, Robert Rosa","GUI, age-related macular degeneration, cursor movement, graphical user interface, icon size, low vision, search strategy",1,8
10.1145/354324.354329,ASSETS,2000,A framework of assistive pointers for low vision users, ," Manipulating a mouse pointer is often difficult for the low  vision computer user. Working with such a small, mobile screen object is very visually demanding. Although  several techniques have been used to address this problem, the design space of assistive pointers has  not been fully explored by the current state of the art. This paper proposes a four dimensional framework  to fully articulate the design space of assistive pointers for low vision users. The dimensions of the  framework describe the key attributes of assistance offered to users by any pointing solution: the perceptual  channel that carries the assistance, the stage of targeting supported by the assistance, the relationship  between the assistance and the interface, and the degree of availability associated with the assistance.   Keywords Low vision, partial vision, visual impairment, adaptive technology, mouse pointer, graphical  interface 1. ","Julie Fraser, Carl Gutwin","adaptive technology, graphical interface, low vision, mouse pointer, partial vision, visual impairment",9,16
10.1145/354324.354330,ASSETS,2000,Constructing sonified haptic line graphs for the blind student,first steps," Line graphs stand as an established information visualisation and analysis  technique taught at various levels of difficulty according to standard Mathematics curricula. It has  been argued that blind individuals cannot use line graphs as a visualisation and analytic tool because  they currently primarily exist in the visual medium. The research described in this paper aims at making  line graphs accessible to blind students through auditory and haptic media. We describe (1) our design  space for representing line graphs, (2) the technology we use to develop our prototypes and (3) the insights  from our preliminary work. KEYWORDS: force feedback, haptic display, line graphs, spatial sound, and  visual impairment ","Rameshsharma Ramloll, Wai Yu, Stephen Brewster, Beate Riedel, Mike Burton, Gisela Dimigen","force feedback, haptic display, line graphs, spatial sound, visual impairment",17,25
10.1145/354324.354332,ASSETS,2000,Tactile imaging using watershed-based image segmentation, ,"A new image segmentation method is proposed for the automatic conversion of images from visual to tactile form. The proposed method utilizes a watershed-based algorithm for obtaining the initial segmentation. A new joint region merging criterion is developed to reduce the number of initial regions in a more appropriate way. The joint criterion combines, in a weighted fashion, criteria based on region homogeneity and edge integrity, which have each been applied marginally with some relative success. However, each criteria has shown some significant drawbacks when applied independently. The new criterion takes joint advantage of these two methods, giving a final segmentation that is more visually appropriate. Experimental results are presented showing the advantages of the new merging criterion. Also, the proposed method is compared with multiresolution (MR) edge detection techniques for tactile imaging applications. Additional results are included showing the advantages of the segmentation procedure over MR edge detection techniques.","Sergio Hernandez, Kenneth Barner","blind and non-visual interfaces, image segmentation, tactile imaging",26,33
10.1145/354324.354334,ASSETS,2000,A study of blind drawing practice,creating graphical information without the visual channel," Existing drawing tools  for blind users give inadequate contextual feedback on the state of the drawing, leaving blind users  unable to comprehend and successfully produce graphical information. We have investigated a tactile method  of drawing used by blind users that mimics drawing with a pencil and a paper. Our study revealed a set  of properties that must be incorporated into drawing tools for blind users, including giving feedback  for relocating important points, determining angles, and communicating the overall structure of the drawing.  We describe a gridÂ­based model that provides these properties in a primitiveÂ­based 2D graphics environment,  and we introduce its use in drawing and other graphical interactions. KEYWORDS Non-visual drawing tools,  GUIs for blind users, contextual inquiry, feedback, grid ","Hesham Kamel, James Landay","GUIs for blind users, contextual inquiry, feedback, grid, non-visual drawing tools",34,41
10.1145/354324.354335,ASSETS,2000,New technology enables many-fold reduction in the cost of refreshable Braille displays, ,"  By analysis of the primary cost factors for existing refreshable Braille displays, a team at NIST has  pioneered a new technology that can reduce the cost of the electromechanical portions of a Braille display  by an extremely large factor, and the overall cost of a Braille display by as much as a factor of ten.  A massive cost reduction in displays creates a new model for the purchase and use of Braille displays  by individuals, by employers, and by educators. Readability and user control issues are are addressed.  It is hoped that this technology will open a significant new market for low cost, high performance refreshable  Braille displays. Keywords Refreshable Braille, passive pin retention, wheel.  ","John Roberts, Oliver Slattery, David Kardos, Brett Swope","passive pin retention, refreshable Braille, wheel",42,49
10.1145/354324.354338,ASSETS,2000,A storytelling robot for pediatric rehabilitation, ," We are developing  a prototype storytelling robot for use with children in rehabilitation. Children can remotely control  a large furry robot by using a variety of body sensors adapted to their disability or rehabilitation  goal. In doing so, they can teach the robot to act out series of movements or ""emotions"" and then write  stories -using a storytelling software - including those movements in the story. The story can then be  ""played"" by the remote controlled robot, which acts out the story. We believe that this robot can motivate  the children and help them reach their therapy goals through therapeutic play, either by exercising muscles  or joints (e.g. for physically challenges children) or by reflecting on the stories (e.g. for children  with developmental disabilities). We use an innovative design methodology involving children as design  partners. Keywords Therapeutic play, robot, children, user interface, design process, rehabilitation   ","Catherine Plaisant, Allison Druin, Corinna Lathan, Kapil Dakhane, Kris Edwards, Jack Vice, Jaime Montemayor","children, design process, rehabilitation, robot, therapeutic play, user interface",50,55
10.1145/354324.354340,ASSETS,2000,A virtual reality-based exercise program for stroke rehabilitation, ," A PC  based desktop Virtual Reality system was developed for rehabilitating hand function in stroke patients.  The system uses two hand input devices, a CyberGlove and a RMII force feedback glove, to allow the user  to interact with one of four rehabilitation exercises. Each of which is designed to exercise one specific  parameter of hand movement, namely range, speed, fractionation or strength. The therapy program is semi-automated  and personalized to each user through the use of performance-based target levels. These are adapted between  sessions in order to induce the user to improve. Feedback is provided to each user throughout the exercise  sessions. To further motivate the user to continue the exercise program, screen displays are designed  as interactive games. The system is described and sample data is presented from preliminary studies performed  on control subjects. Keywords Virtual Reality, Rehabilitation, Stroke, Haptic Glove, CyberGlove, Rutgers  Master II.  ","David Jack, Rares Boian, Alma Merians, Sergei Adamovich, Marilyn Tremaine, Michael Recce, Grigore Burdea, Howard Poizner","CyberGlove, Rutgers Master II, haptic glove, rehabilitation, stroke, virtual reality",56,63
10.1145/354324.354343,ASSETS,2000,Fast web by using updated content extraction and a bookmark facility, ," This paper describes  improved methods of web access for the visually impaired. Some A few web access systems for the visually  impaired have already been developed and are widely used. The improvements described in this paper are  in two areas. The first is a fastest mean of jumping from the current sentence position into desired  sentence position within web pages. The second is a facility for searching for sentences that have been  updated since a previous viewing. User testing was carried out, and the two facilities were found to  reduce not only the web page access time but also the user s mental workload. Keywords web access, visually  impaired, bookmark, updated-sentence search facility  ","Tsuyoshi Ebina, Seiji Igi, Teruhisa Miyake","bookmark, search facility, updated-sentence, visually impaired, web access",64,71
10.1145/354324.354345,ASSETS,2000,A comparison of voice controlled and mouse controlled web browsing, ," Voice controlled web browsers allow users to navigate by speaking the text of a link or an  associated number instead of clicking with a mouse. One such browser is Conversa, by Conversational Computing.  This within subjects study with 18 subjects compared voice browsing with traditional mouse-based browsing.  It attempted to identify which of three common hypertext forms (linear slide show, grid/tiled map, and  hierarchical menu) are well suited to voice navigation, and whether voice navigation is helped by numbering  links. The study shows that voice control adds approximately 50% to the performance time for certain  types of tasks. Subjective satisfaction measures indicate that for voice browsing, textual links are  preferable to numbered links. Keywords Human-computer interaction, user interfaces, voice browsers,  voice recognition, web browsing ","Kevin Christian, Bill Kules, Ben Shneiderman, Adel Youssef","human-computer, interaction, user interfaces, voice browsers, voice recognition, web browsing",72,79
10.1145/354324.354346,ASSETS,2000,Evaluating web resources for disability access, ," A majority of Web based information, facilities  and services is unnecessarily inaccessible to people with certain disabilities, largely due to a lack  of awareness of accessibility issues on the part of developers. This paper argues that currently available  accessibility evaluation methods are unsatisfactory in the scope and presentation of their results. Consequently,  there is a need for a metaÂ­method which utilises the strengths of current methods, but which also bridges  their weaknesses. The paper discusses a comprehensive, yet usable methodology for evaluating web sites  for accessibility. Using this methodology, a semiÂ­automatic accessibility evaluation tool is proposed,  which will guide evaluators through the auditing process and produce a set of tailored recommendations  for making the subject site accessible. Keywords Accessibility, Usability, Disability, Evaluation, Web  Resources. ","Murray Rowan, Peter Gregor, David Sloan, Paul Booth","accessibility, disability, evaluation, usability, web resources",80,84
10.1145/354324.354347,ASSETS,2000,An empirical investigation of ways in which some of the problems encountered by some dyslexics may be alleviated using computer techniques, ," This research describes the development of a highly configurable word processing environment  to alleviate some of the difficulties encountered by dyslexics when producing and reading text. It also  describes a pragmatic, empirical methodology, closely involving dyslexic users, which has proved highly  effective. All dyslexic subjects tested were able to use the software to identify and store a configuration  of background and foreground colour, text typeface and font, and spacing between characters, words and  lines which they found easier to read than the default settings. Successful tests were also carried out  to investigate the use of different appearances (font, colour etc.) to alleviate character recognition  and reversal problems. Keywords Dyslexia, user-centred design, configuration, word processing I","Peter Gregor, Alan Newell","configuration, dyslexia, user-centered design, word processing",85,91
10.1145/354324.354348,ASSETS,2000,An intelligent tutoring system for deaf learners of written English, ," This paper describes progress toward a prototype impleÂ­mentation  of a tool which aims to improve literacy in deaf high school and college students who are native (or  near native) signers of American Sign Language (ASL). We envision a system that will take a piece of  text written by a deaf student, analyze that text for grammatical errors, and engage that student in  a tutorial dialogue, enabling the stuÂ­dent to generate appropriate corrections to the text. A strong  focus of this work is to develop a system which adapts this process to the knowledge level and learning  strengths of the user and which has the .exibility to engage in multi-modal, multi-lingual tutorial instruction  utilizing both English and the native language of the user. Keywords Intelligent tutoring systems, user  modeling, English literÂ­acy, second language acquisition, American Sign Language ","Lisa Michaud, Kathleen McCoy, Christopher Pennington","American Sign Language, English literacy, intelligent tutoring systems, second language acquisition, user modeling",92,100
10.1145/354324.354349,ASSETS,2000,The development of language processing support for the ViSiCAST project, ,"ViSiCAST is a major new project funded by the European Union, aiming to provide improved access to services and facilities for deaf citizens through sign language presented by a virtual human, or avatar. We give here an outline of the project, and describe early work in the area of linguistics and language processing. This work covers two distinct but related areas: first, the development of an XML-compliant notation for deaf sign language gestures, which can be used to drive the signing avatar; and, second, the development of a framework supporting the translation of natural language text into this gesture-orientated notation.","R. Elliott, J. Glauert, J. Kennaway, I. Marshall","language processing, sign language, virtual signing",101,108
10.1145/354324.354350,ASSETS,2000,The LF-ASD brain computer interface,on-line identification of imagined finger flexions in subjects with spinal cord injuries," Our research  has focused on developing a brain-controlled switch that is suitable for asynchronous control applications.  We have developed a switch, the Low Frequency Asynchronous Switch Design (LF-ASD) that users can activate  by imagining movement. On-line implementations of the LF-ASD has shown promising results in respond to  actual index finger flexions and imagined finger flexions within able-bodied subjects. This work reports  the results of our first test with subjects with high-level spinal-cord injuries. In this study, two  subjects with high-level spinal-cord injuries were able to control the LF-ASD with imagined voluntary  movements with hit (true positive) rates from 45-48% and false positive rates below 1%. Keywords Brain,  human, computer, machine, interface, BCI, EEG, spinal-cord injury, switch.  ","Steven Mason, Ziba Bozorgzadeh, Gary Birch","BCI, EEG, brain, computer, human, interface, machine, spinal-cord injury, switch",109,113
10.1145/354324.354351,ASSETS,2000,Human factors issues in the neural signals direct brain-computer interfaces, ,"  Controlling a computer directly by brain signals has been made possible by the development of a neurotrophic  electrode that is implanted in the human motor cortex. The success of this technology can be enhanced  by researching and developing new human-computer interface paradigms for neural signal control. This  paper summarizes progress to date on the software aspects of the Neural Signals brain-computer interface  project and presents a vision and strategy for upcoming research. Keywords Neural signals, brain-computer  interfaces, accessible software. ","Melody Moore, Philip Kennedy","accessible software, brain-computer interfaces, neural signals",114,120
10.1145/354324.354352,ASSETS,2000,Neck range of motion and use of computer head controls, ," Computer head controls provide an alternative means of computer access  for people with disabilities. However, a person s ability to use head controls may be reduced if his  or her disability involves neck movement limitations. In this study, 15 subjects without disabilities  and 10 subjects with disabilities received neck range of motion evaluations and performed computer exercises  using head controls. Regression analysis was used to determine the relationship between neck range of  motion and performance on computer exercises. Reduced neck range of motion was found to be correlated  with reduced functional range for moving the cursor across the screen, and reduced accuracy and speed  in icon selection. Fitts Law-type models were fit to the data, indicating higher Fitts law slopes for  subjects with disabilities compared to subjects without disabilities. Results also indicate that vertical  cursor movements are faster than horizontal or diagonal movements. Keywords Disability, head controls,  head movement  ","Edmund LoPresti, David Brienza, Jennifer Angelo, Lars Gilbertson, Jonathan Sakai","disability, head controls, head movement",121,128
10.1145/354324.354354,ASSETS,2000,Investigating the applicability of user models for motion-impaired users, ," This paper considers  the differences between users with motion-impairments and able-bodied users when they interact with computers  and the implications for user models. Most interface design and usability assessment practices are based  on explicit or implicit models of user behaviour. This paper studies the applicability of an existing  interface design user model to motion-impaired users for the relatively straightforward task of button  activation. A discussion of the empirical results is provided and the paper concludes that there are  significant differences between the behaviour of motion-impaired users and the accepted modelling theory.   Keywords Universal Access, motion-impaired users, user models ","Simeon Keates, John Clarkson, Peter Robinson","motion-impaired users, universal access, user models",129,136
10.1145/354324.354355,ASSETS,2000,Evaluation of scanning user interfaces using real-time-data usage logs, ," This research concerns the use of Human Computer Interaction models to assist in the provision  of the Electronic Assistive Technologies (EAT) for people with severe disabilities. The novel feature  of this work is that the evaluation is conducted through the model-based analysis of automatically generated  usage logs. This analysis provides a source of feedback to clinicians, which has previously been unavailable  from assistive technology users. The overall aim of the research is to provide a set of tools to assist  with the prescription and configuration of assistive technologies. KEYWORDS Electronic Assistive Technology,  Integrated systems, Analysis, Modelling. ","Peter O'Neill, Chris Roast, Mark Hawley","Electronic Assistive Technology, analysis, integrated systems, modelling",137,141
10.1145/354324.354356,ASSETS,2000,A Java programming tool for students with visual disabilities, ," This paper reports on a tool for assisting students with visual disabilities in learning how  to program. The tool is meant to be used by computer science majors learning the proÂ­gramming language  Java. As part of the developmental process of building this tool, we have implemented a rapid prototype  to be used by people with disabilities in order to define appropriate requirements for the full version  of the tool. This requires that the prototype is completely usable via a keyboard and speech interface,  and it is easily adaptable for trying out different strategies. In this paper, we present the motivation  and philosophy of the full tool, called JavaSpeak. We also present the details of a prototype implementation  of JavaSpeak. Keywords: Java, programming tool, students with visual disabilities, learning to program   ","Ann Smith, Joan Francioni, Sam Matzek","Java, learning to program, programming tool, students with visual disabilities",142,148
10.1145/354324.354362,ASSETS,2000,"Programming by voice, VocalProgramming", ,A system that enables a person to program without typing is needed because of the high incidents of repetitive stress injuries among people who program. This paper presents a design for a system that generates environments that enables people to program by voice and a method of determining if the system is successful. It also shows how this generator can be used to support entering data and writing XML documents.,"Stephen Arnold, Leo Mark, John Goldthwaite","XML, computer programming, voice recognition",149,155
10.1145/354324.354363,ASSETS,2000,A semantic transcoding system to adapt Web services for users with disabilities, ,"Among the most critical issues of the internet today is how to make Web content accessible to all users, especially to users with disabilities. To meet the diverse needs and abilities of this population, the Web today calls for the development of new systems and methods to enable the same content to be adapted for display according to specific, often conflicting needs. One way to achieve this goal is through Web con- tent transcoding. This paper presents a system, called Aurora, that transcodes Web content based on semantic rather than syntactic constructs. The goal is to deliver Web-based services (such as auction, search engine, travel, etc.) to a diverse set of users according to their specific needs. Using a schema-driven framework, Aurora extracts and maps Web content into domain-specific XML data based on abstract user goals. In doing so, it separates the meaning Web content from its presentation. The system further enables an extensible set of interface adaptors to generate custom Web pages, on-the-fly, from this standardized XML data. Ultimately, it streamlines and customizes the Web interface to faciliate navigation. The mechanisms of this rule-based semantic transcoding system and its advantages and limitations as a strategy to make Web services more accessible are the subject of this paper.","Anita Huang, Neel Sundaresan","Web accessiblity, Web intermediaries, XML transcoding, adaptability, adaptivity, disabled users",156,163
10.1145/354324.354371,ASSETS,2000,Transcoding proxy for nonvisual web access, ,"These days, the web has been coming to play various types of roles, so each site has been designed in a complex way to integrate as many roles as possible. Web authors tend to cram various functions and many links into one page to improve usability for sighted users. This authoring trend makes nonvisual Web access harder. To solve this problem, we decided to develop a system to transcode already-existing Web pages to be accessible, which works as an intermediary (proxy) between a Web server and a user. Our transcoding proxy consists of 5 modules using 3 kinds of annotations. The user interface of the system is characterized by three transcoding modes: simplification, full-text and original page. In this paper, we will describe an overview of our transcoding proxy as well as the user interface of the system.","Hironobu Takagi, Chieko Asakawa","Web accessibility, annotations, blind, differential, portal, transcoding",164,171
10.1145/354324.354588,ASSETS,2000,Annotation-based transcoding for nonvisual web access, ," These days, Web authors try to describe as much information as possible in one page using various  types of visual effects. This information is visually fragmented into groupings. Blind users read the  Web contents in tag order, but visually fragmented groupings are not accessible using tag order reading.  In addition, the Web contents are designed to be visually appealing using a lot of images. This style  makes nonvisual Web access harder. Therefore we decided to develop an annotation-based transcoding system  to convert already-existing Web pages to be accessible, which works between a Web server and a user.  It consists of two components, one for structural annotations and one for commentary annotations. Structural  annotations are used to recognize visually fragmented groupings as well as to show the importance and  basic role of each group. Commentary annotations are used to give users a useful description of each  grouping. In this paper, we will describe our transcoding method for nonvisual Web access based on the  annotations. Keywords Nonvisual Web access, transcoding system, structural annotation, commentary annotation,  blind ","Chieko Asakawa, Hironobu Takagi","blind, commentary annotation, nonvisual web access, structural annotation, transcoding system",172,179
10.1145/354324.354373,ASSETS,2000,A domain specific language framework for non-visual browsing of complex HTML structures, ,"We present a general framework for navigating complex structures— specifically, tables, frames, and forms—found in web-pages. Our framework is based on an (automatically or manually created) program written in a domain specific language that captures the seman- tic structure of the table/frame/form as well as specifies the strategy to be used for navigating it. We describe our general framework and the domain specific language we have designed.","E. Pontelli, W. Xiong, G. Gupta, A. Karshmer","HTML, Web browsers, domain specific languages",180,187
10.1145/354324.354375,ASSETS,2000,Constructive exploration of spatial information by blind users, ," When blind  people wish to walk through an area not fully known to them, they have to prepare themselves even more  thoroughly than sighted pedestrians. We propose a new approach to support this preparation with the help  of an interactive computer method, called constructive exploraÂ­tion. Using this method, the user is guided  in physically constructing the spatial arrangement to be learned using building blocks. We describe two  implementations of the concept, one with a graspable interface with object tracking and the other employing  a force feedback device. We report on first tests of the implementations. Keywords Orientation aids,  blind users, augmented reality, force feedÂ­back devices ","Jochen Schneider, Thomas Strothotte","augmented reality, blind users, force feedback devices, orientation aids",188,192
10.1145/354324.354380,ASSETS,2000,Wearable interfaces for orientation and wayfinding, ,"People with severe visual impairment need a means of remaining oriented to their environment as they move through it. Three wearable orientation interfaces were developed and evaluated toward this purpose: a stereophonic sonic guide (sonic “carrot”), speech output, and shoulder-tapping system. Street crossing was used as a critical test setting in which to evaluate these interfaces. The shoulder-tapping system was found most universally usable. Considering the great variety of co-morbidities within this population, the authors concluded that a combined tapping/speech interface would provide usability and flexibility to the greatest number of people under the widest range of environmental conditions.","David Ross, Bruce Blasch","blindness, orientation aid, street crossing, wayfinding",193,200
10.1145/638249.638253,ASSETS,2002,From assistive technology to a web accessibility service, ,"This paper considers different ways to enhance access to the World Wide Web for persons with sensory, cognitive, or motor limitations. Paradoxically, while complex Web architectures may seem to have inhibited accessibility, they have broadened the range of points where we can try to improve it. This paper identifies these points and evaluates the advantages and disadvantages of each. In particular, it describes a project to develop a strategy to enhance access that can be distributed across multiple control points and implemented as an aggregation of Web services.","Peter Fairweather, Vicki Hanson, Sam Detweiler, Richard Schwerdtfeger","Web services, World Wide Web, accessibility, adaptive interfaces",4,8
10.1145/638249.638254,ASSETS,2002,Improving the accessibility of aurally rendered HTML tables, ,"Current techniques employed to aurally render HTML tables often result in output that is very difficult for sight-impaired users to understand. This paper proposes TTPML, an XML-compliant markup language, which facilitates the generation of prose descriptions of tabular information. The markup language enables content creators to specify contextual reinforcement of, and linear navigation through, tabular information. The markup language may be applied to pre-existing Web content and is reusable across multiple tables. TTPML may be interpreted by origin servers, proxy servers, or browsers. We believe that our approach benefits sight-impaired users by improving accessibility to tabular information.","Robert Filepp, James Challenger, Daniela Rosu","Web accessibility, XML, aural interfaces, tables",9,16
10.1145/638249.638255,ASSETS,2002,Web accessibility for low bandwidth input, ,"One of the first, most common, and most useful applications that today's computer users access is the World Wide Web (web). One population of users for whom the web is especially important is those with motor disabilities, because it may enable them to do things that they might not otherwise be able to do: shopping; getting an education; running a business. This is particularly important for low bandwidth users: users with such limited motor and speech that they can only produce one or two signals when communicating with a computer. We present requirements for low bandwidth web accessibility, and two tools that address these requirements. The first is a modified web browser, the second a proxy that modifies HTML. Both work without requiring web page authors to modify their pages.","Jennifer Mankoff, Anind Dey, Udit Batra, Melody Moore","WWW, low bandwidth input, motor impairment, web proxy",17,24
10.1145/638249.638256,ASSETS,2002,"Navigation of HTML tables, frames, and XML fragments", ,"In this paper, we provide a progress report on the development of technology to support the non-visual navigation of complex HTML and XML structures.","E. Pontelli, D. Gillan, W. Xiong, E. Saad, G. Gupta, A. Karshmer","Web Accessibility, visually impaired users",25,32
10.1145/638249.638258,ASSETS,2002,Sketching images eyes-free,a grid-based dynamic drawing tool for the blind,"In this paper we describe one method of transforming a mouse-based graphical user interface into a navigable, grid-based auditory interface. We also report the results of an experiment that tested the effectiveness of a drawing tool for the blind called IC2D that uses this interaction style. The experiment included eight visually impaired participants and eight blindfolded sighted participants. The results show that auditory interpretation of graphics is an effective interface technique for visually impaired users. Further, the experiment demonstrates that visually impaired users can develop meaningful drawings when given adequate technological support.","Hesham Kamel, James Landay","IC2D, auditory user interfaces, drawing, graphical semantic enhancement, graphics, grid, visually impaired",33,40
10.1145/638249.638259,ASSETS,2002,Design and implementation of virtual environments training of the visually impaire, ,"This paper presents the virtual reality applications developed for the feasibility study tests of the EU funded IST project ENORASI. ENORASI aims at developing a highly interactive and extensible haptic VR training system that allows visually impaired people, especially those blind from birth, to study and interact with various virtual objects. A number of custom applications have been developed based on the interface provided by the CyberGrasp haptic device. Eight test categories were identified and corresponding tests were developed for each category. Twenty-six blind persons conducted the tests and the evaluation results have shown the degree of acceptance of the technology and the feasibility of the proposed approach.","D. Tzovaras, G. Nikolakis, G. Fergadis, S. Malasiotis, M. Stavrakis","haptics, training, virtual environments, visually impaired",41,48
10.1145/638249.638260,ASSETS,2002,Multimodal feedback,establishing a performance baseline for improved access by individuals with visual impairments,"Multimodal interfaces have the potential to enhance a user's overall performance, especially when one perceptual channel, such as vision, is compromised. This research investigated how unimodal, bimodal, and trimodal feedback affected the performance of fully sighted users. Limited research exists that investigates how fully sighted users react to multimodal feedback forms, and to-date even less research is available that has investigated how users with visual impairments respond to multiple forms of feedback. A complex direct manipulation task, consisting of a series search and selection drag-and-drop subtasks, was evaluated in this study. The multiple forms of feedback investigated were auditory, haptic and visual. Each form of feedback was tested alone and in combination. User performance was assessed through measures of workload time. Workload was measured objectively and subjectively, through the physiological measure of pupil diameter and a portion of the NASA Task Load Index (TLX) workload survey, respectively. Time was captured by a measure of how long it took to complete a particular element of the task. The results demonstrate that multimodal feedback improves the performance of fully sighted users and offers great potential to users with visual impairments. As a result, this study serves as a baseline to drive the research and development of effective feedback combinations to enhance performance for individuals with visual impairments.","Holly Vitense, Julie Jacko, V. Emery","auditory, feedback, haptic, human-computer interaction, multimodal, visual, visual impairment",49,56
10.1145/638249.638261,ASSETS,2002,Multimodal virtual reality versus printed medium in visualization for blind people, ,"In this paper, we describe a study comparing the strengths of a multimodal Virtual Reality (VR) interface against traditional tactile diagrams in conveying information to visually impaired and blind people. The multimodal VR interface consists of a force feedback device (SensAble PHANTOM), synthesized speech and non-speech audio. Potential advantages of the VR technology are well known however its real usability in comparison with the conventional paper-based medium is seldom investigated. We have addressed this issue in our evaluation. The experimental results show benefits from using the multimodal approach in terms of more accurate information about the graphs obtained by users.","Wai Yu, Stephen Brewster","assistive technology, haptics, human computer interaction, multimodal interface, virtual reality",57,64
10.1145/638249.638263,ASSETS,2002,Auditory and tactile interfaces for representing the visual effects on the web, ,"In this paper, we describe auditory and tactile interfaces to represent visual effects nonvisually for blind users, allowing intuitive recognition of visual content that appears on the Web. This research examines how visual effects could be recognized by blind subjects using the senses of hearing and touch, aiming at integrating the results into a practical system in the future. As an initial step, two experiments were performed, one for sonification and tactilization of a page overview based on color-based fragmented groupings without speech, and one for sonification and tactilization of emphasized text based on analyzing rich text information with speech. The subjects could recognize visual representations presented by auditory and tactile interfaces throughout the experiment, and were conscious of the importance of the visual structures. We believe this shows our approach may be practical and available in the future.We will summarize our results and discuss what kind of information is suitable for each sense, as well as the next planned experiment and other future work.","Chieko Asakawa, Hironobu Takagi, Shuichi Ino, Tohru Ifukube","auditory interface, blind, nonvisual, sonification, tactile interface, tactilization",65,72
10.1145/638249.638264,ASSETS,2002,"Planning, reasoning, and agents for non-visual navigation of tables and frames", ,"In this paper we demonstrate how the DSL for Table navigation [16] can be reinterpreted in the context of an <i>action theory</i> [8]. We also show how this generalization provides the ability to carry out more complex tasks such as (<i>i</i>) allowing the user to describe the objective of his/her navigation as a goal and let automatic mechanisms (i.e., a <i>planner</i>) develop (part of) the navigation process; and (<i>ii</i>) allowing the semantic description to predefine not only complete navigation strategies (as in [16]) but also partial skeletons, making the remaining part of the navigation dependent on run-time factors, e.g., user's goals, specific aspects of the table's content, User's run-time decisions.","Enrico Pontelli, Tran Son","agents, domain specific language, semantics navigation",73,80
10.1145/638249.638265,ASSETS,2002,Site-wide annotation,reconstructing existing pages to be accessible,"The Web has become a new information resource for the blind. However, Web accessibility is becoming worse, since page authors tend to care only for the visual appearance. We have developed an Accessibility Transcoding System to solve this problem. This system has the ability to transcode complete pages on annotated sites into totally accessible pages without changing the original pages. However, site-wide annotation authoring is an extremely tedious and time-consuming task. This prevented us from applying our transcoding system to a wide variety of sites. In order to overcome this difficulty, we developed a new algorithm, ""Dynamic Annotation Matching"". By utilizing this algorithm, our transcoding system can automatically determine appropriate annotations based on each page's layout. We also developed a site-wide annotation-authoring tool, ""Site Pattern Analyzer."" We evaluated the feasibility of creating site-wide annotations by using the algorithm and the tool, and report on our success here.","Hironobu Takagi, Chieko Asakawa, Kentarou Fukuda, Junji Maeda","Web Accessibility, annotation, layout-based annotation matching, transcoding",81,88
10.1145/638249.638266,ASSETS,2002,Using handhelds to help people with motor impairments, ,"People with Muscular Dystrophy (MD) and certain other muscular and nervous system disorders lose their gross motor control while retaining fine motor control. The result is that they lose the ability to move their wrists and arms, and therefore their ability to operate a mouse and keyboard. However, they can often still use their fingers to control a pencil or stylus, and thus can use a handheld computer such as a Palm. We have developed software that allows the handheld to substitute for the mouse and keyboard of a PC, and tested it with four people (ages 10, 12, 27 and 53) with MD. The 12-year old had lost the ability to use a mouse and keyboard, but with our software, he was able to use the Palm to access email, the web and computer games. The 27-year-old reported that he found the Palm so much better that he was using it full-time instead of a keyboard and mouse. The other two subjects said that our software was much less tiring than using the conventional input devices, and enabled them to use computers for longer periods. We report the results of these case studies, and the adaptations made to our software for people with disabilities.","Brad Myers, Jacob Wobbrock, Sunny Yang, Brian Yeung, Jeffrey Nichols, Robert Miller","Muscular Dystrophy, Palm pilot, Pebbles, Personal Digital Assistants (PDAs), assistive technologies, disabilities, hand-held computers, handicapped",89,96
10.1145/638249.638268,ASSETS,2002,Ongoing investigation of the ways in which some of the problems encountered by some dyslexics can be alleviated using computer techniques, ,"This paper describes the ongoing development of a highly configurable word processing environment developed using a pragmatic, obstacle-by-obstacle approach to alleviating some of the visual problems encountered by dyslexic computer users. The paper describes the current version of the software and the development methodology as well as the results of a pilot study which indicated that visual environment individually configured using the SeeWord software improved reading accuracy as well as subjectively rated reading comfort.","Anna Dickinson, Peter Gregor, Alan Newell","configuration, dyslexia, user-centred design, word processing",97,103
10.1145/638249.638269,ASSETS,2002,Virtual environments for social skills training,the importance of scaffolding in practice,"Virtual Environments (VE's) offer the potential for users to explore social situations and 'try out' different behaviour responses for a variety of simulates social interactions. One of the challenges for the VE developer is how to construct the VE to allow freedom of exploration and flexibility in interactive behaviour, without the risk of users deliberately or inadvertently missing important learning goals. Scaffolding embedded within the VE software can aid the user's learning in different contexts, such as individual, tutored or group learning situations. This paper describes two single-user VE scenarios that have been developed within the AS interactive project and presents observation results from initial trials conducted at a user school.","Steven Kerr, Helen Neale, Sue Cobb","Virtual Environments, autism, scaffolding of learning, social skills training",104,110
10.1145/638249.638270,ASSETS,2002,Modeling educational software for people with disabilities,theory and practice,"Interactive multimedia learning systems are not suitable for people with disabilities. They tend to propose interfaces which are not accessible for learners with vision or auditory disabilities. Modeling techniques are necessary to map real world experiences to virtual worlds by using 3D auditory representations of objects for blind people and visual representations for deaf people. In this paper we describe common aspects and differences in the process of modeling the real world for applications involving tests and evaluations of cognitive tasks with people with reduced visual or auditory cues. To validate our concepts, we examine two existing systems using them as examples: AudioDoom and Whisper. AudioDoom allows blind children to explore and interact with virtual worlds created with spatial sound. Whisper implements a workplace to help people with impaired auditory abilities to recognize speech errors. The new common model considers not only the representation of the real world as proposed by the system but also the modeling of the learner's knowledge about the virtual world. This can be used by the tutoring system to enable the learner to receive relevant feedback. Finally, we analyze the most important characteristics in developing systems by comparing and evaluating them and proposing some recommendations and guidelines.","Nelson Baloian, Wolfram Luther, Jaime S&#225;nchez","modeling methodologies, sensory disabilities, tutoring systems, user adapted interfaces",111,118
10.1145/638249.638272,ASSETS,2002,Zooming interfaces!,enhancing the performance of eye controlled pointing devices,"This paper quantifies the benefits and usability problems associated with eye-based pointing direct interaction on a standard graphical user interface. It shows where and how, with the addition of a second supporting modality, the typically poor performance and subjective assessment of eye-based pointing devices can be improved to match the performance of other assistive technology devices. It shows that target size is the overriding factor affecting device performance and that when target sizes are artificially increased by 'zooming in' on the interface under the control of a supporting modality then eye-based pointing becomes a viable and usable interaction methodology for people with high-level motor disabilities.","Richard Bates, Howell Istance","assistive technology, eye-tracking, graphical user interfaces, pointing devices, zoom screen",119,126
10.1145/638249.638273,ASSETS,2002,HaWCoS,"the ""hands-free"" wheelchair control system","A system allowing to control an electrically powered wheelchair without using the hands is introduced. HaWCoS -- the ""<i>Ha</i>nds-free"" <i>W</i>heelchair <i>Co</i>ntrol System -- relies upon muscle contractions as input signals. The working principle is as follows. The constant stream of EMG signals associated with any arbitrary muscle of the wheelchair driver is monitored and reduced to a stream of contraction events. The reduced stream affects an internal program state which is translated into appropriate commands understood by the wheelchair electronics. The feasibility of the proposed approach is illustrated by a prototypical implementation for a state-of-the-art wheelchair. Operating a HaWCoS-wheelchair requires extremely little effort, which makes the system suitable even for people suffering from <i>very</i> severe physical disabilities.","Torsten Felzer, Bernd Freisleben","EMG signal, electrical wheelchair, muscle control",127,134
10.1145/638249.638274,ASSETS,2002,Cursor measures for motion-impaired computer users, ,"""Point and click"" interactions remain one of the key features of graphical user interfaces (GUIs). People with motion-impairments, however, can often have difficulty with accurate control of standard pointing devices. This paper discusses work that aims to reveal the nature of these difficulties through analyses that consider the cursor's path of movement. A range of potential cursor measures was applied, and a number of them were found to be significant in capturing the differences between able-bodied users and motion-impaired users, as well as the differences between a haptic force feedback condition and a control condition. cursor measures found in the literature, however, do not make up a comprehensive list, but provide a starting point for analysing cursor movements more completely. Six new cursor characteristics for motion-impaired users are introduced to capture aspects of cursor movement different from those already proposed.","Simeon Keates, Faustina Hwang, Patrick Langdon, P. Clarkson, Peter Robinson","cursor studies, force feedback, motion-impaired users",135,142
10.1145/638249.638275,ASSETS,2002,An invisible keyguard, ,"Overlap errors, in which two keys are pressed down at once, are a common typing error for people with motor disabilities. Keyguards are a commonly suggested means to may reduce overlap errors. However, they are also unpopular with many users. We present an alternative to the keyguard, a software filter which targets overlap errors. Basic, keystroke timing-based, and language-based techniques for identifying and correcting overlap errors are described. Their performance is compared using a corpus of typing data recorded by keyboard users with motor disabilities. The best filter performance was obtained by keystroke timing characteristics to identify and filter out extra characters. Accuracy of error identification was dependent on the typing style of the user. The filter accurately corrected 80% of the overlap errors presented. Combining the identification and correction techniques gave a 50--75% reduction in errors for the three study participants with the highest error rates.",Shari Trewin,"OverlapKeys, accessibility, keyboard, keyguard, motor disabilities, typing errors",143,149
10.1145/638249.638277,ASSETS,2002,Designing for dynamic diversity,interfaces for older people,"In this paper, we describe why designers need to look beyond the twin aims of designing for the 'typical' user and designing ""prostheses"". Making accessible interfaces for older people is a unique but many-faceted challenge. Effective applications and interface design needs to address the dynamic diversity of the human species. We introduce a new design paradigm, Design for Dynamic Diversity, and suggest a methodology to assist its achievement, User Sensitive Inclusive Design.To support our argument for a new form of design we report experimentation, which indicates that older people have significantly different and dynamically changing needs. We also put forward initial solutions for Designing for Dynamic Diversity, where memory, vision and confidence provide the parameters for discussion, and illustrate the importance of User Sensitive Inclusive Design in establishing a framework for the operation of Design for Dynamic Diversity.","Peter Gregor, Alan Newell, Mary Zajicek","Design for Dynamic Diversity, HCI, User Sensitive Inclusive Design, aging, design for all, older people, universal accessibility, usability engineering",151,156
10.1145/638249.638278,ASSETS,2002,A novel multi-stage approach to the detection of visuo-spatial neglect based on the analysis of figure-copying tasks, ,"This paper examines a computer-based technique for the detection of visuo-spatial neglect from the responses of a simple geometric shape copying task. Defining pass/fail criteria based on the presence of drawn components, responses can be accurately and objectively assessed. More importantly, we show that by analysing novel dynamic performance features detailing timing and constructional aspects of each response, significant performance deficits can be noted in drawings made by clinically diagnosed neglect subjects that would have been classified as 'normal' using conventional static analysis, thus improving the sensitivity of the assessment.","R. Guest, M. Fairhurst","automated diagnosis, drawing analysis, visuo-spatial neglect",157,161
10.1145/638249.638279,ASSETS,2002,Assistive social interaction for non-speaking people living in the community, ,"The move from institution to community care has resulted in more disabled and elderly people receiving care at home. For some, their disability or frailty prevents them from being involved in social activities outside the home, resulting in unacceptable social isolation. This problem is compounded if the person has a speech or language impairment. In general, social interaction is important for people, and they often use stories, pictures and other media to present important events to others, In this paper, we will describe a communication service designed to provide non-speaking people with a means to interact socially when living independently, based on the sharing of stories using pictures and other media.","Nick Hine, John Arnott","Internet, assitive communication, community care, social isolation, videoconferencing",162,169
10.1145/638249.638280,ASSETS,2002,Older adults' evaluations of speech output, ,"Speech output is frequently used to provide access to interactive systems for visually impaired users, many of whom are older adults. This paper considers the use of speech output within the context of an Intelligent Home System designed to allow older adults to remain living independently for longer. The importance of user evaluations of the system 'voice' in this context is discussed and an experiment is reported that investigated the effect of voice gender and type (natural or synthetic) on older users' evaluations. A within-subjects factorial design was used with sixteen participants over the age of 65. The results show that male voices were preferred to female voices overall and natural voices were preferred to synthetic voices. The implications of these results for the choice of system voice characteristics for speech output are discussed.","Lorna Lines, Kate Hone","assistive technology, human-computer interaction, intelligent home systems., older adults, speech output, visual impairment",170,177
10.1145/638249.638282,ASSETS,2002,Speech-based cursor control, ,"Speech recognition can be a powerful tool for individuals with physical disabilities that hinder their ability to use traditional input devices. State-of-the-art speech recognition systems typically provide mechanisms for both data entry and cursor control, but the researchers continue to investigate methods of improving these interactions. Numerous researchers are investigating methods to improve the underlying technologies that make speech recognition possible and others focus on understanding the difficulties users experience using dictation-oriented applications, but few researchers have investigated the issues involved in speech-based cursor control. In this article, we describe a study that investigates the efficacy of two variations of a standard speech-based cursor control mechanism. One employs the standard mouse cursor while the second provides a predictive cursor designed to help users compensate for the delays often associated with speech recognition. As expected, larger targets and shorter distances resulted in shorter target selection times while larger targets also resulted in fewer errors. Although there were no differences between the standard and predictive cursors, a relationship between the delays associated with spoken input, the speed at which the cursor moves, and the minimum size for targets that can be reliably selected emerged that can guide the application of similar speech-based cursor control mechanisms as well as future research.","Azfar Karimullah, Andrew Sears","cursor, mouse cursor, navigation, predictive, speech recognition",178,185
10.1145/638249.638283,ASSETS,2002,A predictive Blissymbolic to English translation system, ,This paper reports on the use of predictive techniques to translate Blissymbol sentences into grammatically correct English. Evaluations of this approach show that it is possible to translate short sentences by analysing the likelihood of word tri-gram occurrences in English source texts. The translation system is designed to be a component of a Blissymbol word processor which allows users to convert Blissymbol sentences into grammatically correct English.,"Annalu Waller, Kris Jack","AAC, Blissymbolics, natural language translation",186,191
10.1145/638249.638284,ASSETS,2002,Speech recognition in university classrooms,liberated learning project,"The LIBERATED LEARNING PROJECT (LLP) is an applied research project studying two core questions:1) Can speech recognition (SR) technology successfully digitize lectures to display spoken words as text in university classrooms?2) Can speech recognition technology be used successfully as an alternative to traditional classroom notetaking for persons with disabilities?This paper addresses these intriguing questions and explores the underlying complex relationship between speech recognition technology, university educational environments, and disability issues.","Keith Bain, Sara Basson, Mike Wald","accessibility, higher education, speech recognition",192,196
10.1145/638249.638285,ASSETS,2002,Voice over Workplace (VoWP),voice navigation in a complex business GUI,"Voice interfaces can be used to meet some accessibility requirements for physically disabled users, but only if they address inherent usability problems, namely, the trade-off between user efficiency and ambiguity handling. This paper explores usability issues related to voice interfaces for complex GUIs. We present two user studies on a series of interface designs to support voice navigation within a complex business GUI, and discuss the findings as they relate to efficiency and ambiguity handling. We conclude by discussing future directions for this work, including the addition of data input capabilities, which will be necessary to provide a truly accessible solution.","Frankie James, Jeff Roelands","GUI, accessibility, physical disabilities, user studies, voice interface",197,204
10.1145/638249.638287,ASSETS,2002,"Tessa, a system to aid communication with deaf people", ,"TESSA is an experimental system that aims to aid transactions between a deaf person and a clerk in a Post Office by translating the clerk's speech to sign language. A speech recogniser recognises speech from the clerk and the system then synthesizes the appropriate sequence of signs in British Sign language (BSL) using a specially-developed avatar. By using a phrase lookup approach to language translation, which is appropriate for the highly constrained discourse in a Post Office, we were able to build a working system that we could evaluate. We summarise the results of this evaluation (undertaken by deaf users and Post office clerks), and discuss how the findings from the evaluation are being used in the development of an improved system.","Stephen Cox, Michael Lincoln, Judy Tryggvason, Melanie Nakisa, Mark Wells, Marcus Tutt, Sanja Abbott","Aids for the Deaf, avatars, interactive systems, speech recognition, translation systems",205,212
10.1145/638249.638288,ASSETS,2002,"Capturing phrases for ICU-Talk, a communication aid for intubated intensive care patients.", ,"The need for intubated patients, within the intensive care setting, to communicate more effectively led to the development of ICU-Talk, an augmentative and alternative communication aid. The communication aid contains a database containing both core and patient-specific vocabulary. Many users of communication aids can provide direct input into the vocabulary, but intensive care patients are not in this position. This paper discusses the methods chosen to gather the vocabulary for an intensive care setting.","S. Ashraf, A. Judson, I. Ricketts, A. Waller, N. Alm, B. Gordon, F. MacAulay, J. Brodie, M. Etchels, A. Warden, A. Shearer","AAC, ICU, communication, vocabulary",213,217
10.1145/638249.638289,ASSETS,2002,A new generation of communication aids under the ULYSSES component-based framework, ,"In this paper, we introduce a new generation of computer-based communication aids, designed and developed using state of the art software engineering models and architectures. The communicators we present are based on a component-based framework called ULYSSES that aims to simplify the integration of multi-vendor components into low cost products and maximizes modularity and reusability. Following the ULYSSES approach, one can build up powerful and reliable applications, adaptable to various user needs and requirements. For <i>developers</i> of AAC components, ULYSSES provides an engineering-for-reuse environment with guidelines and tools to build software modules, which can operate effectively and interact with each other transparently, without even being aware of each other's existence. Furthermore, ULYSSES grants a process of engineering-with-reuse for AAC system <i>integrators</i> for the selection and assembly of components on demand to build user-specific robust communicators out of pre-fabricated software parts. Thus, adding or removing characteristics and features as needed, is becoming an easy task for system AAC systems integrators. Three complete Interpersonal Communication Aids are presented as cases of ULYSSES application in this specific domain.","Georgios Kouroupetroglou, Alexandros Pino","Augmentative and Alternative Communication (AAC), communication aids, communicators, component based development, framework architecture",218,225
10.1145/638249.638290,ASSETS,2002,"ICU-Talk, a communication aid for intubated intensive care patients", ,"A Multi-disciplinary project staffed by personnel from nursing, computer science and speech and language therapy developed a computer based communication aid called ICU-Talk. This device has been designed specifically for intubated patients in hospital intensive care units. The ICU-Talk device was trialled with real patients. This paper reports the challenges faced when developing a device for this patient group and environment. A description of the methods used to produce ICU-Talk and results from the trials will be presented.","F. MacAulay, A. Judson, M. Etchels, S. Ashraf, I. Ricketts, A. Waller, J. Brodie, N. Alm, A. Warden, A. Shearer, B. Gordon","AAC, HCI, ICU, communication, usability",226,230
10.1145/1028630.1028633,ASSETS,2004,Audio enriched links,web page previews for blind users,"Audio Enriched Links provide previews of linked web pages to users with visual impairments. Before a user follows a hyperlink, the Audio Enriched Links software presents a spoken summary of the next page including its title, its relation to the current page, statistics about its content, and some highlights from its content. We believe that such a summary may be a useful surrogate for a full web page, and help users with visual impairments decide whether or not to spend time visiting a linked page. In this paper, we present some motivation for the Audio Enriched Links project. We describe the design and implementation of the current software prototype, and discuss the results of an initial evaluation involving four participants. We conclude with some implications of this work and directions for future research.",Peter Parente,"accessibility, speech preview, visual impairment, web page preview",2,8
10.1145/1028630.1028634,ASSETS,2004,The audio abacus,representing numerical values with nonspeech sound for the visually impaired,"Point estimation is a relatively unexplored facet of sonification. We present a new computer application, the Audio Abacus, designed to transform numbers into tones following the analogy of an abacus. As this is an entirely novel approach to sonifying exact data values, we have begun a systematic line of investigation into the application settings that work most effectively. Results are presented for an initial study. Users were able to perform relatively well with very little practice or training, boding well for this type of display. Further investigations are planned. This could prove to be very useful for visually impaired individuals given the common nature of numerical data in everyday settings.","Bruce Walker, Jeffrey Lindsay, Justin Godfrey","auditory display, data sonification, value estimation, visually impaired users",9,15
10.1145/1028630.1028635,ASSETS,2004,Rendering tables in audio,the interaction of structure and reading styles,"Tables remain a persistent problem for visually impaired people using screen readers. Tables are complex structures that are widely used for different purposes such as spatial layout or data summarisation. The multi-dimensional nature of tables challenges the linear interaction styles typically supported by screen readers. To read a table, a user needs to maintain coherency of, and interact with more than one dimension. In this paper, we first characterise why tables are useful in print, but difficult to read in the audio. We present a survey of the relationship between table structure, intention and the reading styles employed to use the content of tables. We then present two different approaches for interacting with tables non-visually. These approaches are designed to support the characteristics of tables that make them such a popular and useful means of conveying information. The first approach provides a small table browser called EVITA (Enabling Visually Impaired Table Access), whose aim is to enable non-visual table browsing and reading in an analogous manner to the print medium. The second approach provides a table lineariser to transform tables into a form such that they can be easily read by screen readers.","Yeliz Yesilada, Robert Stevens, Carole Goble, Shazad Hussein","audio interaction, browsing, table, visual impairment",16,23
10.1145/1028630.1028636,ASSETS,2004,Memory enhancement through audio, ,"A number of studies have proposed interactive applications for blind people. One line of research is the use of interactive interfaces based on sound to enhance cognition in blind children. Even though these studies have emphasized learning and cognition, there is still a shortage of applications to assist the development and use of memory in these children. This study presents the design, development, and usability of AudioMemory, a virtual environment based on audio to develop and use short-term memory. AudioMemory was developed by and for blind children. They participated in the design and usability tested the software during and after development. We also introduce AudioMath, an instance of AudioMemory to assist mathematics learning in children with visual disabilities. Our results evidenced that sound can be a powerful interface to develop and enhance memory and mathematics learning in blind children.","Jaime S&#225;nchez, H&#233;ctor Flores","audio-based navigation, blind children, cognitive memory, spatialized sound, usability, virtual acoustic environments",24,31
10.1145/1028630.1028638,ASSETS,2004,Accessibility of Internet websites through time, ,"Using Internet Archive's Wayback Machine, a random sample of websites from 1997-2002 were retrospectively analyzed for effects that technology has on accessibility for persons with disabilities and compared to government websites. Analysis of Variance (ANOVA) and Tukey's HSD were used to determine differences among years. Random websites become progressively inaccessible through the years (p&#60;0.0001) [as shown by increasing Web Accessibility Barrier (WAB) scores], while complexity of the websites increased through the years (p&#60;0.0001). Pearson's correlation (r) was performed to correlate accessibility and complexity: r=0.463 (p&#60;0.01). Government websites remain accessible while increasing in complexity: r=0.14 (p&#60;0.041). It is concluded that increasing complexity, oftentimes caused by adding new technology to a Web page, inadvertently contributes to increasing barriers to accessibility for persons with disabilities.","Stephanie Hackett, Bambang Parmanto, Xiaoming Zeng","internet archive, web accessibility, world wide web",32,39
10.1145/1028630.1028639,ASSETS,2004,Evaluation of a non-visual molecule browser, ,"This paper describes the evaluation of software, software designed to allow visually impaired users to explore the structures of chemical molecules using a speech based presentation. Molecular structures are typically presented as two dimensional schematics, and are an important example of a widely used form of diagram -- the graph. software is designed for exploring this specific class of graph. Among its features is the ability to recognise and make explicit features of the graph that would otherwise need to be inferred. The evaluation compared software with a simpler version without this facility, and found that participants were able to explore molecular structures more easily. We discuss the software, evaluation and results, particularly comparing them with theoretical considerations about how sighted readers use diagrams. Finally, we extract the important issues for non-visual graph presentation: making implicit features explicit; enabling hierarchical and connection-based browsing; allowing annotation; and helping users keep their orientation.","Andy Brown, Steve Pettifer, Robert Stevens","browsing, connection browsing, graphs, hierarchical browsing, molecules, non-visual, synthetic speech",40,47
10.1145/1028630.1028640,ASSETS,2004,A galvanic skin response interface for people with severe motor disabilities, ,"Biometric input devices can provide assistive technology access to people who have little or no motor control. We explore a biometric control interface based on the Galvanic Skin Response, to determine its effectiveness as a non-muscular channel of input. This paper presents data from several online studies of a locked-in subject using a Galvanic Skin Response system for communication and control. We present issues with GSR control, and approaches that may improve accuracy and information transfer rate.","Melody Moore, Umang Dua","assistive technology, biometric control, galvanic skin response, locked-in syndrome",48,54
10.1145/1028630.1028642,ASSETS,2004,UMA,a system for universal mathematics accessibility,"We describe the UMA system, a system developed under a multi-institution collaboration for making mathematics universally accessible. The UMA system includes translators that freely inter-convert mathematical documents transcribed in formats used by unsighted individual (Nemeth, Marburg) to those used by sighted individuals (LaTeX, Math-ML, OpenMath) and vice versa. The UMA system also includes notation-independent tools for aural navigation of mathematics. In this paper, we give an overview of the UMA system and the techniques used for realizing it.","A. Karshmer, G. Gupta, E. Pontelli, K. Miesenberger, N. Ammalai, D. Gopal, M. Batusic, B. St&#246;ger, B. Palmer, H-F. Guo","math accessibility, visually impaired",55,62
10.1145/1028630.1028643,ASSETS,2004,Middleware to expand context and preview in hypertext, ,"Movement, or mobility, is key to the accessibility, design, and usability of many hypermedia resources (websites); and key to good mobility is context and preview by probing. This is especially the case for visually impaired users when a hypertext anchor is inaccurately described or is described out of context. This means confusion and disorientation. Mobility is similarly reduced when the link target of the anchor has no relationship to the expected information present on the hypertext node (web-page). We suggest that confident movement with purpose, ease, and accuracy can only be achieved when complete contextual information and an accurate description of the proposed destination (preview) are available. Our past work (1) deriving mobility heuristics from mobility models, (2) transforming web-pages based on these heuristics, and(3) building tools to analyse and access these transformed pages; has shown us that a tool to expand context and preview would be useful. In this paper we describe the development of such a middleware tool to automatically and dynamically annotate web-pages with additional context information present within the page, and preview information present within hypertext link destinations found on the page.","Simon Harper, Carole Goble, Robert Stevens, Yeliz Yesilada","mobility, practical implementation, travel, visually impaired, web",63,70
10.1145/1028630.1028644,ASSETS,2004,Automating accessibility: the dynamic keyboard, ,"People with motor disabilities may need to adjust the configuration of their input devices, but often find this an obscure and difficult process. The Dynamic Keyboard exemplifies a potential solution. It continuously adjusts fundamental keyboard accessibility features to suit the requirements of the current user, based on a keyboard use model. Early field results indicate that users have not chosen to take control of these accessibility features from the Dynamic Keyboard, and that a variety of settings are being used. A more detailed ongoing study suggests that automatic adjustment of the key repeat delay feature is acceptable to users, while the debounce feature may not be appropriate for dynamic adjustment.",Shari Trewin,"acessibility, configuration, keyboards, self-adapting systems",71,78
10.1145/1028630.1028645,ASSETS,2004,MEMOS,an interactive assistive system for prospective memory deficit compensation-architecture and functionality,"The Mobile Extensible Memory Aid System (MEMOS) is an electronic memory aid system which was developed to support patients with deficits in the prospective memory after a brain injury. A special palmtop computer, the Personal Memory Assistant (PMA), reminds the patient of important tasks and supervises the patient's actions. The PMA communicates with a stationary care system via a bidirectional cellular radio connection (GPRS). MEMOS features structured interactive reminding impulses, a flexible task scheduling, integration of a heterogeneous group of caregivers and integration in the patient's everyday life. The bidirectional communication allows for reporting of critical situations back to a responsible care-giver, so MEMOS can be used even in a critical context. This paper describes the requirements for a memory aid system, the design and functionality of MEMOS as well as its application in the practical care of patients.",Hendrik Schulze,"assistive technology, cognitive prosthesis, electronic memory aid, health care application, mobile computing, prospective memory disturbances, usability",79,85
10.1145/1028630.1028647,ASSETS,2004,Eyedraw,a system for drawing pictures with eye movements,"This paper describes the design and development of EyeDraw, a software program that will enable children with severe mobility impairments to use an eye tracker to draw pictures with their eyes so that they can have the same creative developmental experiences as nondisabled children. EyeDraw incorporates computer-control and software application advances that address the special needs of people with motor impairments, with emphasis on the needs of children. The contributions of the project include (a) a new technique for using the eyes to control the computer when accomplishing a spatial task, (b) the crafting of task-relevant functionality to support this new technique in its application to drawing pictures, and (c) a user-tested implementation of the idea within a working computer program. User testing with nondisabled users suggests that we have designed and built an eye-cursor and eye drawing control system that can be used by almost anyone with normal control of their eyes. The core technique will be generally useful for a range of computer control tasks such as selecting a group of icons on the desktop by drawing a box around them.","Anthony Hornof, Anna Cavender, Rob Hoselton","art, children, drawing, eye tracking, input devices, interaction techniques, universal access",86,93
10.1145/1028630.1028648,ASSETS,2004,Speech-based cursor control,a study of grid-based solutions,"Speech recognition can be a powerful tool for use in human-computer interaction. Many researchers are investigating the use of speech recognition systems for dictation-based activities, resulting in dramatic improvements in recent years. However, this same experimentation has confirmed that recognition errors and the delays inherent with speech recognition result in unacceptably long task completion times and error rates for cursor control tasks. This study explores the potential of a speech-controlled grid-based cursor control mechanism. An experiment evaluated two alternative grid-based solutions, both using 3-3 grids. One provided a single cursor in the middle of the grid. The second allows users to select a target using any of nine cursors. The results confirm that the nine-cursor solution allowed users to select targets of varying size, distance and direction significantly faster than the one-cursor solution. Overall results are encouraging when compared to earlier evaluations of other speech-based cursor control solutions.","Liwei Dai, Rich Goldman, Andrew Sears, Jeremy Lozier","cursor control, grid, mouse, navigation, speech recognition",94,101
10.1145/1028630.1028649,ASSETS,2004,Mouse movements of motion-impaired users: a submovement analysis, ,"Understanding human movement is key to improving input devices and interaction techniques. This paper presents a study of mouse movements of motion-impaired users, with an aim to gaining a better understanding of impaired movement. The cursor trajectories of six motion-impaired users and three able-bodied users are studied according to their submovement structure. Several aspects of the movement are studied, including the frequency and duration of pauses between submovements, verification times, the number of submovements, the peak speed of submovements and the accuracy of submovements in two-dimensions. Results include findings that some motion-impaired users pause more often and for longer than able-bodied users, require up to five times more submovements to complete the same task, and exhibit a correlation between error and peak submovement speed that does not exist for able-bodied users.","Faustina Hwang, Simeon Keates, Patrick Langdon, John Clarkson","cursor trajectory, motion-impaired, mouse movement, pointing device, submovement structure",102,109
10.1145/1028630.1028650,ASSETS,2004,Text entry from power wheelchairs: edgewrite for joysticks and touchpads, ,"Power wheelchair joysticks have been used to control a mouse cursor on desktop computers, but they offer no integrated text entry solution, confining users to point-and-click or point-and-dwell with on-screen keyboards. But on-screen keyboards reduce useful screen real-estate, exacerbate the need for frequent window management, and impose a second focus of attention. By contrast, we present two integrated gestural text entry methods designed for use from power wheelchairs: one for joysticks and the other for touchpads. Both techniques are adaptations of EdgeWrite, originally a stylus-based unistroke method designed for people with tremor. In a preliminary study of 7 power wheelchair users, we found that touchpad EdgeWrite was faster than joystick WiVik, and joystick EdgeWrite was only slightly slower after minimal practice. These findings reflect ""walk up and use""-ability and warrant further investigation into extended use.","Jacob Wobbrock, Brad Myers, Htet Aung, Edmund LoPresti","computer access, edge wite, gestures, joystick, pebbles, power wheelchair, text entry, text input, touchpad, unistrokes",110,117
10.1145/1028630.1028652,ASSETS,2004,Strategic design for users with diabetic retinopathy,factors influencing performance in a menu-selection task,"This paper examines factors that affect performance of a basic menu selection task by users who are visually healthy and users with Diabetic Retinopathy (DR) in order to inform better interface design. Interface characteristics such as multimodal feedback, Windows&#174; accessibility settings, and menu item location were investigated. Analyses of Variance (ANOVA) were employed to examine the effects of interface features on task performance. Linear regression was used to further examine and model various contextual factors that influenced task performance. Results indicated that Windows&#174; accessibility settings significantly improved performance of participants with more progressed DR. Additionally, other factors, including age, computer experience, visual acuity, and menu location were significant predictors of the time required for subjects to complete the task.","Paula Edwards, Leon Barnard, V. Emery, Ji Yi, Kevin Moloney, Thitima Kongnakorn, Julie Jacko, Fran&#231;ois Sainfort, Pamela Oliver, Joseph Pizzimenti, Annette Bade, Greg Fecho, Josephine Shallo-Hoffmann","auditory feedback, diabetic retinopathy, haptic feedback, menus, multimodal feedback, visual impairment, windows accessibility settings",118,125
10.1145/1028630.1028653,ASSETS,2004,Image pre-compensation to facilitate computer access for users with refractive errors, ,"The use of computer technology for everyday tasks has become increasingly important in today's world. Frequently, computer technology makes use of Graphical User Interfaces (GUIs), presented through monitors or LCD displays. This type of visual interface is not well suited for users with visual limitations due to refractive errors, particularly when they are severe and not correctable by common means. In order to facilitate computer access for users with refractive deficiencies, an algorithm was developed, using a priori knowledge of the visual aberration, to generate an inverse transformation of the images that are then displayed on-screen, countering the effect of the aberration. The result is that when the user observes the screen displaying the transformed images, the image perceived in the retina will be similar to the original image. The algorithm was tested by artificially introducing a spherical aberration in the field of view of 14 subjects, totaling 28 individual eyes. Results show that when viewing the screen, this method of compensation improves the visual performance of the subjects tested in comparison to viewing uncompensated images.","Miguel Alonso, Armando Barreto, J. Cremades","compensation, deblurring, deconvolution, low vision, point, spread function, visual aberration, wavefront aberration",126,132
10.1145/1028630.1028654,ASSETS,2004,Nonvisual tool for navigating hierarchical structures, ,"The hierarchical structure of a program can be quite complex. As such, many Integrated Development Environments (IDEs) provide graphical representations of program structure at different levels of abstraction. Such representations are not very accessible to non-sighted programmers, as screen readers are not able to portray the underlying hierarchical structure of the information. In this paper, we define a set of requirements for an accessible tree navigation strategy. An implementation of this strategy was developed as a plug-in to the Eclipse IDE and was tested by twelve student programmers. The evaluation of the tool shows the strategy to be an efficient and effective way for a non-sighted programmer to navigate hierarchical structures.","Ann Smith, Justin Cook, Joan Francioni, Asif Hossain, Mohd Anwar, M. Rahman","hierarchical structures, java programmers, navigation",133,139
10.1145/1028630.1028656,ASSETS,2004,Designing a cognitive aid for the home,a case-study approach,"Cognitive impairments play a large role in the lives of surviviors of mild traumatic brain injuries who are unable to return to their prior level of independence in their homes. Computational support has the potential to enable these individuals to regain control over some aspects of their lives. Our research aims to carefully seek out issues that might be appropriate for computational support and to build enabling technologies that increase individuals' functional independence in the home environment. Using a case-study approach, we explored the needs and informed the design of a pacing aid for an individual with a cognitive impairment whose quality of life was negatively affected by her inability to pace herself during her morning routine. The contributions of this research include insights we gained with our methodology, two sets of design dimensions: user-centered contraints developed from capabilities and preferences of our users and system-centered capabilities that could be explored in potential designs, a design concept which illustrates the application of these design dimensions into a potential pacing aid, and evaluations of paper prototypes guided by the design dimensions.","Jessica Paradise, Elizabeth Mynatt, Cliff Williams, John Goldthwaite","case study, cognitive impairment, home, pacing, traumatic brain injury",140,146
10.1145/1028630.1028657,ASSETS,2004,Design and development of an indoor navigation and object identification system for the blind, ,"In this paper we present a new system that assists blind users in orienting themselves in indoor environments. We developed a sensor module that can be handled like a flashlight by a blind user and can be used for searching tasks within the three-dimensional environment. By pressing keys, inquiries concerning object characteristics, position, orientation and navigation can be sent to a connected portable computer, or to a federation of data servers providing models of the environment. Finally these inquiries are acoustically answered over a text-to-speech engine.","Andreas Hub, Joachim Diepstraten, Thomas Ertl","blind users, impaired vision, indoor navigation, mobile computing",147,152
10.1145/1028630.1028658,ASSETS,2004,The ethnographically informed participatory design of a PD application to support communication, ,"Aphasia is an acquired communication deficit that impacts the different language modalities. PDAs have a form factor and feature set that suggest they could be effective communication tools for people with aphasia. An ethnographic study was conducted with one participant both to learn about communication strategies used by people with aphasia, and to observe how a PDA is incorporated into those strategies. The most significant usability issues found were file access and organization. A participatory design phase followed, resulting in a paper prototype of a file management system that addressed the key usability issues identified. The participatory approach continued during the implementation of a high-fidelity prototype.","Rhian Davies, Skip Marcella, Joanna McGrenere, Barbara Purves","aphasia, assistive technology, augmentative alternative communication, cognitive disabilities, ethnography, handheld devices, participatory design, universal usability",153,160
10.1145/1028630.1028659,ASSETS,2004,visiBabble for reinforcement of early vocalization, ,The visiBabble system processes infant vocalizations in real-time. It responds to the infant's syllable-like productions with brightly colored animations and records the acoustic-phonetic analysis. The system reinforces the production of syllabic utterances that are associated with later language and cognitive development. We report here on the development of the visiBabble prototype and field-testing of the system.,"Harriet Fell, Cynthia Cress, Joel MacAuslan, Linda Ferrier","acoustic analysis, acoustic landmarks, babbles, early intervention",161,168
10.1145/1028630.1028661,ASSETS,2004,A web accessibility service: update and findings, ,"We report here on our progress on a project first described at the ASSETS 2002 conference. At that time, we had developed a prototype system in which a proxy server intermediary was used to adapt Web pages to meet the needs of older adults. Since that report, we field tested the prototype and learned of problems with the proxy approach. We report on the lessons learned from that work and on our new approach towards meeting the Web needs of older adults and users with disabilities. This new software makes adaptations on the client machine, with greater accuracy and speed than was possible with the proxy server approach. It transforms Web pages ""on the fly"", without requiring that all Web content be re-written. The new software has been in use for a year and we report here on our findings from the usage. We discuss this approach in the context of Web accessibility standards and Web usability.","Vicki Hanson, John Richards","standards, user interface, web accessibility",169,176
10.1145/1028630.1028662,ASSETS,2004,Accessibility designer,visualizing usability for the blind,"These days, accessibility-related regulations and guidelines have been accelerating the improvement of Web accessibility. One of the accelerating factors is the development and deployment of accessibility evaluation tools for authoring time and repair time. They mainly focus on creating compliant Web sites by analyzing the HTML syntax of pages, and report that pages are compliant when there are no syntactical errors. However, such compliant pages are often not truly usable by blind users. This is because current evaluation tools merely check if the HTML tags are appropriately used to be compliant with regulations and guidelines. It would be better if such tools paid more attention to real usability, especially on time-oriented usability factors, such as the speed to reach target content, the ease of understanding the page structure, and the navigability, in order to help Web designers to create not simply compliant pages but also usable pages for the blind. Therefore, we decided to develop Accessibility Designer (aDesigner), which has capabilities to visualize blind users' usability by using colors and gradations. The visualization function allows Web designers to grasp the weak points in their pages, and to recognize how accessible or inaccessible their pages are at a glance. In this paper, after reviewing the related work, we describe our approach to visualize blind users' usability followed by an overview of Accessibility Designer. We then report on our evaluations of real Web sites using Accessibility Designer. After discussing the results, we conclude the paper.","Hironobu Takagi, Chieko Asakawa, Kentarou Fukuda, Junji Maeda","accessibility, accessibility checker, blind, visually impaired, voice usability",177,184
10.1145/1028630.1028663,ASSETS,2004,Semantic bookmarking for non-visual web access, ,"Bookmarks are shortcuts that enable quick access of the desired Web content. They have become a standard feature in any browser and recent studies have shown that they can be very useful for non-visual Web access as well. Current bookmarking techniques in assistive Web browsers are rigidly tied to the structure of Web pages. Consequently they are susceptible to even slight changes in the structure of Web pages. In this paper we propose &#60;i>semantic bookmarking&#60;/i> for non-visual Web access. With the help of an ontology that represents concepts in a domain, content in Web pages can be semantically associated with bookmarks. As long as these associations can be identified, semantic bookmarks are resilient in the face of structural changes to the Web page. The use of ontologies allows semantic bookmarks to span multiple Web sites covered by a common domain. This contributes to the ease of information retrieval and bookmark maintenance. In this paper we describe highly automated techniques for creating and retrieving semantic bookmarks. These techniques have been incorporated into an assistive Web browser. Preliminary experimental evidence suggests the effectiveness of semantic bookmarks for non-visual Web access.","Saikat Mukherjee, I. Ramakrishnan, Michael Kifer","assistive browsing, bookmarks, semantic partitioning",185,192
10.1145/1090785.1090790,ASSETS,2005,What help do older people need?,constructing a functional design space of electronic assistive technology applications,"In times of ageing populations and shrinking care resources, electronic assistive technology (EAT) has the potential of contributing to guaranteeing frail older people a continued high quality of life. This paper provides users and designers of EAT with an instrument for choosing and producing relevant and useful EAT applications in the form of a functional design space. We present the field study that led to the design space, and give advice on using the tool.","Dennis Maciuszek, Johan Aberg, Nahid Shahmehri","design space, field study, interactive agents, needs, older adults, software components, user involvement",4,11
10.1145/1090785.1090791,ASSETS,2005,An exploratory investigation of handheld computer interaction for older adults with visual impairments, ,"This study explores factors affecting handheld computer interaction for older adults with Age-related Macular Degeneration (AMD). This is largely uncharted territory, as empirical investigations of human-computer interaction (HCI) concerning users with visual dysfunction and/or older adults have focused primarily on desktop computers. For this study, participants with AMD and visually-healthy controls used a handheld computer to search, select and manipulate familiar playing card icons under varied icon set sizes, inter-icon spacing and auditory feedback conditions. While all participants demonstrated a high rate of task completion, linear regression revealed several relationships between task efficiency and the interface, user characteristics and ocular factors. Two ocular measures, severity of AMD and contrast sensitivity, were found to be highly predictive of efficiency. The outcomes of this work reveal that users with visual impairments can effectively interact with GUIs on small displays in the presence of low-cost, easily implemented design interventions. This study presents a rich data set and is intended to inspire future work exploring the interactions of individuals with visual impairments with non-traditional information technology platforms, such as handheld computers.","V. Leonard, Julie Jacko, Joseph Pizzimenti","auditory feedback, dpacing, drag and drop, icons, macular degeneration, mobile computing, older adults, visual impairment",12,19
10.1145/1090785.1090792,ASSETS,2005,Programmer-focused website accessibility evaluations, ,"Suggested methods for conducting website accessibility evaluations have typically focused on the needs of end-users who have disabilities. However, programmers, not people with disabilities, are the end-users of evaluations reports generated by accessibility specialists. Programmers' capacity and resource needs are seldom met by the voluminous reports and long lists of individual website fixes commonly produced using earlier methods. The rationale for the need to consider the whole website development process, and the social characteristics of programmers and project managers is presented. A new programmer-centric Streamlined Evaluation and Reporting Process for Accessibility (SERPA) is described in detail.","Chris Law, Julie Jacko, Paula Edwards","accessibility, evaluation, internet, reporting",20,27
10.1145/1090785.1090793,ASSETS,2005,The information-theoretic analysis of unimodal interfaces and their multimodal counterparts, ,"That multimodal interfaces have benefits over unimodal ones has often been asserted. Several such benefits have been described informally, but, to date, few have actually been formalized or quantified. In this paper, the hypothesized benefits of <i>semantically redundant multimodal input actions</i> are described formally and are quantified using the formalisms provided by Information Theory. A reinterpretation of Keates and Robinson's empirical data (1998) shows that their criticism of multimodal interfaces was, in part, unfounded.",Melanie Baljko,"augmentative and alternative communication (AAC), interface evaluation, interventions for communication disorders, multimodal interfaces, speech generating devices (SGD), voice output communication aids (VOCA)",28,35
10.1145/1090785.1090795,ASSETS,2005,Wizard-of-Oz test of ARTUR,a computer-based speech training system with articulation correction,"This study has been performed in order to test the human-machine interface of a computer-based speech training aid named ARTUR with the main feature that it can give suggestions on how to improve articulation. Two user groups were involved: three children aged 9-14 with extensive experience of speech training, and three children aged 6. All children had general language disorders.The study indicates that the present interface is usable without prior training or instructions, even for the younger children, although it needs some improvement to fit illiterate children. The granularity of the mesh that classifies mispronunciations was satisfactory, but can be developed further.","Olle B&#228;lter, Olov Engwall, Anne-Marie &#214;ster, Hedvig Kjellstr&#246;m","Wizard-of-Oz, computer-based speech training system, user interface",36,43
10.1145/1090785.1090796,ASSETS,2005,Representing coordination and non-coordination in an american sign language animation, ,"While strings and syntax trees are used by the Natural Language Processing community to represent the structure of spoken languages, these encodings are difficult to adapt to a signed language like American Sign Language (ASL). In particular, the multichannel nature of an ASL performance makes it difficult to encode in a linear single-channel string. This paper will introduce the Partition/Constitute (P/C) Formalism, a new method of computationally representing a linguistic signal containing multiple channels. The formalism allows coordination and non-coordination relationships to be encoded between different portions of a signal. The P/C formalism will be compared to representations used in related research in gesture animation. The way in which P/C is used by this project to build an English-to-ASL machine translation system will also be discussed.",Matt Huenerfauth,"accessibility technology for the deaf, american sign language, gesture generation, multimodal generation",44,51
10.1145/1090785.1090797,ASSETS,2005,Visualizing non-speech sounds for the deaf, ,"Sounds constantly occur around us, keeping us aware of our surroundings. People who are deaf have difficulty maintaining an awareness of these ambient sounds. We present an investigation of peripheral, visual displays to help people who are deaf maintain an awareness of sounds in the environment. Our contribution is twofold. First, we present a set of visual design preferences and functional requirements for peripheral visualizations of non-speech audio that will help improve future applications. Visual design preferences include ease of interpretation, glance-ability, and appropriate distractions. Functional requirements include the ability to identify what sound occurred, view a history of displayed sounds, customize the information that is shown, and determine the accuracy of displayed information. Second, we designed, implemented, and evaluated two fully functioning prototypes that embody these preferences and requirements, serving as examples for future designers and furthering progress toward understanding how to best provide peripheral audio awareness for the deaf.","Tara Matthews, Janette Fong, Jennifer Mankoff","deaf, peripheral display, sound visualization",52,59
10.1145/1090785.1090799,ASSETS,2005,Accuracy and frequency analysis of multitouch interfaces for individuals with Parkinsonian and essential hand tremor, ,"In this study, the accuracy of an optical mouse, optical trackball, isotonic joystick, and a FingerWorks MultiTouch Surface (MTS) are compared for users suffering from Parkinsonian tremor and essential tremor. Using a data acquisition program, <i>WinFitts</i>, created at the University of Oregon's HCI Lab, data collected from five subjects with Parkinsonian tremor, five with essential tremor, and eleven with no tremor is analyzed and compared. Both <i>temporal</i> and <i>spatial</i> analyses are obtained from all of the subject data. The time-based measures of performance for each device include the uses of Fitts' law and the <i>Proximity Movement Time</i>, while the spatially-based measures include the use of the <i>Deviation Accuracy</i> and the <i>Click Histogram</i>. A statistical analysis is performed using a <i>t</i>-test to show the differences between the resulting means of some of the measures. By using the MUSIC spectral estimation technique, an analysis of the frequency and the amplitude of the tremor showed how certain devices performed in hand tremor suppression.","Eric Frett, Kenneth Barner","Fitts' law, Parkinson's disease, essential tremor, human-computer interaction, multitouch surfaces",60,67
10.1145/1090785.1090800,ASSETS,2005,Effect of age and Parkinson's disease on cursor positioning using a mouse, ,"Point-and-click tasks are known to present difficulties to users with physical impairments, particularly motor- or vision-based, and to older adults. This paper presents the results of a study to quantify and understand the effects of age and impairment on the ability to perform such tasks. Results from four separate user groups are presented and compared using metrics that describe the features of the movements made. Distinct differences in behaviour between all of the user groups are observed and the reasons for those differences are discussed.","Simeon Keates, Shari Trewin","age, cursor, cursor positioning tasks, disability, mouse, performance measurement",68,75
10.1145/1090785.1090801,ASSETS,2005,The migratory cursor,accurate speech-based cursor movement by moving multiple ghost cursors using non-verbal vocalizations,"We present the migratory cursor, which is an interactive interface that enables users to move a cursor to any desired position quickly and accurately using voice alone. The migratory cursor combines discrete specification that allows a user to specify a location quickly, but approximately, with continuous specification that allows the user to specify a location more precisely, but slowly. The migratory cursor displays multiple ghost cursors that are aligned vertically or horizontally with the actual cursor. The user quickly specifies an approximate position by referring to the ghost cursor nearest the desired position, and then uses non-verbal vocalizations to move the ghost cursors continuously until one is on the desired position. The time spent using the continuous specification which is slow to use is short, since it is used just for fine refinement. In addition, the migratory cursor employs only two directional movements: vertical and horizontal, so that the user can move it quickly to any desired position. Moreover, the user can easily and accurately stop cursor movements by becoming silent when the cursor reaches the desired position. We tested the usefulness of the migratory cursor, and showed that users could move the cursor to a desired position quickly and accurately.","Yoshiyuki Mihara, Etsuya Shibayama, Shin Takahashi","non-verbal voice input, speech-based cursor movement",76,83
10.1145/1090785.1090802,ASSETS,2005,Toward Goldilocks' pointing device,"determining a ""just right"" gain setting for users with physical impairments","We designed and evaluated an agent that recommends a pointing device gain for a given user, with mixed success. 12 participants with physical impairments used the Input Device Agent (IDA), to determine a recommended gain based on their performance over a series of target acquisition trials. IDA recommended a gain other than the Windows default for 9 of 12 subjects. Subsequent performance using the IDA gain showed no meaningful differences as compared to the default setting or users' pre-study settings. Across all gains used by these subjects, however, gain did have a significant effect on throughput, percent of error-free trials, cursor entries, and overshoot. Linear models of gain's effect on performance showed that its effect on throughput is relatively small, with only a 13% difference from highest throughput (at gain = 10) to lowest throughput (at gain = 6). Cursor entries were more strongly affected, showing a steady increase with increasing gain.","Heidi Koester, Edmund LoPresti, Richard Simpson","adaptive computer interfaces, assistive technology, computer pointing devices, physical impairment",84,89
10.1145/1090785.1090804,ASSETS,2005,Gist summaries for visually impaired surfers, ,"Anecdotal evidence suggests that Web document summaries provide the sighted reader with a basis for making decisions regarding the route to take within non-linear text; and additional research shows that sighted people use 'Gist' summaries as decision points to bolster their browsing behaviour. Other studies have found that visually impaired users are hindered in their cognition of the content of Web-pages because users must wait for an entire Web-page to be read before deciding on it's usefulness to their current task. In these cases, we draw similarities between sighted and visually impaired users, in that sighted users cannot see the target of a Web Anchor and are therefore 'handicapped'<sup>1</sup> by the technology. Previously, we have investigate four simple summarisation algorithms against each other and a manually created summary; producing empirical evidence as a formative evaluation. This evaluation concludes that users prefer simple automatically generated 'gist' summaries thereby reducing cognitive overload and increasing awareness of the focus of the Web-page under investigation. In this paper we focus on the development of 'FireFox' based tool which creates a summary of a Web page 'on-the-fly'. The algorithm used to create this summary is based on the results of our formative evaluation which automatically and dynamically annotates Web pages with the generated 'gist' summary. In this way visually impaired users are supported in their decisions as the relevancy of the page at hand.","Simon Harper, Neha Patel","document engineering, tools, visual impairment, web",90,97
10.1145/1090785.1090805,ASSETS,2005,Talking braille,a wireless ubiquitous computing network for orientation and wayfinding,"An ubiquitous computing network is being developed to assist persons with vision loss in finding their way around buildings and other indoor public spaces. It is based on the ""Cyber Crumb"" concept: the idea that tiny, inexpensive solar-powered digital chips can be used to store relevant pieces of information that can be placed along building walkways like a trail of crumbs to follow. A wireless network of ""crumbs"" provides access from any point in the building to a central server that provides orientation and wayfinding information. Initial hardware and consumer tests verify feasibility and benefit.","David Ross, Alexander Lightman","blindness, mobility aid, orientation, wayfinding",98,105
10.1145/1090785.1090806,ASSETS,2005,A wearable face recognition system for individuals with visual impairments, ,"This paper describes the iCare Interaction Assistant, an assistive device for helping the individuals who are visually impaired during social interactions. The research presented here addresses the problems encountered in implementing real-time face recognition algorithms on a wearable device. Face recognition is the initial step towards building a comprehensive social interaction assistant that will identify and interpret facial expressions, emotions and gestures. Experiments conducted for selecting a face recognition algorithm that works despite changes in facial pose and illumination angle are reported. Performance details of the face recognition algorithms tested on the device are presented along with the overall performance of the system. The specifics of the hardware components used in the wearable device are mentioned and the block diagram of the wearable system is explained in detail.","Sreekar Krishna, Greg Little, John Black, Sethuraman Panchanathan","assistive device for visually impaired, face recognition, social interaction aide, wearable computing",106,113
10.1145/1090785.1090807,ASSETS,2005,Sparsha,a comprehensive indian language toolset for the blind,"Braille and audio feedback based systems have vastly improved the lives of the visually impaired across a wide majority of the globe. However, more than 13 million visually impaired people in the Indian sub-continent could not benefit much from such systems. This was primarily due to the difference in the technology required for Indian languages compared to those corresponding to other popular languages of the world. In this paper, we describe the Sparsha toolset. The contribution made by this research has enabled the visually impaired to read and write in Indian vernaculars with the help of a computer.","Anirban Lahiri, Satya Chattopadhyay, Anupam Basu","Indian languages, audio feedback, braille, visual impairment",114,120
10.1145/1090785.1090809,ASSETS,2005,Semantic knowledge in word completion, ,"We propose an integrated approach to interactive word-completion for users with linguistic disabilities in which semantic knowledge combines with $n$-gram probabilities to predict semantically more-appropriate words than $n$-gram methods alone. First, semantic relatives are found for English words, specifically for nouns, and they form the semantic knowledge base. The selection process for these semantically related words is first to rank the pointwise mutual information of co-occurring words in a large corpus and then to identify the semantic relatedness of these words by a Lesk-like filter. Then, the semantic knowledge is used to measure the semantic association of completion candidates with the context. Those that are semantically appropriate to the context are promoted to the top positions in prediction lists due to their high association with context. Experimental results show a performance improvement when using the integrated model for the completion of nouns.","Jianhua Li, Graeme Hirst","linguistic semantics, pointwise mutual information, word completion",121,128
10.1145/1090785.1090810,ASSETS,2005,Research-derived web design guidelines for older people, ,"This paper presents the development of a set of research-derived ageing-centred Web design guidelines. An initial set of guidelines was first developed through an extensive review of the HCI and ageing literature and through employing a series of classification methods (card sorting and affinity diagrams) were employed as a means for obtaining a revised and more robust set of guidelines. A group of older Web users were then involved in evaluating the usefulness of the guidelines. To provide evaluation context for these users, two websites targeted to older people were used. This study makes several contributions to the field. First, it is perhaps the first manuscript that proposes ageing-friendly guidelines that are for most part backed by published studies. Second, the guidelines proposed in this study have been thoroughly examined through a series of expert and user verifications, which should give users of these guidelines confidence of their validity.","Sri Kurniawan, Panayiotis Zaphiris","HCI, ageing, elderly, seniors, web design guidelines",129,135
10.1145/1090785.1090811,ASSETS,2005,Autism/excel study, ,"Five high school students with ASD (autistic spectrum disorder) participating in the Excel/Autism study were able to demonstrate mastery of a set of Excel topics. The Excel curriculum covered approximately the same topics as are covered in the Excel portion of Computer Business Applications, a class for regular education students at Fox Chapel Area High School, a high school in suburban Pittsburgh. The students with ASD were provided with one-on-one tutoring support. Two of the five ASD participants self-initiated activities and engaged in generative thinking to a substantial degree over the course of the eight instructional sessions for which data was recorded. Two others demonstrated lesser amounts of this behavior, and one participant did not demonstrate any. The ASD experimental participants, as compared to a treatment group of three students with ASD who did not receive instruction in Excel, demonstrated improvement in a multi-step planning task which was significant.",Mary Hart,"adapted computer curriculum, autism, generative thinking, multi-step planning, spreadsheets",136,141
10.1145/1090785.1090812,ASSETS,2005,Requirements gathering with alzheimer's patients and caregivers, ,"Technology may be able to play a role in improving the quality of life for Alzheimer's patients and their caregivers. We are evaluating the feasibility of an information appliance with the goal of alleviating repetitive questioning behaviour, a contributing factor to caregiver stress. Interviews were conducted with persons with Alzheimer's disease and their caregivers to determine the nature of the repetitive questioning behaviour, the information needs of patients, and the interaction abilities of both the patients and the caregivers. We report results of these interviews and discuss the challenges of requirements gathering with persons with Alzheimer's disease and the feasibility of introducing an information appliance to this population.","Kirstie Hawkey, Kori Inkpen, Kenneth Rockwood, Michael McAllister, Jacob Slonim","alzheimer's disease, assistive technology, cognitive aging, information appliance, user-centered design",142,149
10.1145/1090785.1090814,ASSETS,2005,Automating tactile graphics translation, ,"Access to graphical images (bar charts, diagrams, line graphs, etc.) that are in a tactile form (representation through which content can be accessed by touch) is inadequate for students who are blind and take mathematics, science, and engineering courses. We describe our analysis of the current work practices of tactile graphics specialists who create tactile forms of graphical images. We propose automated means by which to improve the efficiency of current work practices.We describe the implementation of various components of this new automated process, which includes image classification, segmentation, simplification, and layout. We summarize our development of the tactile graphics assistant, which will enable tactile graphics specialists to be more efficient in creating tactile graphics both in batches and individually. We describe our unique team of researchers, practitioners, and student consultants who are blind, all of whom are needed to successfully develop this new way of translating tactile graphics.","Richard Ladner, Melody Ivory, Rajesh Rao, Sheryl Burgstahler, Dan Comden, Sangyun Hahn, Matthew Renzelmann, Satria Krisnandi, Mahalakshmi Ramasamy, Beverly Slabosky, Andrew Martin, Amelia Lacenski, Stuart Olsen, Dmitri Groce","accessibility, braille, disability, image processing, machine learning, tactile graphics, user study",150,157
10.1145/1090785.1090815,ASSETS,2005,SmartColor,disambiguation framework for the colorblind,"Failure in visual communication between the author and the colorblind reader is caused when color effects that the author expects for the reader to experience are not observed by the reader. The proposed framework allows the author to annotate his/her intended color effects to the colored document. They are used to generate a repainted document that let the colorblind enjoy similar color effects that normal color vision person does for the original document. The annotations are formulated as a set of mathematical constraints that can describe several commonly used color effects. Constraints are defined over the normal vision color space. Then they are projected onto the restricted color space that corresponds to the one that the colorblind perceives. Finally, the projected constraints are resolved for the search of best repainting of the document that most successfully presents to the colorblind person the color effects experienced by the normal vision person on the original document. Effectiveness of the proposal is shown by colorblind simulation.","Ken Wakita, Kenta Shimamura","colorblindness, constraint system, vision",158,165
10.1145/1090785.1090816,ASSETS,2005,Automatic production of tactile graphics from scalable vector graphics, ,"This paper presents a method to convert vector graphics into tactile representations for the blind. Generating tactile pictures from vector graphics is an important effort to bring more accessibility to the WWW as well as other means of communications since vector graphics are an increasing trend in web based graphics. Prior research has investigated methods that extracts object boundaries from images to produce raised-line tactile pictures. The proposed method extends this idea for vector graphics, producing tactile pictures where important outlines are emphasized. Important outlines are determined by using the hierarchical structure of a vector graphic. A Braille printer is used where raised dots are embossed for the outlining boundaries. Important and detail boundaries are embossed with dots of larger and smaller height, respectively, while all other regions contain no raised dots. Results testing a person's ability to discriminate, identify, and comprehend tactile pictures shows the proposed methods' advantage over two other methods.","Stephen Krufka, Kenneth Barner","blindness, braille embosser, edge/boundary detection, scalable vector graphics, tactile images, tactile pictures",166,172
10.1145/1090785.1090817,ASSETS,2005,3D sound interactive environments for problem solving, ,"Audio-based virtual environments have been increasingly used to foster cognitive and learning skills. A number of studies have also highlighted that the use of technology can help learners to develop affective skills such as motivation and self-esteem. This study presents the design and usability of 3D interactive environments for children with visual disabilities to help them to solve problems related with the Chilean geography and culture. We introduce AudioChile, a virtual environment that can be navigated through 3D sound to enhance spatiality and immersion throughout the environment. 3D sound is used to orientate, avoid obstacles, and identify the position of diverse personages and objects within the environment. We have found during usability evaluation that sound can be fundamental for attention and motivation purposes during interaction. Learners identified and differentiated clearly environmental sounds to solve everyday problems, spatial orientation, and laterality.","Jaime S&#225;nchez, Mauricio S&#225;enz","3D sound, hyperstories, problem solving, role-playing game, virtual world",173,179
10.1145/1168987.1168990,ASSETS,2006,From letters to words,efficient stroke-based word completion for trackball text entry,"We present a major extension to our previous work on <i>Trackball EdgeWrite</i>--a unistroke text entry method for trackballs--by taking it from a character-level technique to a word-level one. Our design is called <i>stroke-based word completion,</i> and it enables efficient word selection as part of the stroke-making process. Unlike most word completion designs, which require users to select words from a list, our technique allows users to select words by performing a fluid <i>crossing gesture.</i> Our theoretical model shows this word-level design to be 45.0% faster than our prior model for character-only strokes. A study with a subject with spinal cord injury comparing Trackball EdgeWrite to the onscreen keyboard <i>WiViK,</i> both using word prediction and completion, shows that Trackball EdgeWrite is competitive with WiViK in speed (12.09 vs. 11.82 WPM) and accuracy (3.95% vs. 2.21% total errors), but less visually tedious and ultimately preferred. The results also show that word-level Trackball EdgeWrite is 46.5% faster and 36.7% more accurate than our subject's prior peak performance with character-level Trackball EdgeWrite, and 75.2% faster and 40.2% more accurate than his prior peak performance with his preferred on-screen keyboard. An additional evaluation of the same subject over a two-month field deployment shows a 43.9% reduction in unistrokes due to strokebased word completion in Trackball EdgeWrite.","Jacob Wobbrock, Brad Myers","EdgeWrite, Fitts' law, Hick-Hyman law, WiViK, Zipf's law, gestures, goal crossing, steering law, text input, trackballs, unistrokes, word prediction and completion, word-level text entry",2,9
10.1145/1168987.1168991,ASSETS,2006,Alternative text entry using different input methods, ,"This paper deals with PC-based alternative (i.e., keyboardfree) text entry and the issues related to emulating keystrokes with only a limited number of input signals. The previously introduced HaMCoS tool tries to enable someone who cannot use the hands to enter text almost as fast as someone exclusively using a manual mouse. To achieve this rather ambitious goal, HaMCoS provides two different (but combinable) solutions. On the one hand, word completion is offered as a shortcut technique. On the other hand, in addition to a mere on-screen keyboard, a completely new application has been implemented where selecting characters is somehow similar to entering Morse code (but with <i>four</i> 'bits' instead of <i>dots and dashes</i> only). In order to show the effect of these measures, the times needed to copy a moderately long text in various circumstances are reported.","Torsten Felzer, Rainer Nordmann","BCI, bio-signals, hands-free operation, human-computer interaction, keyboard-free text entry, morse code, muscle control, single-switch, word completion",10,17
10.1145/1168987.1168992,ASSETS,2006,Indirect text entry using one or two keys, ,"This paper introduces a new descriptive model for indirect text composition facilities that is based on the notion of a <i>containment hierarchy.</i> This paper also demonstrates a novel, computer-aided technique for the design of indirect text selection interfaces -- one in which Huffman coding is used for the derivation of the containment hierarchy. This approach guarantees the derivation of optimal containment hierarchies, insofar as <i>mean encoding length.</i> This paper describes an empirical study of two two-key indirect text entry variants and compares them to one another and to the predictive model. The intended application of these techniques is the design of improved indirect text entry facilities for the users of AAC systems.","Melanie Baljko, Andrew Tam","augmentative and alternative communication (AAC), indirect text entry, information theory, interface evaluation, interventions for communication disorders, scanning, speech generating devices (SGD), voice output communication aids (VOCA)",18,25
10.1145/1168987.1168993,ASSETS,2006,Developing steady clicks:,a method of cursor assistance for people with motor impairments,"Slipping while clicking and accidental clicks are a source of errors for mouse users with motor impairments. The Steady Clicks assistance feature suppresses these errors by freezing the cursor during mouse clicks, preventing overlapping button presses and suppressing clicks made while the mouse is moving at a high velocity. Evaluation with eleven target users found that Steady Clicks enabled participants to select targets using significantly fewer attempts. Overall task performance times were significantly improved for the five participants with the highest slip rates. Blocking of overlapping and high velocity clicks also shows promise as an error filter. Nine participants preferred Steady Clicks to the unassisted condition. If used in conjunction with existing techniques for cursor positioning, all of the major sources of clicking errors observed in empirical studies would be addressed, enabling faster and more effective mouse use for those who currently struggle with the standard mouse.","Shari Trewin, Simeon Keates, Karyn Moffatt","clicking, clicking errors, disability, mouse, pointing and selection tasks, target acquisition, user input",26,33
10.1145/1168987.1168995,ASSETS,2006,A multi-domain approach for enhancing text display for users with visual aberrations, ,"In this paper, we describe a multi-domain approach for enhancing text displayed on a computer screen for users with visual aberrations. This research is based on a <i>priori</i> knowledge of the user's visual aberration, as measured by a wavefront analyzer. With this information it is possible to generate text that, when displayed to this user, will counteract his/her visual aberration. The method described in this paper advances the development of techniques for providing such compensation by integrating spatial information in the image as a means to eliminate some of the shortcomings inherent in using display devices such as monitors or LCD panels.","Miguel Alonso, Armando Barreto, Julie Jacko, Malek Adjouadi","deconvolution, pre-compensation visual aberration, spatial processing, wavefront aberration function, wavefront analysis",34,39
10.1145/1168987.1168996,ASSETS,2006,Accommodating color blind computer users, ,"Important visual information often disappears when color documents are viewed by color blind people. The algorithm introduced here maps colors using the World Wide Web Consortium evaluation criteria so that detail is preserved for color blind viewers, especially dichromats. The algorithm has four parts: 1) select a representative set of colors from the source document; 2) compute target color distances using color and brightness differences; 3) solve an optimization step that preserves the target distances for a particular class of color blind viewer; and 4) interpolate the mapped colors across the remaining colors in the document. We demonstrate the efficacy of our method using simulations and critique our method in the context of earlier work.","Luke Jefferson, Richard Harvey","accessibility, assistive technology, color vision deficiency",40,47
10.1145/1168987.1168997,ASSETS,2006,Lambda:,a multimodal approach to making mathematics accessible to blind students,"The study of mathematics is all but precluded to most blind students because of the reliance on visual notations. The Lambda System is an attempt to overcome this barrier to access through the development of a linear mathematical notation which can be manipulated by a multimodal mathematical editor. This provides access through braille, synthetic speech and a visual display. Initial results from a longitudinal study with prospective users are encouraging.","Alistair Edwards, Heather McCartney, Flavio Fogarolo","MathML, blind students, braille, mathematics education, synthetic speech",48,54
10.1145/1168987.1168998,ASSETS,2006,Measuring website usability for visually impaired people-a modified GOMS analysis, ,"Web designers regularly wonder which version of a design would suit best their target groups' needs. This becomes even more complicated if the design is to comply with accessibility rules. This paper describes an interaction model of blind users' interaction strategies. This model is based on GOMS (Goals, Operators, Methods, Selection rules) and can be used to measure aspects of website usability for blind users. The model evolved from findings of user observations and field studies. It can be applied to specific layouts in order to find the 'best' alternative. 'Classic' GOMS models lack functions which are necessary for the presented GOMS model. Thus, new structures to extend the classic GOMS notation are proposed. Finally, an example GOMS analysis is run on a modified version of the ASSETS '06 web page.",Henrik Tonn-Eichst&#228;dt,"GOMS, accessibility, blind users, braille, evaluation, screen reader, speech, usability, visually impaired users",55,62
10.1145/1168987.1169000,ASSETS,2006,Dynamically adapting GUIs to diverse input devices, ,"Many of today's desktop applications are designed for use with a pointing device and keyboard. Someone with a disability, or in a unique environment, may not be able to use one or both of these devices. We have developed an approach for automatically modifying desktop applications to accommodate a variety of input alternatives as well as a demonstration implementation, the Input Adapter Tool (IAT). Our work is differentiated from past work by our focus on <i>input adaptation</i> (such as adapting a paint program to work without a pointing device) rather than <i>output adaptation</i> (such as adapting web pages to work on a cellphone). We present an analysis showing how different common interactive elements and navigation techniques can be adapted to specific input modalities. We also describe IAT, which supports a subset of these adaptations, and illustrate how it adapts different inputs to two applications, a paint program and a form entry program.","Scott Carter, Amy Hurst, Jennifer Mankoff, Jack Li","accessibility, interaction techniques, toolkits",63,70
10.1145/1168987.1169001,ASSETS,2006,MobileASL:,intelligibility of sign language video as constrained by mobile phone technology,"For Deaf people, access to the mobile telephone network in the United States is currently limited to text messaging, forcing communication in English as opposed to American Sign Language (ASL), the preferred language. Because ASL is a visual language, mobile video phones have the potential to give Deaf people access to real-time mobile communication in their preferred language. However, even today's best video compression techniques can not yield intelligible ASL at limited cell phone network bandwidths. Motivated by this constraint, we conducted one focus group and one user study with members of the Deaf Community to determine the intelligibility effects of video compression techniques that exploit the visual nature of sign language. Inspired by eyetracking results that show high resolution foveal vision is maintained around the face, we studied region-of-interest encodings (where the face is encoded at higher quality) as well as reduced frame rates (where fewer, better quality, frames are displayed every second). At all bit rates studied here, participants preferred moderate quality increases in the face region, sacrificing quality in other regions. They also preferred slightly lower frame rates because they yield better quality frames for a fixed bit rate. These results show promise for realtime access to the current cell phone network through signlanguage-specific encoding techniques.","Anna Cavender, Richard Ladner, Eve Riskin","american sign language (ASL), deaf community, mobile telephone use, video compression",71,78
10.1145/1168987.1169002,ASSETS,2006,American sign language recognition in game development for deaf children, ,"CopyCat is an American Sign Language (ASL) game, which uses gesture recognition technology to help young deaf children practice ASL skills. We describe a brief history of the game, an overview of recent user studies, and the results of recent work on the problem of continuous, user-independent sign language recognition in classroom settings. Our database of signing samples was collected from user studies of deaf children playing aWizard of Oz version of the game at the Atlanta Area School for the Deaf (AASD). Our data set is characterized by disfluencies inherent in continuous signing, varied user characteristics including clothing and skin tones, and illumination changes in the classroom. The dataset consisted of 541 phrase samples and 1,959 individual sign samples of five children signing game phrases from a 22 word vocabulary. Our recognition approach uses color histogram adaptation for robust hand segmentation and tracking. The children wear small colored gloves with wireless accelerometers mounted on the back of their wrists. The hand shape information is combined with accelerometer data and used to train hidden Markov models for recognition. We evaluated our approach by using leave-one-out validation; this technique iterates through each child, training on data from four children and testing on the remaining child's data. We achieved average word accuracies per child ranging from 91.75% to 73.73% for the user-independent models.","Helene Brashear, Valerie Henderson, Kwang-Hyun Park, Harley Hamilton, Seungyon Lee, Thad Starner","ASL, game, recognition, sign language",79,86
10.1145/1168987.1169003,ASSETS,2006,"Are ""universal design resources"" designed for designers?", ,"Universal design (UD) is an approach to design that incorporates things which can be used by all people to the greatest extent possible. UD in information and communication technologies (ICTs) is of growing importance because standard ICTs have great potential to be usable by all people, including people with disabilities (PWDs). Currently, PWDs who need ICTs often have less access because the products have not been universally designed. We hypothesize that one of the reasons for the slow adoption of UD is that universal design resources (UDRs) are not adequate for facilitating designers' tasks. We investigated the usability of UDRs from designers' perspectives. A heuristic evaluation on eight selected UDRs was conducted, and the opinions of contributors to the content of these resources were collected through a web-based survey study. The results of the heuristic evaluation show that most of the investigated UDRs do not provide a clear central idea and fail to support the cognitive processes of designers. The results of the survey also confirmed that the content of these resources do not systematically address the needs of designers as end-users during the development process.","Young Choi, Ji Yi, Chris Law, Julie Jacko","assistive technology, design resources, heuristic evaluation, universal design",87,94
10.1145/1168987.1169005,ASSETS,2006,Indoor wayfinding:,developing a functional interface for individuals with cognitive impairments,"Assistive technology for wayfinding will significantly improve the quality of life for many individuals with cognitive impairments. The user interface of such a system is as crucial as the underlying implementation and localization technology. We built a system using the Wizard-of-Oz technique that let us experiment with many guidance strategies and interface modalities. Through user studies, we evaluated various configurations of the user interface for accuracy of route completion, time to completion, and user preferences. We used a counter-balanced design that included different modalities (images, audio, and text) and different routes. We found that although users were able to use all types of modalities to find their way indoors, they varied significantly in their preferred modalities. We also found that timing of directions requires careful attention, as does providing users with confirmation messages at appropriate times. Our findings suggest that the ability to adapt indoor wayfinding devices for specific users' preferences and needs will be particularly important.","Alan Liu, Harlan Hile, Henry Kautz, Gaetano Borriello, Pat Brown, Mark Harniss, Kurt Johnson","cognitive disability, ubiquitous computing, user interface, wizard-of-oz",95,102
10.1145/1168987.1169006,ASSETS,2006,Where's my stuff?,design and evaluation of a mobile system for locating lost items for the visually impaired,"Finding lost items is a common problem for the visually impaired and is something that computing technology can help alleviate. In this paper, we present the design and evaluation of a mobile solution, called FETCH, for allowing the visually impaired to track and locate objects they lose frequently but for which they do not have a specific strategy for tracking. FETCH uses devices the user already owns, such as their cell phone or laptop, to locate objects around their house. Results from a focus group with visually impaired users informed the design of the system. We then studied the usability of a laptop solution in a laboratory study and studied the usability and usefulness of the system through a one-month deployment and diary study. These studies demonstrate that FETCH is usable and useful, but there is still room for improvement.","Julie Kientz, Shwetak Patel, Arwa Tyebkhan, Brian Gane, Jennifer Wiley, Gregory Abowd","assistive technology, item location, mobile technology, ubiquitous computing, visually impaired",103,110
10.1145/1168987.1169007,ASSETS,2006,Interactive tracking of movable objects for the blind on the basis of environment models and perception-oriented object recognition methods, ,"In previous work we have presented a prototype of an assistant system for the blind that can be used for self-localization and interactive object identification of static objects stored within 3D environment models. In this paper we present a new method for interactive tracking of various types of movable objects. The state of fixed movable objects, like doors, can be recognized by comparing the distance between sensor data and a 3D model. For the identification and model-based tracking of free movable objects, like chairs, we have developed an algorithm that is similar to human perception, based on shape and color comparisons to trained objects. Further, using a common face detection algorithm, our assistant system informs the user of the presence of people, and enables the localization of a real person based on interactive tracking of virtual models of humans.","Andreas Hub, Tim Hartter, Thomas Ertl","blind users, impaired vision, indoor navigation, mobile computing",111,118
10.1145/1168987.1169008,ASSETS,2006,Using an audio interface to assist users Who are visually impaired with steering tasks, ,"In this paper we describe the latest results in our on-going study of techniques to present relational graphs to users with visual impairments. Our work tests the effectiveness of the PLUMB software package, which uses audio feedback and the pen-based Tablet PC interface to relay graphs and diagrams to users with visual impairments. Our study included human trials with ten participants without usable vision, in which we evaluated the users' ability to perform steering tasks under varying conditions.","Robert Cohen, Valerie Haven, Jessica Lanzoni, Arthur Meacham, Joelle Skaff, Michael Wissell","accessibility, audio, graph, sonification",119,124
10.1145/1168987.1169010,ASSETS,2006,Networked reminiscence therapy for individuals with dementia by using photo and video sharing, ,"Reminiscence therapy, which is effective for increasing the selfesteem of and for reducing behavioral disturbances in individuals with dementia, is usually conducted in a group led by experienced staff. However, due to the shortage of care attendants, only a limited number of patients at home can receive the benefits of this therapy. To provide this therapy for patients anytime or anywhere, we have developed a networked reminiscence therapy system that combines IP videophones with a photo- and video-sharing mechanism based on Web technology. First, we prepared the experimental setup in a hospital and examined whether dementia patients could communicate with therapists by videophone. Then we conducted a field trial of networked reminiscence therapy with a more realistic situation where remote volunteers communicated with dementia sufferers in the care home by IP videophones connected by broadband network. In this paper, we describe our developed system. Then, we present experimental results showing that dementia sufferers could communicate with therapists by videophone and that networked reminiscence sessions were generally as successful for individuals with dementia as face-to-face reminiscence sessions.","Noriaki Kuwahara, Shinji Abe, Kiyoshi Yasuda, Kazuhiro Kuwabara","IP videophone, dementia, internet, photo sharing, reminiscence therapy, web browser",125,132
10.1145/1168987.1169011,ASSETS,2006,Attention analysis in interactive software for children with autism, ,"This work is a part of an ongoing project that focuses on potential applications of an interactive system that helps children with autism. Autism is classified as a neurodevelopmental disorder that manifests itself in markedly abnormal social interaction, communication ability, patterns of interests, and patterns of behavior [1]. Children with autism are socially impaired and usually do not attend to the people around them. An interesting point which characterized children with autism is that they are unable to choose which event is more or less important. As a consequence they are often saturated because of too many stimuli and thus they adopt an extremely repetitive, unusual, self-injurious, or aggressive behaviour. Recently, a new trend of using human computer interface (HCI) technology and computer science in the treatment of autism has emerged [2, 3]. The platform we developed helps children with autism to focus their attention on a specific task. In this article, we only present the attention analysis system which is a part of a more general system that used a multi-agent architecture [4]. Each task proposed on our system fit to each child, is reproducible and evolutive following a specific scenario defined by the expert. This scenario takes into account age, ability, and degree of autism of each child. In order to focus a child's attention onto the relevant object, our system displays or plays specific stimulus; once again the specific stimulus is defined for each child. Symbol or sound represents an emotional and satisfaction value for the child. The major problem is to define the correct moment when the system has to (dis)play this signal. We tackle this problem by defining a robust measure of attention. This measure is defined by analyzing the gaze direction and the face orientation, and incorporating the child's specific profile. Following expert directives, our system helps children to categorize elementary perception (strong, smooth, quick, slow, big, small...). Our objective is that children re-use these classifications in others situations.","A. Mohamed, V. Courboulay, K. Sehaba, M. Menard","attention analysis, behavior study, children with autism",133,140
10.1145/1168987.1169012,ASSETS,2006,Understanding emotion through multimedia,comparison between hearing-impaired people and people with hearing abilities,"We conducted an experiment to determine the abilities of hearing-impaired and normal-hearing people to recognize intended emotions conveyed in four types of stimuli: a drum performance, a drum performance accompanied by a drawing expressing the same intended emotion, and a drum performance accompanied by one of two types of motion pictures. The recognition rate was the highest for a drum performance accompanied by a drawing even though participants in both groups found it difficult to identify the intended emotion because they felt the two stimuli sometimes conveyed different emotions. Visual stimuli were especially effective for performances whose intended emotions were not clear by themselves. The difference in ability to recognize intended emotions between the hearing-impaired and normal-hearing participants was insignificant. The results of this and a series of experiments will enable us to better understand the similarities and differences between how people with different hearing abilities encode and decode emotions in and from sound and visual media. We should then be able to develop a system that will enable hearing-impaired and normal-hearing people to play music together.","Rumi Hiraga, Nobuko Kato","drum performance, emotion, hearing-impairment, recognition",141,148
10.1145/1168987.1169013,ASSETS,2006,Determining the impact of computer frustration on the mood of blind users browsing the web, ,"While previous studies have investigated the impact of frustration on computer users' mood as well as the causes of frustration, no research has ever been conducted to examine the relationship between computer frustrations and mood change for users with visual impairment. In this paper, we report on a study that examined the frustrating experiences and mood change of 100 participants, all with visual impairments, when they were browsing the web. The result shows that frustration does cause the participants' mood to deteriorate. However, the amount of time lost due to frustrating situations does not have a significant impact on users' mood, which is very different from the previous research on users without visual impairment. The impact on work seems to have the greatest impact on user mood.","Jonathan Lazar, Jinjuan Feng, Aaron Allen","assistive technology, emotion, error, frustration, screen reader, time diary, visual impairment, web usability",149,156
10.1145/1168987.1169015,ASSETS,2006,Transforming flash to XML for accessibility evaluations, ,"Rich Internet content, such as Flash and DHTML, has been spreading all over the net, since it can provide rich and dynamic Web experiences for the sighted majority. It is obvious that this content is inaccessible for visually impaired people because of its visual richness. For Flash, many efforts have been made to address the issue, such as accessibility guidelines and best practices documents However, the amount of accessible content has not been increasing in spite of these efforts. One of the severe issues is the lack of tools to create accessible content. Current Web accessibility technologies are built on top of XMLbased technology infrastructures. In contrast, there is no foundation for investigating inside of Flash content, since it is distributed in a binary format. This characteristic has prevented vendors from developing Flash accessibility technologies. In order to address this issue, this paper proposes a method to transform existing Flash content into XML structures. It combines two approaches for accessing the internal structures. One approach is to obtain MSAA output through the Flash Player and the other is to acquire information by injecting <i>ActionScript bridge</i> code into the content. In this paper, we will first give an overview of the accessibility framework for Flash content, and then present our XML transformation and checking method. A prototype of the checker has been implemented, and some preliminary results of accessibility evaluations are discussed.","Shin Saito, Hironobu Takagi, Chieko Asakawa","accessibility, accessibility checker, blind, flash, visually impaired",157,164
10.1145/1168987.1169016,ASSETS,2006,Analyzing visual layout for a non-visual presentation-document interface, ,"Presentation documents play important roles in many fields, such as business and education. The principal purpose of presentation documents is to convey information visually, so recognizing the visual layout is essential for understanding those documents. However it is inherently difficult for the blind people to recognize a visual layout, because there are numerous types of charts in presentation documents. As the first step to solve such problems, this study focuses on diagrams in which objects or groups of objects are bound by arrows. Such diagrams usually show relationships among the objects. If such relationships could be recognized by screen readers, it would make them accessible. However, the presentation authoring applications do not have functions for embedding these relationships among objects. Therefore this paper proposes a visual analysis method for diagram structure in presentation documents to automatically create metadata. It generates metadata which describes the relationships of objects, and the source-destination relationships of arrows. Then a novel interface utilizing the metadata was prototyped to present the visual structure of presentation documents in a tree view. This allows blind users to understand presentation documents easily, because it represents the visual structure that current screen readers cannot expose. In addition, they are familiar with the tree view interface, so they can use it without training. Finally, an evaluation shows that our method for automatically creating the metadata can be applied to various types of diagrams in presentation documents.","Tatsuya Ishihara, Hironobu Takagi, Takashi Itoh, Chieko Asakawa","alternative interface, diagram, metadata, visual analysis",165,172
10.1145/1168987.1169017,ASSETS,2006,Learning and perceiving colors haptically, ,"Color is an integral part of spatial perception and there is a need to develop systems that render color information accessible to blind individuals. A novel system that allows learning, presentation and analysis of color information, designed in consultations with focus groups of individuals who are blind is proposed. Our system is based on a methodology that renders colors as textures through a haptic device. The aim of the proposed approach is to enable color perception and provide a basis for assessing color similarity. Initial testing of the system shows that both blind individuals and sighted individuals can recognize colors through our approach and further assess similarity between colors through the system. A space was obtained through multidimensional scaling performed on similarity scores between pairs of colors as presented through our system. This space obtained high congruency with the chromaticity diagram and the hue saturation color wheel which shows the validity of our system to allow color visualization. A realtime system based on the proposed mapping is designed to allow realtime color perception.","Kanav Kahol, Jamieson French, Laura Bratton, Sethuraman Panchanathan","color perception, haptic user interfaces",173,180
10.1145/1168987.1169018,ASSETS,2006,WebInSight:,making web images accessible,"Images without alternative text are a barrier to equal web access for blind users. To illustrate the problem, we conducted a series of studies that conclusively show that a large fraction of significant images have no alternative text. To ameliorate this problem, we introduce WebInSight, a system that automatically creates and inserts alternative text into web pages on-the-fly. To formulate alternative text for images, we present three labeling modules based on web context analysis, enhanced optical character recognition (OCR) and human labeling. The system caches alternative text in a local database and can add new labels seamlessly after a web page is downloaded, resulting in minimal impact to the browsing experience.","Jeffrey Bigham, Ryan Kaminsky, Richard Ladner, Oscar Danielsson, Gordon Hempton","optical character recognition, transformation proxy, web accessibility, web studies",181,188
10.1145/1168987.1169020,ASSETS,2006,Improvements in vision-based pointer control, ,"Vision-based head trackers have been around for some years and are even beginning to be commercialized, but problems remain with respect to usability. Users without the ability to use traditional pointing devices - the intended audience of such systems - have no alternative if the automatic boot strapping process fails, there is room for improvement in face tracking, and the pointer movement dynamics do not support accurate and efficient pointing. This paper describes a novel head tracking pointer that addresses these problems.",Rick Kjeldsen,"accessible user interfaces, human-computer interaction, vision-based user interfaces",189,196
10.1145/1168987.1169021,ASSETS,2006,The vocal joystick:,evaluation of voice-based cursor control techniques,"Mouse control has become a crucial aspect of many modern day computer interactions. This poses a challenge for individuals with motor impairments or those whose use of hands are restricted due to situational constraints. We present a system called the Vocal Joystick which allows the user to continuously control the mouse cursor by varying vocal parameters such as vowel quality, loudness and pitch. A survey of existing cursor control methods is presented to highlight the key characteristics of the Vocal Joystick. Evaluations were conducted to characterize expert performance capability of the Vocal Joystick, and to compare novice user performance and preference for the Vocal Joystick and two other existing speech based cursor control methods. Our results show that Fitts' law is a good predictor of the speedaccuracy tradeoff for the Vocal Joystick, and suggests that the optimal performance of the Vocal Joystick may be comparable to that of a conventional hand-operated joystick. Novice user evaluations show that the Vocal Joystick can be used by people without extensive training, and that it presents a viable alternative to existing speech-based cursor control methods.","Susumu Harada, James Landay, Jonathan Malkin, Xiao Li, Jeff Bilmes","Fitts' law, continuous input, cursor control, speech recognition, voice-based interface",197,204
10.1145/1168987.1169022,ASSETS,2006,A voice-activated syntax-directed editor for manually disabled programmers, ,"This paper discusses a research project targeted at the design and implementation of an interface intended to allow manually disabled people to more easily perform the task of programming. It proposes a Speech User Interface (SUI) targeted for this task. Voice was selected as the means of input as an alternative to the keyboard and mouse. Traditional programming IDEs tend to be character and line oriented. It is argued that this orientation is not conducive to voice input, and so a syntaxdirected programming interface is proposed. To test the viability of this combination of voice with a syntax-directed approach, an editor named VASDE (Voice-Activated Syntax-Directed Editor) was implemented using ECLIPSE as the underlying platform for development. This paper describes the syntax-directed interface, VASDE, and some of the lessons learned from initial usability studies.","Thomas Hubbell, David Langan, Thomas Hain","IDE, programming by voice, speech user interface, syntax directed",205,212
10.1145/1168987.1169023,ASSETS,2006,Non-speech input and speech recognition for real-time control of computer games, ,"This paper reports a comparison of user performance (time and accuracy) when controlling a popular arcade game of Tetris using speech recognition or non-speech (humming) input techniques. The preliminary qualitative study with seven participants shows that users were able to control the game using both methods but required more training and feedback for the humming control. The revised interface, which implemented these requirements, was positively responded by users. The quantitative test with 12 other participants shows that humming excelled in both time and accuracy, especially over longer distances and advanced difficulty levels.","Adam Sporka, Sri Kurniawan, Murni Mahmud, Pavel Slav&#237;k","acoustic input, game control, motor-impaired users, non-speech control, speech recognition, voice interaction",213,220
10.1145/1296843.1296847,ASSETS,2007,A comparison of area pointing and goal crossing for people with and without motor impairments, ,"Prior work has highlighted the challenges faced by people with motor impairments when trying to acquire on-screen targets using a mouse or trackball. Two reasons for this are the difficulty of positioning the mouse cursor within a confined area, and the challenge of accurately executing a click. We hypothesize that both of these difficulties with area pointing may be alleviated in a different target acquisition paradigm called ""goal crossing."" In goal crossing, users do not acquire a confined area, but instead pass over a target line. Although goal crossing has been studied for able-bodied users, its suitability for people with motor impairments is unknown. We present a study of 16 people, 8 of whom had motor impairments, using mice and trackballs to do area pointing and goal crossing. Our results indicate that Fitts' law models both techniques for both user groups. Furthermore, although throughput for able-bodied users was higher for area pointing than for goal crossing (4.72 vs. 3.61 bits/s), the opposite was true for users with motor impairments (2.34 vs. 2.88 bits/s), suggesting that goal crossing may be viable for them. However, error rates were higher for goal crossing than for area pointing under a strict definition of crossing errors (6.23% vs. 1.94%). Subjective results indicate a preference for goal crossing among motor-impaired users. This work provides the empirical foundation from which to pursue the design of crossing-based interfaces as accessible alternatives to pointing-based interfaces.","Jacob Wobbrock, Krzysztof Gajos","Fitts' law, area pointing, goal crossing, mouse, steering law, target acquisition, throughput, trackball",3,10
10.1145/1296843.1296848,ASSETS,2007,Slipping and drifting,using older users to uncover pen-based target acquisition difficulties,"This paper presents the results of a study to gather information on the underlying causes of pen -based target acquisition difficulty. In order to observe both simple and complex interaction, two tasks (menu and Fitts' tapping) were used. Thirty-six participants across three age groups (18-54, 54-69, and 70-85) were included to draw out both general shortcomings of targeting, and those difficulties unique to older users. Three primary sources of target acquisition difficulty were identified: slipping off the target, drifting unexpectedly from one menu to the next, and missing a menu selection by selecting the top edge of the item below. Based on these difficulties, we then evolved several designs for improving pen-based target acquisition. An additional finding was that including older users as participants allowed us to uncover pen-interaction deficiencies that we would likely have missed otherwise.","Karyn Moffatt, Joanna McGrenere","inclusive design, older users, pen-based interaction, tablet PC, target acquisition, universal usability",11,18
10.1145/1296843.1296849,ASSETS,2007,Barrier pointing,using physical edges to assist target acquisition on mobile device touch screens,"Mobile phones and personal digital assistants (PDAs) are incredibly popular pervasive technologies. Many of these devices contain touch screens, which can present problems for users with motor impairments due to small targets and their reliance on tapping for target acquisition. In order to select a target, users must tap on the screen, an action which requires the precise motion of flying into a target and lifting without slipping. In this paper, we propose a new technique for target acquisition called <i>barrier pointing</i>, which leverages the elevated physical edges surrounding the screen to improve pointing accuracy. After designing a series of barrier pointing techniques, we conducted an initial study with 9 able bodied users and 9 users with motor impairments in order to discover the parameters that make barrier pointing successful. From this data, we offer an in-depth analysis of the performance of two motor impaired users for whom barrier pointing was especially beneficial. We show the importance of providing physical stability by allowing the stylus to press against the screen and its physical edge. We offer other design insights and lessons learned that can inform future attempts at leveraging the physical properties of mobile devices to improve accessibility.","Jon Froehlich, Jacob Wobbrock, Shaun Kane","PDAs, accessible interfaces, corners, edges, mobile phones, motor impairments, target acquisition, touch screens",19,26
10.1145/1296843.1296850,ASSETS,2007,Voicedraw,a hands-free voice-driven drawing application for people with motor impairments,"We present VoiceDraw, a voice-driven drawing application for people with motor impairments that provides a way to generate free-form drawings without needing manual interaction. VoiceDraw was designed and built to investigate the potential of the human voice as a modality to bring fluid, continuous direct manipulation interaction to users who lack the use of their hands. VoiceDraw also allows us to study the issues surrounding the design of a user interface optimized for non-speech voice-based interaction. We describe the features of the VoiceDraw application, our design process, including our user-centered design sessions with a 'voice painter', and offer lessons learned that could inform future voice-based design efforts. In particular, we offer insights for mapping human voice to continuous control.","Susumu Harada, Jacob Wobbrock, James Landay","computer art, continuous input, drawing, motor impairments, painting, speech recognition, voice-based user interfaces",27,34
10.1145/1296843.1296852,ASSETS,2007,Automatic accessibility transcoding for flash content, ,"It is not surprising that rich Internet content, such as Flash and DHTML, is some of the most pervasive content because of its visual attractiveness to the sighted majority. Such visually rich content has been causing severe accessibility problems, especially for people with visual disabilities. For Flash content, the kinds of accessibility information necessary for screen readers is not usually provided in the existing content. A typical example of such missing data is the lack of alternative text for buttons, hypertext links, widget roles, and so on. One of the major reasons is that the current accessibility framework of Flash content imposes a burden on content authors to make their content accessible. As a result, adding support for accessibility tends to be neglected, and screen reader users are left out of the richer Internet experiences. Therefore, we decided to develop an automatic accessibility transcoding system for Flash content to allow users to access a wider range of existing content, and to reduce the workload for content authors by using an automatic repair algorithm. It works as a client-side transcoding system based on the internal object model inside the Flash content. It adds and repairs accessibility information for existing Flash content, so screen readers can present more accessible information to users. Our experiment using the pilot system showed that 55% of the missing alternative texts for buttons in the tested websites could be added automatically.","Daisuke Sato, Hisashi Miyashita, Hironobu Takagi, Chieko Asakawa","accessibility, automatic repair, flash content, transcoding, visually impaired",35,42
10.1145/1296843.1296853,ASSETS,2007,SAMBA,a semi-automatic method for measuring barriers of accessibility,"Although they play an important role in any assessment procedure, web accessibility metrics are not yet well developed and studied. In addition, most metrics are geared towards conformance, and therefore are not well suited to answer questions whether the web site has critical barriers with respect to some user group. The paper addresses some open issues: how can accessibility be measured other than by conformance to certain guidelines? How can a metric merge results produced by accessibility evaluation tools and by expert reviewers? Does it consider error rates of the tool? How can a metric consider also severity of accessibility barriers? Can a metric tell us if a web site is more accessible for certain user groups rather than others?. The paper presents a new methodology and associated metric for measuring accessibility that efficiently combine expert reviews with automatic evaluation of web pages. Examples and data drawn from tests performed on 1500 web pages are also presented.","Giorgio Brajnik, Raffaella Lomuscio","accessibility, accessibility metric, evaluation method, quality assessment, web accessibility",43,50
10.1145/1296843.1296854,ASSETS,2007,WebinSitu,a comparative analysis of blind and sighted browsing behavior,"Web browsing is inefficient for blind web users because of persistent accessibility problems, but the extent of these problems and their practical effects from the perspective of the user has not been sufficiently examined. We conducted a study <i>in situ</i> to investigate the accessibility of the web as experienced by web users. This remote study used an advanced web proxy that leverages AJAX technology to record both the pages viewed and the actions taken by users on the web pages that they visited. Our study was conducted remotely over the period of one week, and our participants used the assistive technology and software to which they were already accustomed and had already configured according to preference. These advantages allowed us to aggregate observations of many users and to explore the practical effects on and coping strategies employed by our blind participants. Our study reflects web accessibility from the perspective of web users and describes quantitative differences in the browsing behavior of blind and sighted web users.","Jeffrey Bigham, Anna Cavender, Jeremy Brudvik, Jacob Wobbrock, Richard Ladner","blind users, web accessibility, web studies",51,58
10.1145/1296843.1296855,ASSETS,2007,Effects of sampling methods on web accessibility evaluations, ,"Except for trivial cases, any accessibility evaluation has to be based on some method for selecting pages to be analyzed. But this selection process may bias the evaluation. Up to know, not much is known about available selection methods, and about their effectiveness and efficiency. The paper addresses the following open issues: how to define the quality of the selection process, which processes are better than others, how to measure their difference in quality, which factors may affect quality (type of assessment, size of the page pool, structural features of the web site). These issues are investigated through an experimental evaluation of thirteen sampling methods applied to 32000 web pages. While some of the conclusions are not surprising (for example, that sample size affect accuracy), others were not expected at all (that minimal sampling size obtains a high accuracy level under certain circumstances).","Giorgio Brajnik, Andrea Mulas, Claudia Pitton","accessibility metric, sampling methods, web accessibility",59,66
10.1145/1296843.1296857,ASSETS,2007,Improving accessibility to statistical graphs,the iGraph-Lite system,"Information is often presented in graphical form. Unfortunately, current assistive technologies such as screen readers are not well-equipped to handle these representations. To provide accessibility to graphs published in ``The Daily"" (Statistics Canada's main dissemination venue), we have developed iGraph-Lite, a system that provides short verbal descriptions of the information depicted in graphs and a way to also interact with this information.","Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis Boucher, Antoine Chretien, Martin Lachance","accessibility (blind and visually impaired), natural language interaction, statistical graphs",67,74
10.1145/1296843.1296858,ASSETS,2007,Automated tactile graphics translation,in the field,"We address the practical problem of automating the process of translating figures from mathematics, science, and engineering textbooks to a tactile form suitable for blind students. The Tactile Graphics Assistant (TGA) and accompanying workflow is described. Components of the TGA that identify text and replace it with Braille use machine learning, computational geometry, and optimization algorithms. We followed through with the ideas in our 2005 paper by creating a more detailed workflow, translating actual images, and analyzing the translation time. Our experience in translating more than 2,300 figures from 4 textbooks demonstrates that figures can be translated in ten minutes or less of human time on average. We describe our experience with training tactile graphics specialists to use the new TGA technology.","Chandrika Jayant, Matt Renzelmann, Dana Wen, Satria Krisnandi, Richard Ladner, Dan Comden","accessibility, braille, disability, image processing, machine learning, tactile graphics, user study",75,82
10.1145/1296843.1296859,ASSETS,2007,Haptic comparison of size (relative magnitude) in blind and sighted people, ,"Applications for blind users often involve the mapping of information such as size (magnitude) from one sensory domain (vision) onto another (sound or touch). For example, visual perception of length can be estimated directly by touch, or encoded to pitch or even vibration. Applications for blind users will benefit from fundamental research into human perception of computer-generated substitutions for vision. In this paper we present the results of a haptics-only experiment with the PHANToM that measures human performance (time and accuracy) judging relative magnitude with computer generated haptic properties. Magnitude was represented by either physical length (displacement), or vibration varied by frequency or amplitude. Eleven blind and eleven blindfolded sighted individuals participated. Displacement tasks were 50% slower than vibration conditions for all participants. Accuracy for displacement and vibration varied by amplitude was equivalent. Vibration varied by frequency was significantly less accurate, although we are cautious about the reliability of those results. Blind participants took 50% longer with equivalent accuracy to sighted participants. Sightedness had no effect on performance regarding the type of display. No other interaction effects were found. These results suggest that vibration varied by amplitude provides a faster and equally accurate display of magnitude compared with the traditional displacement approach. Secondly, the same coding benefits equally well visually disabled and sighted individuals.","Sarah Douglas, Shasta Willson","accessibility, blind, haptic interaction, multi-modal, phantom, visually disabled, visually impaired",83,90
10.1145/1296843.1296860,ASSETS,2007,Aibrowser for multimedia,introducing multimedia content accessibility for visually impaired users,"Multimedia content with Rich Internet Applications using Dynamic HTML (DHTML) and Adobe Flash is now becoming popular in various websites. However, visually impaired users cannot deal with such content due to audio interference with the speech from screen readers and intricate structures strongly optimized for sighted users. We have been developing an Accessibility Internet Browserfor Multimedia (aiBrowser) to address these problems. Thebrowser has two novel features: non-visual multimedia audiocontrols and alternative user interfaces using externalmetadata. First, by using the aiBrowser, users can directlycontrol the audio from the embedded media with fixed shortcutkeys. Therefore, this allows blind users to increase ordecrease the media volume, and pause or stop the mediato handle conflicts between the audio of the media and thespeech from the screen reader. Second, the aiBrowser canprovide an alternative simplified user interface suitable forscreen readers by using external metadata, which can evenbe applied to dynamic content such as DHTML and Flash. In this paper, we discuss accessibility problems with multimedia content due to streaming media and the dynamic changes in such content, and explain how the aiBrowser addresses these problems by describing non-visual multimedia audio controls and external metadata-based alternative user interfaces. The evaluation of the aiBrowser was conducted by comparing it to JAWS, one of the most popular screen readers, on three well known multimedia-content-intensive websites. The evaluation showed that the aiBrowser made the contentthat was inaccessible with JAWS relatively accessibleby using the multimedia audio controls and alternative interfaceswith metadata which included alternative text, headinginformation, and so on. It also drastically reduced thekeystrokes for navigation with aiBrowser, which implies toimprove the non-visual usability.","Hisashi Miyashita, Daisuke Sato, Hironobu Takagi, Chieko Asakawa","DHTML, accessibility, flash, multimedia control, streaming media",91,98
10.1145/1296843.1296862,ASSETS,2007,Photonote evaluation,aiding students with disabilities in a lecture environment,"Visual material presented in lectures can be enhanced for students with disabilities by using high-resolution digital-still cameras. The Photonote system uses a digital-still camera to capture visual information, a digital-video camera to capture a lecturer and a second digital-video camera to capture a sign-language interpreter, if necessary. The visual information is enhanced using computer-vision algorithms and presented alongside the recorded video and audio to provide an accurate representation of a lecture which can be used by students with disabilities for review purposes. This paper presents the Photonote system and a user study evaluating its effectiveness at aiding students with disabilities in a lecture environment.","Gregory Hughes, Peter Robinson", ,99,106
10.1145/1296843.1296863,ASSETS,2007,Dual educational electronic textbooks,the starlight platform,"This paper presents a novel software platform for developing and interacting with multimodal interactive electronic textbooks that provide a Dual User Interface, i.e., an interface concurrently accessible by visually impaired and sighted persons. The platform, named Starlight, comprises two sub-systems: (a) the ""Writer"", facilitating the authoring of electronic textbooks, encompassing various categories of interactive exercises (Q&A, multiple choice, fill in the blanks, etc.); and (b) the ""Reader"", enabling multimodal interaction with the created electronic textbooks, supporting various features like searching, book-marking, replay of sentences / paragraphs, user annotations / comments, activity recording, and context-sensitive help. An iterative, user-centered design process was adopted, involving from the very early stages students and educators, resulting in the creation of eight textbooks for the primary and high school that are currently available in the Greek market. The paper discusses the competitive features of the Dual User Interface and of supplied functionality compared to existing accessible electronic books. It also consolidates the key design findings, elaborating on prominent design issues, design rational, and respective solutions, highlighting strengths and weaknesses, and outlining directions for future work.","Dimitris Grammenos, Anthony Savidis, Yannis Georgalis, Themistoklis Bourdenas, Constantine Stephanidis","blind students, braille display, electronic books, loosely coupled dual interaction, non-visual interaction, synthetic speech",107,114
10.1145/1296843.1296864,ASSETS,2007,A software model to support collaborative mathematical work between braille and sighted users, ,"In this paper we describe a software model that we have developed within the framework of the MaWEn project (Mathematical Working Environment). Based on the MathML standard, this model enables collaboration between sighted people and users of Braille. It allows for synchronisation of Braille and graphical views of scientific contents as well as offering improved navigational functions for Braille users, in both reading and editing modes. The UMCL (Universal Maths Conversion Library) is used to support various national Braille Mathematical notations. After presenting the model, its implementation in MaWEn prototypes is described.","Dominique Archambault, Bernhard St&#246;ger, Mario Batusic, Claudia Fahrengruber, Klaus Miesenberger","braille, collaborative work, conversion, inclusive education, mathematics",115,122
10.1145/1296843.1296865,ASSETS,2007,Techniques to assist in developing accessibility engineers, ,This paper describes techniques used in a recent computer science course designed to develop accessibility engineers. It provides sufficient detail for other instructors to replicate the highly successful experience that resulted. It also discuses a number of results of the course that act as indicators of its success.,"Jim Carter, David Fourney","accessibility, assistive technology, universal access, universal access reference model, usability, user-system model",123,130
10.1145/1296843.1296867,ASSETS,2007,Providing good memory cues for people with episodic memory impairment, ,"Alzheimer's disease impairs episodic memory and subtly and progressively robs people of their ability to remember their recent experiences. In this paper, we describe two studies that lead to a better understanding of how caregivers use cues to support episodic memory impairment and what types of cues are best for supporting recollection. We also show how good memory cues differ between people with and without episodic memory impairment. We discuss how this improved understanding impacts the design of lifelogging technologies for automatically capturing and extracting the best memory cues to assist overburdened caregivers and people with episodic memory impairment in supporting recollection of episodic memory.","Matthew Lee, Anind Dey","alzheimer's disease, caregiver burden, cues, elders, episodic memory",131,138
10.1145/1296843.1296868,ASSETS,2007,Data visualisation and data mining technology for supporting care for older people, ,"The overall purpose of the research discussed here is the enhancement of home-based care by revealing individual patterns in the life of a person, through modelling of the ""busyness"" of activity in their dwelling, so that care can be better tailored to their needs and changing circumstances. The use of data mining and on-line analytical processing (OLAP) is potentially interesting in this context because of the possibility of exploring, detecting and predicting changes in the level of activity of people's movement that may reflect change in well-being. An investigation is presented here into the use of data mining and visualisation to illustrate activity from sensor data from a trial project run in a domestic context.","Nubia Gil, Nicolas Hine, John Arnott, Julienne Hanson, Richard Curry, Telmo Amaral, Dorota Osipovic","assistive technology, data mining, independent living, older adults, visualisation",139,146
10.1145/1296843.1296869,ASSETS,2007,An operantly conditioned looking task for assessing infant auditory processing ability, ,"In this paper, we describe the design and evaluation of a gaze-driven interface for the assessment of rapid auditory processing abilities in infants aged 4 to 6 months. A cross-modal operant conditioning procedure is used to reinforce anticipatory eye movements in response to changes in a continuous auditory stream. Using this procedure, we hope to develop a clinical tool that will enable early identification of individuals at risk for language-based learning impairments. Some of the unique opportunities and challenges inherent to designing for infant-computer interaction are discussed.","Jason Nawyn, Cynthia Roesler, Teresa Realpe-Bonilla, Naseem Choudhury, April Benasich","early assessment, eye tracking, infant-computer interaction, language impairment, operant conditioning",147,154
10.1145/1296843.1296871,ASSETS,2007,"Using participatory activities with seniors to critique, build, and evaluate mobile phones", ,"Mobile phones can provide a number of benefits to older people. However, most mobile phone designs and form factors are targeted at younger people and middle-aged adults. To inform the design of mobile phones for seniors, we ran several participatory activities where seniors critiqued current mobile phones, chose important applications, and built their own imagined mobile phone system. We prototyped this system on a real mobile phone and evaluated the seniors' performance through user tests and a real-world deployment. We found that our participants wanted more than simple phone functions, and instead wanted a variety of application areas. While they were able to learn to use the software with little difficulty, hardware design made completing some tasks frustrating or difficult. Based on our experience with our participants, we offer considerations for the community about how to design mobile devices for seniors and how to engage them in participatory activities.","Michael Massimi, Ronald Baecker, Michael Wu","accessibility, elderly, guidelines, mobile devices, mobile phones, participatory design, scenarios, seniors, usability",155,162
10.1145/1296843.1296872,ASSETS,2007,Variable frame rate for low power mobile sign language communication, ,"The MobileASL project aims to increase accessibility by enabling Deaf people to communicate over video cell phones in their native language, American Sign Language (ASL). Real-time video over cell phones can be a computationally intensive task that quickly drains the battery, rendering the cell phone useless. Properties of conversational sign language allow us to save power and bits: namely, lower frame rates are possible when one person is not signing due to turn-taking, and signing can potentially employ a lower frame rate than fingerspelling. We conduct a user study with native signers to examine the intelligibility of varying the frame rate based on activity in the video. We then describe several methods for automatically determining the activity of signing or not signing from the video stream in real-time. Our results show that varying the frame rate during turn-taking is a good way to save power without sacrificing intelligibility, and that automatic activity analysis is feasible.","Neva Cherniavsky, Anna Cavender, Richard Ladner, Eve Riskin","activity analysis, deaf community, low power, mobile telephone use, sign language",163,170
10.1145/1296843.1296873,ASSETS,2007,Observing Sara,a case study of a blind person's interactions with technology,"While software is increasingly being improved to enhance access and use, software interfaces nonetheless often create barriers for people who are blind. In response, the blind computer user develops <i>workarounds</i>, strategies to overcome the constraints of a physical and social world engineered for the sighted. This paper describes an interview and observational study of a blind college student interacting with various technologies within her home. Structured around Blythe, Monk and Park's <i>Technology Biographies</i>, these experience centered sessions focus not only on technology function, but on the relationship of function to the meanings and values that this student attributes to technology use in different settings. Studying a single user across a range of devices and tasks provides a broader and more nuanced understanding of the contexts and causes of task failure and of the workarounds employed than is possible with a more narrowly focused usability study. Themes that were revealed across a range of tasks include the importance for technologies to not ""mark"" the user as being blind within a predominantly sighted social world, to support user independence through portability and user control, and to allow user ""resets"" and brute-force fallbacks in the face of persistent task failure.","Kristen Shinohara, Josh Tenenberg","assistive technology, technology biographies, user-centered inclusive design",171,178
10.1145/1296843.1296874,ASSETS,2007,Understanding mobile phone requirements for young adults with cognitive disabilities, ,"Mobile phones have transformed the way we communicate with friends and family, coordinate our daily activities, and organize our lives. For families with children with cognitive disabilities there is widespread hope, though not always fulfilled, that personal technologies - particularly mobile phones - can bring a dramatic increase in their children's level of safety, independence, and social connectedness. In this research, we conducted semi-structured interviews with five families to understand the current patterns of remote communication among young adults with cognitive disabilities and their parental caregivers, and the role that remote communication played in increasing independence and safety. While some of the young adults used mobile phones and some did not, we identified common themes in requirements, patterns of use, and desires for an accessible mobile-phone based remote communication system. Requirements include the need for a simplified navigation menu with fewer options and a rugged handset and charger input. Families used mobile phones for safety check-ins and help getting <i>un-stuck</i>. While parents desired increased social involvement for their children, they observed that their children did not often chat with friends on the phone.",Melissa Dawe,"assistive technology, cognitive disabilities, design requirements, ethnography, mobile phones, semi-structured interviews",179,186
10.1145/1296843.1296876,ASSETS,2007,The design and field evaluation of PhotoTalk,a digital image communication application for people,"Talk is an application for a mobile device that allows people with aphasia to capture and manage digital photographs to support face-to-face communication. Unlike any other augmentative and alternative communication device for people with aphasia, PhotoTalk focuses <i>solely</i> on image capture and organization and is designed to be used independently. Our project used a streamlined process with 3 phases: (1) a rapid participatory design and development phase with two speech-language pathologists acting as representative users, (2) an informal usability study with 5 aphasic participants, which caught usability problems and provided preliminary feedback on the usefulness of PhotoTalk, and (3) a 1 month field evaluation with 2 aphasic participants, which showed that both used it regularly and fairly independently, although not always for its intended communicative purpose. Our field study demonstrated PhotoTalk's promise in terms of its usability and usefulness in real life situations.","Meghan Allen, Joanna McGrenere, Barbara Purves","AAC devices, aphasia, cognitive disability, evaluation, field study, mobile technology, participatory design",187,194
10.1145/1296843.1296877,ASSETS,2007,Corpus studies in word prediction, ,"Word prediction can be used to enhance the communication rate of people with disabilities who use Augmentative and Alternative Communication (AAC) devices. We use statistical methods in a word prediction system, which are trained on a corpus, and then measure the efficacy of the resulting system by calculating the theoretical keystroke savings on some held out data. Ideally training and testing should be done on a large corpus of AAC text covering a variety of topics, but no such corpus exists. We discuss training and testing on a wide variety of corpora meant to approximate text from AAC users. We show that training on a combination of in-domain data with out-of-domain data is often more beneficial than either data set alone and that advanced language modeling such as topic modeling is portable even when applied to very different text.","Keith Trnka, Kathleen McCoy","corpora, language modeling, statistical methods, word prediction",195,202
10.1145/1296843.1296878,ASSETS,2007,SIBYLLE,a system for alternative communication adapting to the context and its user,"In this paper, we describe the latest version of <i>SIBYLLE</i>, an AAC system that permits persons suffering from severe physical disabilities to enter text with any computer application and also to compose messages to be read out by a speech synthesis module. The system consists of a virtual keyboard comprising a set of keypads which allow entering characters or full words by a single-switch selection process. It also comprises a sophisticated word prediction component which dynamically calculates the most appropriate words for a given context. This component is auto-adaptive, i.e. it learns on every text the user has entered. It thus adapts its predictions to the user's language and the current topic of communication as well. So far the system works for French, German and English. Earlier versions of <i>SIBYLLE</i> have been used since 2001 in the Kerpape rehabilitation center (Brittany, France).","Tonio WANDMACHER, Jean-Yves ANTOINE, Franck POIRIER","augmentative and alternative communication, keystroke saving rate, user adaptation, virtual keyboard, word prediction latent semantic analysis",203,210
10.1145/1296843.1296879,ASSETS,2007,Evaluating American Sign Language generation through the participation of native ASL signers, ,"We discuss important factors in the design of evaluation studies for systems that generate animations of American Sign Language (ASL) sentences. In particular, we outline how some cultural and linguistic characteristics of members of the American Deaf community must be taken into account so as to ensure the accuracy of evaluations involving these users. Finally, we describe our implementation and user-based evaluation (by native ASL signers) of a prototype ASL generator to produce sentences containing classifier predicates, frequent and complex spatial phenomena that previous ASL generators have not produced.","Matt Huenerfauth, Liming Zhao, Erdan Gu, Jan Allbeck","accessibility technology for the deaf, american sign language, animation evaluation, natural language generation",211,218
10.1145/1414471.1414475,ASSETS,2008,Software and technologies designed for people with autism,what do users want?,"Software developers, designers and researchers have been looking to technology for solutions to help and educate people with autism for over two decades. There are many examples of seemingly successful technology-based products and prototypes, yet very little is known about how well these solutions are currently integrated into lives of children and adults with autism and their families. This paper reports on results from an anonymous on-line survey intended as a first step to elucidate information about software and technology use. Additionally, data was analyzed to aid creation of future technology-based products for people with autism that are not just effective, but that also meet important user goals and align to their interests and strengths. Major findings included: (1) very few respondents (25\%) had any experience with software or technology designed for people with cognitive disabilities; (2) when asked an open-ended question about what they desire in technology design, respondents reported three major goals (social skills, academic skills, and organization skills), and many suggestions for improvements to software and hardware design; and (3) technology was reported as both a major strength and interest for people with autism.","Cynthia Putnam, Lorna Chong","autism, software and technology design, user-centered design",3,10
10.1145/1414471.1414476,ASSETS,2008,A3,a coding guideline for HCI+autism research using video annotation,"Due to the profile of strengths and weaknesses indicative of autism spectrum disorders (ASD), technology may play a key role in ameliorating communication difficulties with this population. This paper documents coding guidelines established through cross-disciplinary work focused on facilitating communication development in children with ASD using computerized feedback. The guidelines, referred to as A3 (pronounced A-Cubed) or Annotation for ASD Analysis, define and operationalize a set of dependent variables coded via video annotation. Inter-rater reliability data are also presented from a study currently in-progress, as well as related discussion to help guide future work in this area. The design of the A3 methodology is well-suited for the examination and evaluation of the behavior of low-functioning subjects with ASD who interact with technology.","Joshua Hailpern, Karrie Karahalios, James Halle, Laura DeThorne, Mary-Kelsey Coletto","ASD, annotation, audio feedback, autism, coding, guidline, kappa, point-by-point agreement, reliability, video, visualization",11,18
10.1145/1414471.1414477,ASSETS,2008,Technology for just-in-time in-situ learning of facial affect for persons diagnosed with an autism spectrum disorder, ,"Many first-hand accounts from individuals diagnosed with autism spectrum disorders (ASD) highlight the challenges inherent in processing high-speed, complex, and unpredictable social information such as facial expressions in real-time. In this paper, we describe a new technology aimed at helping people capture, analyze, and reflect on a set of social-emotional signals communicated by facial and head movements in live social interaction that occurs with their everyday social companions. We describe our development of a new combination of hardware using a miniature camera connected to an ultramobile PC together with custom software developed to track, capture, interpret, and intuitively present various interpretations of the facial-head movements (e.g., presenting that there is a high probability the person looks ""confused""). This paper describes this new technology together with the results of a series of pilot studies conducted with adolescents diagnosed with ASD who used the technology in their peer-group setting and contributed to its development via their feedback.","Miriam Madsen, Rana el Kaliouby, Matthew Goodwin, Rosalind Picard","affective computing, asperger's syndrome, autism spectrum disorders, facial affect, facial expressions",19,26
10.1145/1414471.1414479,ASSETS,2008,A context aware handheld wayfinding system for individuals with cognitive impairments, ,"A challenge to individuals with cognitive impairments in wayfinding is how to remain oriented, recall routines, and travel in unfamiliar areas in a way relying on limited cognitive capacity. According to psychological model of spatial navigation and the requirements of rehabilitation professionals, a novel wayfinding system is presented with an aim to increase workplace and life independence for people suffering from diseases such as traumatic brain injury, cerebral palsy, mental retardation, schizophrenia, Down syndromes, and Alzheimer's disease. This paper describes an approach to providing distributed cognition support of travel guidance for persons with cognitive disabilities. The unique strength of the system is the ability to provide unique-to-the-user prompts that are triggered by context. As this population is very sensitive to issues of abstraction (e.g. icons) and presents the designer with the need to tailor prompts to a 'universe-of-one' the use of images specific to each user and context is implemented. The key to the approach is to spread the context awareness across the system, with the context being flagged by the QR-code tags and the appropriate response being evoked by displaying the appropriate path guidance images indexed by the intersection of specific end-user and context ID embedded in the tags. By separating the context trigger from the pictorial response, responses can be updated independently of the rest of the installed system, and a single QR-code tag can trigger multiple responses in the PDA depending on the end-user and their specific path. A prototype is built and tested in field experiments with real patients. The experimental results show the human-computer interface is friendly and the capabilities of wayfinding are reliable.","Yao-Jen Chang, Shih-Kai Tsai, Tsen-Yung Wang","cognitive disability, qr code, ubiquitous computing, user interface",27,34
10.1145/1414471.1414480,ASSETS,2008,Computer usage by young individuals with down syndrome,an exploratory study,"In this paper, we discuss the results of an online survey that investigates how children and young adults with Down syndrome use computers and computer-related devices. The survey responses cover 561 individuals with Down syndrome between the age of four to 21. The survey results suggest that the majority of the children and young adults with Down syndrome can use the mouse to interact with computers, which requires spatial, cognitive, and fine motor skills that were previously believed to be quite challenging for individuals with Down syndrome. The results show great difficulty in text entry using keyboards. Young individuals with Down syndrome are using a variety of computer applications and computer related devices, and computers and computer-related devices play important roles in the life of individuals with Down syndrome. There appears to be great potential in computer-related education and training to broaden existing career opportunities for individuals with Down syndrome, and there needs to be further research on this topic.","J. Feng, J. Lazar, L. Kumin, A. Ozok","computer use, down syndrome, human factors, human-computer interaction, kids, usability, young individuals",35,42
10.1145/1414471.1414481,ASSETS,2008,Understanding pointing problems in real world computing environments, ,"Understanding how pointing performance varies in real world computer use and over time can provide valuable insight about how systems should accommodate changes in pointing behavior. Unfortunately, pointing data from individuals with pointing problems is rarely studied during real world use. Instead, it is most frequently evaluated in a laboratory where it is easier to collect and evaluate data. We developed a technique to collect and analyze real world pointing performance which we used to investigate the variance in performance of six individuals with a range of pointing abilities. Features of pointing performance we analyzed include metrics such as movement trajectories, clicking, and double clicking. These individuals exhibited high variance during both supervised and unsupervised (or real world) computer use across multiple login sessions. The high variance found within each participant highlights the potential inaccuracy of judging performance based on a single laboratory session.","Amy Hurst, Jennifer Mankoff, Scott Hudson","data collection in the wild, motor impairments, pointing performance",43,50
10.1145/1414471.1414483,ASSETS,2008,Hover or tap?,supporting pen-based menu navigation for older adults,"Tablet PCs are gaining popularity, but many users, particularly older ones, still struggle with pen-based interaction. One type of error, drifting, occurs when users accidentally hover over an adjacent menu, causing their focus menu to close and the adjacent one to open. In this paper, we propose two approaches to address drifting. The first, tap, requires an explicit tap to switch menus, and thus, eliminates the possibility of a drift. The second, glide, uses a distance threshold to delay switching, and thereby reduce the likelihood of a drift. We performed a comparative evaluation of our approaches with a control interface. Tap was effective at reducing drifts for both groups, but it was only popular among older users. Glide surprisingly did not show any performance improvement. Additional research is needed to determine if the negative findings for glide are a result of the particular threshold used, or reflect a fundamental flaw in the glide approach.","Karyn Moffatt, Sandra Yuen, Joanna McGrenere","inclusive design, older users, pen-based interaction, tablet pc, target acquisition, universal usability",51,58
10.1145/1414471.1414484,ASSETS,2008,Technology devices for older adults to aid self management of chronic health conditions, ,"The overall purpose of this study is the enhancement of devices and visualisations used by older adults as part of a telecare system for the self-management of health conditions. The opinions and feelings towards devices that could be used as part of a telecare system were gathered from a range of older people. This was done through the use of technology evaluation workshops, and the subsequent analysis of the collected data using grounded theory and thematic coding methodologies. Presenting healthcare data to an elderly person with chronic health issues, may be an appropriate way to help that person to better manage their condition, if the data can be understood.","Amritpal Bhachu, Nicolas Hine, John Arnott","assistive care, older adults, self management of health, telecare, visualisations",59,66
10.1145/1414471.1414485,ASSETS,2008,How older and younger adults differ in their approach to problem solving on a complex website, ,"Older adults differ from younger ones in the ways they experience the World Wide Web. For example, they tend to move from page to page more slowly, take more time to complete tasks, make more repeated visits to pages, and take more time to select link targets than their younger counterparts. These differences are consistent with the physical and cognitive declines associated with aging. The picture that emerges has older adults doing the same sorts of things with websites as younger adults, although less efficiently, less accurately and more slowly. This paper questions that view. We present new findings that show that, to accomplish their purposes, older adults may systematically undertake different activities and use different parts of websites than younger adults. We examined how a group of adults 18 to 73 years of age moved through a complex website seeking to solve a specific problem. We found that the users exhibited strong age--related tendencies to follow particular paths and visit particular zones while in pursuit of a common goal. We also assessed how experience with the web may mediate these tendencies. We conclude the paper with a discussion of the implications of the finding that users' characteristics not only affect how they navigate but what activities they undertake along the way.",Peter Fairweather,"aging, hypertext, world wide web",67,72
10.1145/1414471.1414487,ASSETS,2008,Slide rule,making mobile touch screens accessible to blind people using multi-touch interaction techniques,"Recent advances in touch screen technology have increased the prevalence of touch screens and have prompted a wave of new touch screen-based devices. However, touch screens are still largely inaccessible to blind users, who must adopt error-prone compensatory strategies to use them or find accessible alternatives. This inaccessibility is due to interaction techniques that require the user to visually locate objects on the screen. To address this problem, we introduce Slide Rule, a set of audio-based multi-touch interaction techniques that enable blind users to access touch screen applications. We describe the design of Slide Rule, our interaction techniques, and a user study in which 10 blind people used Slide Rule and a button-based Pocket PC screen reader. Results show that Slide Rule was significantly faster than the button-based system, and was preferred by 7 of 10 users. However, users made more errors when using Slide Rule than when using the more familiar button-based system.","Shaun Kane, Jeffrey Bigham, Jacob Wobbrock","accessibility, blindness, mobile devices, multi-touch interaction techniques, speech output, touch screens",73,80
10.1145/1414471.1414488,ASSETS,2008,Note-taker,enabling students who are legally blind to take notes in class,"The act of note-taking is a key component of learning in secondary and post-secondary classrooms. Students who take notes retain information from classroom lectures better, even if they never refer to those notes afterward. However, students who are legally blind, and who wish to take notes in their classrooms are at a disadvantage. Simply equipping classrooms with lecture recording systems does not substitute for note taking, since it does not actively engage the student in note-taking during the lecture. In this paper we detail the problems encountered by one math and computer science student who is legally blind, and we present our proposed solution: the CUbiC Note-Taker, which is a highly portable device that requires no prior classroom setup, and does not require lecturers to adapt their presentations. We also present results from two case studies of the Note-Taker, totaling more than 200 hours of in-class use.","David Hayden, Dirk Colbry, John Black, Sethuraman Panchanathan","assistive note-taking, automatic note-taking, blind, lecture, lecture notes, low-vision, meeting, note-taker, note-taking, notes, student",81,88
10.1145/1414471.1414489,ASSETS,2008,Refreshable tactile graphics applied to schoolbook illustrations for students with visual impairment, ,"This article presents research on making schoolbook illustrations accessible for students with visual impairment. The MaskGen system was developed to interactively transpose illustrations of schoolbooks into tactile graphics. A methodology was designed to transpose the graphics and prepare them to be displayed on the STReSS2, a refreshable tactile device. We experimented different associations of tactile rendering and audio feedbacks to find a model that children with visual impairment could use. We experimented with three scientific graphics (diagram, bar-chart and map) with forty participants: twenty sighted adults, ten adults with visual impairment, and ten children with visual impairment. Results show that the participants with visual impairment liked the tactile graphics and could use them to explore illustrations and answer questions about their content.","Gr&#233;gory Petit, Aude Dufresne, Vincent Levesque, Vincent Hayward, Nicole Trudeau","accessibility, children with special needs, multimodal device, pedagogy, students with visual impairment, tactile graphics",89,96
10.1145/1414471.1414491,ASSETS,2008,Constructing relational diagrams in audio,the multiple perspective hierarchical approach,"Although research on non-visual access to visually represented information is steadily growing, very little work has investigated how such forms of representation could be constructed through non-visual means. We discuss in this paper our approach for providing audio access to relational diagrams using multiple perspective hierarchies, and describe the design of two interaction strategies for constructing and manipulating such diagrams through this approach. A comparative study that we conducted with sighted users showed that a non-guided strategy allowed for significantly faster interaction times, and that both strategies supported similar levels of diagram comprehension. Overall, the reported study revealed that using multiple perspective hierarchies to structure the information encoded in a relational diagram enabled users construct and manipulate such information through an audio-only interface, and that combining aspects from the guided and the non-guided strategies could support greater usability.","Oussama Metatla, Nick Bryan-Kinns, Tony Stockman","accessibility, auditory display, diagram construction, hierarchies, interaction strategies, multiple perspectives, non-visual representations",97,104
10.1145/1414471.1414492,ASSETS,2008,Advanced auditory menus,design and evaluation of auditory scroll bars,"Auditory menus have the potential to make devices that use visual menus accessible to a wide range of users. Visually impaired users could especially benefit from the auditory feedback received during menu navigation. However, auditory menus are a relatively new concept, and there are very few guidelines that describe how to design them. This paper details how visual menu concepts may be applied to auditory menus in order to help develop design guidelines. Specifically, this set of studies examined possible ways of designing an auditory scrollbar for an auditory menu. The following different auditory scrollbar designs were evaluated: single-tone, double-tone, alphabetical grouping, and proportional grouping. Three different evaluations were conducted to determine the best design. The first two evaluations were conducted with sighted users, and the last evaluation was conducted with visually impaired users. The results suggest that pitch polarity does not matter, and proportional grouping is the best of the auditory scrollbar designs evaluated here.","Pavani Yalla, Bruce Walker","auditory menus, auditory scrollbar, non-speech sounds, universal design",105,112
10.1145/1414471.1414494,ASSETS,2008,A comparative test of web accessibility evaluation methods, ,"Accessibility auditors have to choose a method when evaluating accessibility: expert review (a.k.a. conformance testing), user testing, subjective evaluations, barrier walkthrough are some possibilities. However, little is known to date about their relative strengths and weaknesses. Furthermore, what happened for usability evaluation methods is likely to repeat for accessibility: that there is uncertainty about not only pros and cons of methods, but also about criteria to be used to compare them and metrics to measure these criteria. After a quick review and description of methods, the paper illustrates a comparative test of two web accessibility evaluation methods: conformance testing and barrier walkthrough. The comparison aims at determining merits of barrier walkthrough, using conformance testing as a control condition. A comparison framework is outlined, followed by the description of a laboratory experiment with 12 subjects (novice accessibility evaluators), and its results. Significant differences were found in terms of correctness, one of the several metrics used to compare the methods. Reliability also appears to be different.",Giorgio Brajnik,"accessibility evaluation method, quality assessment, web accessibility",113,120
10.1145/1414471.1414495,ASSETS,2008,Investigating sighted users' browsing behaviour to assist web accessibility, ,"The rapid advancement of World Wide Web (Web) technology and constant need for attractive Websites produce pages that hinder visually impaired users. We assert that understanding how sighted users browse Web pages can provide important information that will enhance Web Accessibility, especially for visually impaired users. We present an eye tracking study where sighted users' browsing behaviour on nine Web pages was investigated to determine how the page's visual clutter is related to sighted users' browsing patterns. The results show that salient elements attract users' attention first, users spend more time on the main content of the page and users tend to fixate on the first three or four items on the menu lists. Common gaze patterns begin at the salient elements of the page, move to the main content, header, right column and left column of the page and finish at the footer area. We argue that the results should be used as the initial step for proposing guidelines that assist in designing and transforming Web pages for an easier and faster access for visually impaired users.","Eleni Michailidou, Simon Harper, Sean Bechhofer","eye tracking, visual impairments, visual perception, web accessibility",121,128
10.1145/1414471.1414496,ASSETS,2008,Evaluation of a psycholinguistically motivated timing model for animations of american sign language, ,"Using results in the psycholinguistics literature on the speed and timing of American Sign Language (ASL), we built algorithms to calculate the time-duration of signs and the location/length of pauses during an ASL animation. We conducted a study in which native ASL signers evaluated the ASL animations processed by our algorithms, and we found that: (1) adding linguistically motivated pauses and variations in sign-durations improved signers' performance on a comprehension task and (2) these animations were rated as more understandable by ASL signers.",Matt Huenerfauth,"accessibility technology for the deaf, american sign language, animation, evaluation, natural language generation",129,136
10.1145/1414471.1414498,ASSETS,2008,A user evaluation of the SADIe transcoder, ,"The World Wide Web (Web) is a visually complex, dynamic, multimedia system that can be inaccessible to people with visual impairments. SADIe addresses this problem by using Semantic Web technologies to explicate implicit visual structures through a combination of an upper and lower ontology. This is then used to apply transcoding to a range of Websites. This paper describes a user evaluation that was performed using the SADIe system. Four users were presented with a series of Web pages, some having been adapted using SADIe's transcoding functionality and others retaining in their original state. The results of the evaluation showed that providing answers to a fact based question could be achieved more quickly when the information on the page was exposed via SADIe's transcoding. The data obtained during the experiment was analysed and shown to be statistically significant. This suggests that the transcoding techniques offered by SADIe can assist visually impaired users accessing content on the Web.","Darren Lunn, Sean Bechhofer, Simon Harper","accessibility, sadie, semantic transcoding, visually impaired users, web",137,144
10.1145/1414471.1414499,ASSETS,2008,What's new?,making web page updates accessible,"Web applications facilitated by technologies such as JavaScript, DHTML, AJAX, and Flash use a considerable amount of dynamic web content that is either inaccessible or unusable by blind people. Server side changes to web content cause whole page refreshes, but only small sections of the page update, causing blind web users to search linearly through the page to find new content. The connecting theme is the need to quickly and unobtrusively identify the segments of a web page that have changed and notify the user of them. In this paper we propose Dynamo, a system designed to unify different types of dynamic content and make dynamic content accessible to blind web users. Dynamo treats web page updates uniformly and its methods encompass both web updates enabled through dynamic content and scripting, and updates resulting from static page refreshes, form submissions, and template-based web sites. From an algorithmic and interaction perspective Dynamo detects underlying changes and provides users with a single and intuitive interface for reviewing the changes that have occurred. We report on the quantitative and qualitative results of an evaluation conducted with blind users. These results suggest that Dynamo makes access to dynamic content faster, and that blind web users like it better than existing interfaces.","Yevgen Borodin, Jeffrey Bigham, Rohit Raman, I. Ramakrishnan","blind users, dynamic content, hearsay, non-visual aural interface, screen reader, web browser",145,152
10.1145/1414471.1414500,ASSETS,2008,Accessibility commons,a metadata infrastructure for web accessibility,"Research projects, assistive technology, and individuals all create metadata in order to improve Web accessibility for visually impaired users. However, since these projects are disconnected from one another, this metadata is isolated in separate tools, stored in disparate repositories, and represented in incompatible formats. Web accessibility could be greatly improved if these individual contributions were merged. An integration method will serve as the bridge between future academic research projects and end users, enabling new technologies to reach end users more quickly. Therefore we introduce Accessibility Commons, a common infrastructure to integrate, store, and share metadata designed to improve Web accessibility. We explore existing tools to show how the metadata that they produce could be integrated into this common infrastructure, we present the design decisions made in order to help ensure that our common repository will remain relevant in the future as new metadata is developed, and we discuss how the common infrastructure component facilitates our broader social approach to improving accessibility.","Shinya Kawanaka, Yevgen Borodin, Jeffrey Bigham, Darren Lunn, Hironobu Takagi, Chieko Asakawa","annotation, database, metadata, web accessibility",153,160
10.1145/1414471.1414502,ASSETS,2008,Sudoku access,a sudoku game for people with motor disabilities,"Educational games are a beneficial activity motivating a large number of students in our society. Unfortunately, disabled people have reduced opportunities when using a computer game. We have created a new Sudoku game for people whose motion is impaired, called Sudoku Access. This special interface allows the control of the game either by voice or by a single switch. We conducted a user study of the Sudoku Access that shows that people can play the game quickly and accurately. With this special Sudoku puzzle we can help more people to get involved in computer games and contribute to develop logic thinking and concentration in students. Our research aims at building enabling technologies that increase individuals' functional independence in a game environment.","St&#233;phane Norte, Fernando Lobo","accessibility, scanning systems, speech recognition, sudoku puzzle",161,168
10.1145/1414471.1414503,ASSETS,2008,Blind hero,enabling guitar hero for the visually impaired,"Very few video games have been designed or adapted to allow people with vision impairment to play. Music/rhythm games however are particularly suitable for such people as they are perfectly capable of perceiving audio signals. Guitar Hero is a popular rhythm game yet it is not accessible to the visually impaired as it relies on visual stimuli. This paper explores replacing visual stimuli with haptic stimuli as a viable strategy to make games accessible. We developed a glove that transforms visual information into haptic feedback using small pager motors attached to the tip of each finger. This allows a blind player to play Guitar Hero. Several tests have been conducted and despite minor changes to the gameplay, visually impaired players are able to play the game successfully and enjoy the challenge the game provides. The results of the study also give valuable insights on how to make mainstream games blind-accessible.","Bei Yuan, Eelke Folmer","blind, game accessibility, haptic, visually impaired",169,176
10.1145/1414471.1414504,ASSETS,2008,PowerUp,an accessible virtual world,"PowerUp is a multi-player virtual world educational game with a broad set of accessibility features built in. This paper considers what features are necessary to make virtual worlds usable by individuals with a range of perceptual, physical, and cognitive disabilities. The accessibility features were included in the PowerUp game and validated, to date, with blind and partially sighted users. These features include in-world navigation and orientation tools, font customization, self-voicing text-to-speech output, and keyboard-only and mouse-only navigation. We discuss user requirements gathering, the validation study, and further work needed.","Shari Trewin, Vicki Hanson, Mark Laff, Anna Cavender","3d, accessibility, virtual worlds",177,184
10.1145/1414471.1414506,ASSETS,2008,RouteCheckr,personalized multicriteria routing for mobility impaired pedestrians,"Mobility impaired people use a variety of assistive technologies to navigate independently in everyday life. Although several technical approaches for navigation systems exist, many drawbacks remain due to lack of geospatial resolution, inadequate geographical data provided, and missing adaptation of routes to a multitude of user specific criteria. We developed RouteCheckr, a client/server system for collaborative multimodal annotation of geographical data and personalized routing of mobility impaired pedestrians. The construction of algorithms supporting multiple bipolar criteria is described, applied to route calculation, and demonstrated in our university's campus. To satisfy individual requirements, user profiles are incorporated enabling adaptivity over heterogeneous user groups while preserving privacy. Finally, a general architecture for RouteCheckr is presented and simulation results are analyzed.","Thorsten V&#246;lkel, Gerhard Weber","mobility impaired, multicriteria routing, multimodal annotation",185,192
10.1145/1414471.1414507,ASSETS,2008,Social accessibility,achieving accessibility through collaborative metadata authoring,"Web content is under the control of site owners, and therefore the site owners have the responsibility to make their content accessible. This is a basic assumption of Web accessibility. Users who want access to inaccessible content must ask the site owners for help. However, the process is slow and too often the need is mooted before the content becomes accessible. Social Accessibility is an approach to drastically reduce the burden on site owners and to shorten the time to provide accessible Web content by allowing volunteers worldwide to - renovate' any webpage on the Internet. Users encountering Web access problems anywhere at any time will be able to immediately report the problems to a social computing service. Volunteers can be quickly notified, and they can easily respond by creating and publishing the requested accessibility metadata--also helping any other users who encounter the same problems. Site owners can learn about the methods for future accessibility renovations based on the volunteers' external metadata. There are two key technologies to enable this process, the external metadata that allows volunteers to annotate existing Web content, and the social computing service that supports the collaborative renovations. In this paper, we will first review previous approaches, and then propose the Social Accessibility approach. The scenario, implementation, and results of a pilot service are introduced, followed by discussion of future directions.","Hironobu Takagi, Shinya Kawanaka, Masatomo Kobayashi, Takashi Itoh, Chieko Asakawa","collaborative authoring, metadata, social computing, transcoding, web accessibility",193,200
10.1145/1414471.1414508,ASSETS,2008,Hunting for headings,sighted labeling vs. automatic classification of headings,"Proper use of headings in web pages can make navigation more efficient for blind web users by indicating semantic divisions in the page. Unfortunately, many web pages do not use proper HTML markup (h1-h6 tags) to indicate headings, instead using visual styling to create headings, thus making the distinction between headings and other page text indistinguishable to blind users. In a user study in which sighted participants labeled headings on a set of web pages, participants did not often agree on which elements on the page should be labeled as headings, suggesting why headings are not used properly on the web today. To address this problem, we have created a system called HeadingHunter that predicts whether web page text semantically functions as a heading by examining visual features of the text as rendered in a web browser. Its performance in labeling headings compares favorably with both a manually-classified set of heading examples and the combined results of the sighted labelers in our study. The resulting system illustrates a general methodology of creating simple scripts operating over visual features that can be directly included in existing tools.","Jeremy Brudvik, Jeffrey Bigham, Anna Cavender, Richard Ladner","blind web users, heading tags, web accessibility",201,208
10.1145/1414471.1414510,ASSETS,2008,Text entry for mobile devices and users with severe motor impairments,"handiglyph, a primitive shapes based onscreen keyboard","In recent works, we have developed a text input method based on analogy with capital Latin characters and on the decomposition of characters into basic shapes. It has been designed to be universal and to allow for typing text with only one keystroke per character. In this paper we present a new implementation of this method for users with motor impairments. It uses the same principles in addition to some common techniques used in the Augmentative and Alternative Communication (AAC) domain. The new solution has been tested with a person with Locked-In Syndrome and the results are promising.","Mohammed Belatar, Franck Poirier","accessibility, analogy, autonomy, handicapped people, mobile devices, motor disabilities, onscreen keyboard, text input, versatility",209,216
10.1145/1414471.1414511,ASSETS,2008,Performance-based functional assessment,an algorithm for measuring physical capabilities,"The description of users with motor limitations is a significant dilemma for accessibility researchers and system designers alike. Current practice is to use descriptors such as medical diagnoses to represent a person's physical capabilities. This solution is not adequate due to similarities in functional capabilities between diagnoses as well as differences in capabilities within a diagnosis. An alternative is user self-reporting or observation by another person. These solutions are also problematic because they rely on individual interpretation of capabilities. The current research focuses on defining an objective, quantitative and repeatable methodology for assessing a person's physical capabilities in relation to use of computer technology. Results from this initial study are encouraging, including the development of a model which accounts for up to 85% of the variance in user capabilities.","Kathleen Price, Andrew Sears","HCI, accessibility, functional assessment, physical capabilities",217,224
10.1145/1414471.1414512,ASSETS,2008,Laser pointers and a touch screen,intuitive interfaces for autonomous mobile manipulation for the motor impaired,"El-E (""Ellie"") is a prototype assistive robot designed to help people with severe motor impairments manipulate everyday objects. When given a 3D location, El-E can autonomously approach the location and pick up a nearby object. Based on interviews of patients with amyotrophic lateral sclerosis (ALS), we have developed and tested three distinct interfaces that enable a user to provide a 3D location to El-E and thereby select an object to be manipulated: an ear-mounted laser pointer, a hand-held laser pointer, and a touch screen interface. Within this paper, we present the results from a user study comparing these three user interfaces with a total of 134 trials involving eight patients with varying levels of impairment recruited from the Emory ALS Clinic. During this study, participants used the three interfaces to select everyday objects to be approached, grasped, and lifted off of the ground. The three interfaces enabled motor impaired users to command a robot to pick up an object with a 94.8% success rate overall after less than 10 minutes of learning to use each interface. On average, users selected objects 69% more quickly with the laser pointer interfaces than with the touch screen interface. We also found substantial variation in user preference. With respect to the Revised ALS Functional Rating Scale (ALSFRS-R), users with greater upper-limb mobility tended to prefer the hand-held laser pointer, while those with less upper-limb mobility tended to prefer the ear-mounted laser pointer. Despite the extra efficiency of the laser pointer interfaces, three patients preferred the touch screen interface, which has unique potential for manipulating remote objects out of the user's line of sight. In summary, these results indicate that robots can enhance accessibility by supporting multiple interfaces. Furthermore, this work demonstrates that the communication of 3D locations during human-robot interaction can serve as a powerful abstraction barrier that supports distinct interfaces to assistive robots while using identical, underlying robotic functionality.","Young Sang Choi, Cressel Anderson, Jonathan Glass, Charles Kemp","ALS, amyotrophic lateral sclerosis, assistive robot, human-robot interaction, laser pointer interface, mobile manipulation",225,232
10.1145/1639642.1639646,ASSETS,2009,Comparing evaluation techniques for text readability software for adults with intellectual disabilities, ,"In this paper, we compare alternative techniques for evaluating a software system for simplifying the readability of texts for adults with mild intellectual disabilities (ID). We introduce our research on the development of software to automatically simplify news articles, display them, and read them aloud for adults with ID. Using a Wizard-of-Oz prototype, we conducted experiments with a group of adults with ID to test alternative formats of questions to measure comprehension of the information in the news articles. We have found that some forms of questions work well at measuring the difficulty level of a text: multiple-choice questions with three answer choices, each illustrated with clip-art or a photo. Some types of questions do a poor job: yes/no questions and Likert-scale questions in which participants report their perception of the text's difficulty level. Our findings inform the design of future evaluation studies of computational linguistic software for adults with ID; this study may also be of interest to researchers conducting usability studies or other surveys with adults with ID.","Matt Huenerfauth, Lijun Feng, No&#233;mie Elhadad","assistive technology, intellectual disabilities, natural language processing, text comprehension, text readability assessment",3,10
10.1145/1639642.1639647,ASSETS,2009,Designing judicious interactions for cognitive assistance,the acts of assistance approach,"The completion of complex activities of daily living (ADL) like meal preparation is a key concept for achieving autonomous living. Due to cognitive impairments, some people need to be supported when performing ADL. In this paper, we present a technological approach to guide people with cognitive impairments to complete complex activities. The assistance is provided in the form of pervasive human-machine interactions (HMI), that encourage the person to complete some actions to settle the difficulty identified by the system. These HMI are called ""acts of assistance"". This approach was implemented in Archipel, a cognitive orthosis developed at the DOMUS Laboratory, University of Sherbrooke (Canada). An evaluation of the prototype was conducted around meal preparation activities, involving 12 people with intellectual disabilities. This study demonstrates that our approach is promising.","J&#233;r&#233;my Bauchet, H&#233;l&#232;ne Pigot, Sylvain Giroux, Dany Lussier-Desrochers, Yves Lachapelle, Mounir Mokhtari","activity monitoring, assistance generation, cognitive assistance, pervasive HMI, speech-acts theory",11,18
10.1145/1639642.1639648,ASSETS,2009,Context-aware prompting to transition autonomously through vocational tasks for individuals with cognitive impairments, ,"A challenge to individuals with cognitive impairments in workplaces is how to remain engaged, recall task routines, and transition autonomously across tasks in a way relying on limited cognitive capacity. A novel task prompting system is presented with an aim to increase workplace and life independence for people with traumatic brain injury, cerebral palsy, intellectual disability, schizophrenia, and Down syndromes. This paper describes an approach to providing distributed cognition support of work engagement for persons with cognitive disabilities. The unique strength of the system is the ability to provide unique-to-the-user prompts that are triggered by context. As this population is very sensitive to issues of abstraction (e.g. icons) and presents the designer with the need to tailor prompts to a 'universe-of-one' the use of picture or verbal cues specific to each user and context is implemented. The key to the approach is to spread the context awareness across the system, with the context being flagged by beacon sources and the appropriate response being evoked by displaying the appropriate task prompting cues indexed by the intersection of specific end-user and context ID embedded in the beacons. By separating the context trigger from the pictorial or verbal response, responses can be updated independently of the rest of the installed system, and a single beacon source can trigger multiple responses in the PDA depending on the end-user and their specific tasks. A prototype is built and tested in field experiments involving eight individuals with cognitive impairments. The experimental results show the task load of the human-device interface is low or very low and the capabilities of helping with task engagement are high and reliable.","Yao-Jen Chang, Wan Chih Chang, Tsen-Yung Wang","cognitively impaired, social services, task prompting, ubiquitous computing",19,26
10.1145/1639642.1639649,ASSETS,2009,Customizing directions in an automated wayfinding system for individuals with cognitive impairment, ,"Individuals with cognitive impairments would prefer to live independently, however issues in wayfinding prevent many from fully living, working, and participating in their community. Our research has focused on designing, prototyping, and evaluating a mobile wayfinding system to aid such individuals. Building on the feedback gathered from potential users, we have implemented the system's automated direction selection functionality. Using a decision-theoretic approach, we believe we can create better wayfinding experience that assists users to reach their destination more intuitively than traditional navigation systems. This paper describes the system and results from a study using system-generated directions that inform us of key customization factors that would provide improved wayfinding assistance for individual users.","Alan Liu, Harlan Hile, Gaetano Borriello, Pat Brown, Mark Harniss, Henry Kautz, Kurt Johnson","Markov decision process, cognitive impairments, user interface, wayfinding",27,34
10.1145/1639642.1639651,ASSETS,2009,Usability of a multimodal videogame to improve navigation skills for blind children, ,"This work presents an evaluation study on the usability of a haptic device and a sound-based videogame for the development and use of orientation and mobility (O&M) skills in closed, unfamiliar spaces by blind, school-aged children. A usability evaluation was implemented for a haptic device especially designed for this project (Digital Clock Carpet) and a 3D videogame (MOVA3D), in order to redesign and improve the usability, as well as to learn of its acceptance and the degree of the user's satisfaction with the interaction with these products for O&M purposes. The results show that both the haptic device and the videogame are usable, accepted and pleasant regarding their use by blind children, and that they are ready to be used in the following stage, which will determine their impact on the development and use of O&M skills.","Jaime S&#225;nchez, Mauricio S&#225;enz, Miguel Ripoll","blind children, haptic and audio interfaces, navigation, virtual environments",35,42
10.1145/1639642.1639652,ASSETS,2009,Instant tactile-audio map,enabling access to digital maps for people with visual impairment,"In this paper, we propose an automatic approach, complete with a prototype system, for supporting instant access to maps for local navigation by people with visual impairment. The approach first detects and segments texts from a map image and recreates the remaining graphical parts in a tactile form which can be reproduced immediately through a tactile printer. Then, it generates an SVG (Scalable Vector Graphics) file, which integrates both text and graphical information. The tactile hardcopy and the SVG file together are used to provide a user with interactive access to the map image through a touchpad, resulting in a tactile-audio representation of the original input image. This supports real-time access to the map without tedious conversion by a sighted professional. Evaluations with six users who are blind show that the created tactile-audio maps from our prototype system convey the most important map information and are deemed as potentially useful for local navigation.","Zheshen Wang, Baoxin Li, Terri Hedgpeth, Teresa Haven","accessibility, multi-modal system, tactile map, visual impairment",43,50
10.1145/1639642.1639653,ASSETS,2009,Rock Vibe,Rock Band&#174; computer games for people with no or limited vision,"This paper reports <i>Rock Vibe</i>, a modification performed on <i>Rock Band&#174;</i> computer game to represent visual information using haptic and audio feedback to allow people with no or limited vision to enjoy the game. We modified the drumming activity of <i>Rock Band&#174;</i> by providing users with vibrations on upper and lower arms to represent the drumhead cues and on ankle to represent the kick drum cue. Auditory information is used to provide feedback on correct and timely hit (with various drumming sounds) or errors (with a click sound). Computer's standard speech synthesizer is used to read the menu, song title, instruction and score. A series of evaluations with people with various levels of visual impairment were performed at different stages of the system development. We found that users were able to master the system almost immediately, with some users making no error halfway through the first song.","Troy Allman, Rupinder Dhillon, Molly Landau, Sri Kurniawan","blindness, games, haptic, visual impairment",51,58
10.1145/1639642.1639654,ASSETS,2009,TextSL,a command-based virtual world interface for the visually impaired,"The immersive graphics, large amount of user-generated content, and social interaction opportunities offered by popular virtual worlds, such as Second Life, could eventually make for a more interactive and informative World Wide Web. Unfortunately, virtual worlds are currently not accessible to users who are visually impaired. This paper presents the work on developing TextSL, a client for Second life that can be accessed with a screen reader. Users interact with TextSL using a command-based interface, which allows for performing a plethora of different actions on large numbers of objects and avatars; characterizing features of such virtual worlds. User studies confirm that a command-based interface is a feasible approach towards making virtual worlds accessible, as it allows screen reader users to explore Second Life, communicate with other avatars, and interact with objects as well as sighted users. Command-based exploration and object interaction is significantly slower, but communication can be performed with the same efficiency as in the Second Life viewer. We further identify that at least 31% of the objects in Second Life lack a descriptive name, which is a significant barrier towards making virtual worlds accessible to users who are visually impaired.","Eelke Folmer, Bei Yuan, Dave Carr, Manjari Sapre","games, screen reader, virtual worlds, visual impairments",59,66
10.1145/1639642.1639656,ASSETS,2009,ClassInFocus,enabling improved visual attention strategies for deaf and hard of hearing students,"Deaf and hard of hearing students must juggle their visual attention in current classroom settings. Managing many visual sources of information (instructor, interpreter or captions, slides or whiteboard, classmates, and personal notes) can be a challenge. ClassInFocus automatically notifies students of classroom changes, such as slide changes or new speakers, helping them employ more beneficial observing strategies. A user study of notification techniques shows that students who liked the notifications were more likely to visually utilize them to improve performance.","Anna Cavender, Jeffrey Bigham, Richard Ladner","classroom technology, deaf and hard of hearing users, multimedia conferencing technology",67,74
10.1145/1639642.1639657,ASSETS,2009,Spatial and temporal pyramids for grammatical expression recognition of American sign language, ,"Given that sign language is used as a primary means of communication by as many as two million deaf individuals in the U.S. and as augmentative communication by hearing individuals with a variety of disabilities, the development of robust, real-time sign language recognition technologies would be a major step forward in making computers equally accessible to everyone. However, most research in the field of sign language recognition has focused on the manual component of signs, despite the fact that there is critical grammatical information expressed through facial expressions and head gestures. We propose a novel framework for robust tracking and analysis of facial expression and head gestures, with an application to sign language recognition. We then apply it to recognition with excellent accuracy (&#8805;=95%) of two classes of grammatical expressions, namely wh-questions and negative expressions. Our method is signer-independent and builds on the popular ""bag-of-words"" model, utilizing spatial pyramids to model facial appearance and temporal pyramids to represent patterns of head pose changes.","Nicholas Michael, Dimitris Metaxas, Carol Neidle","expression recognition, face tracking, head pose estimation, kernel codebooks, pyramid match kernel, sign language recognition, soft quantization, spatio-temporal pyramids",75,82
10.1145/1639642.1639658,ASSETS,2009,Accessible motion-capture glove calibration protocol for recording sign language data from deaf subjects, ,"Motion-capture recordings of sign language are used in research on automatic recognition of sign language or generation of sign language animations, which have accessibility applications for deaf users with low levels of written-language literacy. Motion-capture gloves are used to record the wearer's handshape. Unfortunately, these gloves require a time-consuming and inexact manual calibration process each time they are worn. This paper describes the design and evaluation of a new calibration protocol for motion-capture gloves, which is designed to make the process more efficient and to be accessible for participants who are deaf and use American Sign Language (ASL). The protocol was evaluated experimentally; deaf ASL signers wore the gloves, were calibrated (using the new protocol and using a calibration routine provided by the glove manufacturer), and were asked to perform sequences of ASL handshapes. A native ASL signer rated the correctness and understandability of the collected handshape data. The new protocol received significantly higher scores than the standard calibration. The protocol has been made freely available online, and it includes directions for the researcher, images and videos of how participants move their hands during the process, and directions for participants (as ASL videos and English text).","Pengfei Lu, Matt Huenerfauth","CyberGlove?, accessibility technology for people who are deaf, american sign language, animation, calibration, motion-capture glove",83,90
10.1145/1639642.1639660,ASSETS,2009,The one-key challenge,searching for a fast one-key text entry method,"A new one-key text entry method is presented. SAK, for ""scanning ambiguous keyboard"", combines one-key physical input (including error correction) with three virtual letter keys and a SPACE key. The virtual letter keys are highlighted in sequence (""scanned"") and selected when the key bearing the desired letter receives focus. There is only one selection per letter. Selecting SPACE transfers scanning to a word-selection region, which presents a list of candidate words. A novel feature of SAK is multiple-letter-selection in a single scanning interval. In an evaluation with 12 participants, average entry speeds reached 5.11 wpm (all trials, 99% accuracy) or 7.03 wpm (error-free trials). A modification using ""timer restart on selection"" allowed for more time and more selections per scanning interval. One participant performed extended trials (5 blocks x 5 phrases/block) with the modification and reached an average entry speed of 9.28 wpm.",I. MacKenzie,"ambiguous keyboards, assistive technologies, keyboards, mobile computing, scanning keyboards, text entry",91,98
10.1145/1639642.1639661,ASSETS,2009,NavTap,a long term study with excluded blind users,"NavTap is a navigational method that enables blind users to input text in a mobile device by reducing the associated cognitive load. In this paper, we present studies that go beyond a laboratorial setting, exploring the methods' effectiveness and learnability as well as its influence on the users' daily lives. Eight blind users participated in designing the prototype (3 weeks) while five took part in the studies along 16 more weeks. Results gathered in controlled weekly sessions and real life usage logs enabled us to better understand NavTap's advantages and limitations. The method revealed itself both as easy to learn and improve. Indeed, users were able to better control their mobile devices to send SMS and use other tasks that require text input such as managing a phonebook, from day one, in real-life settings. While individual user profiles play an important role in determining their evolution, even less capable users (with age-induced impairments or cognitive difficulties), were able to perform the assigned tasks (sms, directory) both in the laboratory and in everyday use, showing continuous improvement to their skills. According to interviews, none were able to input text before. Nav-Tap dramatically changed their relation with mobile devices and noticeably improved their social interaction capabilities.","Tiago Guerreiro, Hugo Nicolau, Joaquim Jorge, Daniel Gon&#231;alves","blind, evaluation, mobile accessibility, text-entry",99,106
10.1145/1639642.1639662,ASSETS,2009,Haptic handheld wayfinder with pseudo-attraction force for pedestrians with visual impairments, ,"When visually impaired pedestrians walk from one place to another by themselves, they must update their orientation and position to find their way and avoid obstacles and hazards. We present the design of a new haptic direction indicator, whose purpose is to help blind pedestrians travel a path and avoid hazards intuitively and safely by means of haptic navigation. The haptic direction indicator uses a novel kinesthetic perception method called the ""pseudo-attraction force"" technique, which exploits the nonlinear relationship between perceived and physical acceleration to generate a force sensation. In an experiment performed to evaluate with the haptic direction indicator, we found that visually impaired users could safely walk along a predefined route at their usual walking pace, independent of the existence of auditory information. These results demonstrate the utility and usability of the haptic direction indicator, but there is still room for improvement.","Tomohiro Amemiya, Hisashi Sugiyama","asymmetric oscillation, maze task, wayfinding",107,114
10.1145/1639642.1639663,ASSETS,2009,Freedom to roam,a study of mobile device adoption and accessibility for people with visual and motor disabilities,"Mobile devices provide people with disabilities new opportunities to act independently in the world. However, these empowering devices have their own accessibility challenges. We present a formative study that examines how people with visual and motor disabilities select, adapt, and use mobile devices in their daily lives. We interviewed 20 participants with visual and motor disabilities and asked about their current use of mobile devices, including how they select them, how they use them while away from home, and how they adapt to accessibility challenges when on the go. Following the interviews, 19 participants completed a diary study in which they recorded their experiences using mobile devices for one week. Our results show that people with visual and motor disabilities use a variety of strategies to adapt inaccessible mobile devices and successfully use them to perform everyday tasks and navigate independently. We provide guidelines for more accessible and empowering mobile device design.","Shaun Kane, Chandrika Jayant, Jacob Wobbrock, Richard Ladner","accessibility, blindness, diary study, low vision, mobile devices, mobile phones, motor impairment",115,122
10.1145/1639642.1639665,ASSETS,2009,Enriching web information scent for blind users, ,"Link annotation with the accessibility level of the target Web page is an adaptive navigation support technique aimed at increasing blind users' orientation in Web sites. In this work, the accessibility level of a page is measured by exploiting data from evaluation reports produced by two automatic assessment tools. These tools support evaluation of accessibility and usability guideline-sets. As a result, links are annotated with a score that indicates the conformance of the target Web page to blind user accessibility and usability guidelines. A user test with 16 users was conducted in order to observe the strategies they followed when links were annotated with these scores. With annotated links, the navigation paradigm changed from sequential to browsing randomly through the subset of those links with high scores. Even if there was not a general agreement on the correspondence between scores and user perception of accessibility, users found annotations helpful when browsing through links related to a given topic.","Markel Vigo, Barbara Leporini, Fabio Patern&#242;","adaptive navigation, blind users, information scent, web accessibility",123,130
10.1145/1639642.1639666,ASSETS,2009,Validity and reliability of web accessibility guidelines, ,"Although widely used,Web Content Accessibility Guidelines (WCAG) have not been studied from the viewpoint of their validity and reliability. WCAG 2.0 explicitly claim that they are based on ""testable"" criteria, but no scientific evidence exists that this is actually the case. Validity (how well all and only the true problems can be identified) and reliability (the extent to which different evaluations of the same page lead to same results) are key factors for quality of accessibility evaluation methods. They need to be well studied and understood for methods, and guidelines, that are expected to have a major impact. This paper presents an experiment aimed at finding out what is the validity and reliability of different checkpoints taken from WCAG 1.0 and WCAG 2.0. The experiment employed 35 young web developers with some knowledge on web accessibility. Although this is a small-scale experiment, unlikely to provide definite and general answers, results un-equivocally show that with respect to the kind of evaluators chosen in the experiment, checkpoints in general fare very low in terms of reliability, and that from this perspective WCAG 2.0 are not an improvement over WCAG 1.0.",Giorgio Brajnik,"accessibility evaluation evaluation, web accessibility guidelines",131,138
10.1145/1639642.1639668,ASSETS,2009,Evaluating prosodic cues as a means to disambiguate algebraic expressions,an empirical study,"The automatic translation of written mathematical expressions to their spoken equivalent is a difficult task. Written mathematics makes use of specialized symbols and a 2-dimensional layout that is hard to translate into clear and unambiguous spoken words. Our approach is to use prosody to help listeners follow along to mathematical expressions spoken aloud with text-to-speech synthesized voices. To achieve this, we developed and empirically tested XSL transformation rules that automatically translate mathematical expressions marked-up with Presentation MathML into corresponding markup using the Speech Synthesis Markup Language (SSML). In this paper, we report on the results from an empirical study we conducted that showed that the simple insertion of pauses inside spoken mathematical expressions dramatically improved subjects' ability to disambiguate between two similar algebraic expressions. Result from our study should benefit designers of screen readers and related audio-based tools that produce spoken renderings of mathematical expressions.","Ed Gellenbeck, Andreas Stefik","DAISY, MathML, SSML, accessibility, synthetic speech",139,146
10.1145/1639642.1639669,ASSETS,2009,Making Microsoft Excel&#8482;,multimodal presentation of charts,"Several solutions, based on aural and haptic feedback, have been developed to enable access to complex on-line information for people with visual impairments. Nevertheless, there are several components of widely used software applications that are still beyond the reach of screen readers and Braille displays. This paper investigates the non-visual accessibility issues associated with the graphing component of Microsoft Excel"". The goal is to provide flexible multi-modal navigation schemes which can help visually impaired users in comprehending Excel charts. The methodology identifies the need for 3 strategies used in interaction: <i>exploratory, guided,</i> and <i>summarization</i>. Switching between them supports the development of a mental model of a chart. Aural cues and commentaries are integrated in a haptic presentation to help understanding the presented chart. The methodology has been implemented using the Novint Falcon haptic device.","Iyad Abu Doush, Enrico Pontelli, Dominic Simon, Tran Cao Son, Ou Ma","accessible graphs, assistive technology, haptic",147,154
10.1145/1639642.1639670,ASSETS,2009,Including accessibility within and beyond undergraduate computing courses, ,"This paper presents a unique approach to undergraduate teaching in which accessibility topics are completely integrated throughout the curriculum, treating accessibility not as a separate topic, but rather as an integral part of design and development. Means of accomplishing this integration throughout the entire four-year curriculum are presented. We also describe how our expertise in accessible design has extended beyond the education of computer science and engineering students to include Web content authors across campus.","Annalu Waller, Vicki Hanson, David Sloan","accessibility, disability, higher education, inclusion, older adults",155,162
10.1145/1639642.1639672,ASSETS,2009,Speaking through pictures,images vs. icons,"People with aphasia, a condition that impairs the ability to understand or generate written or spoken language, are aided by assistive technology that helps them communicate through a vocabulary of icons. These systems are akin to language translation systems, translating icon arrangements into spoken or written language and vice versa. However, these icon-based systems have little vocabulary breadth or depth, making it difficult for people with aphasia to apply their usage to multiple real world situations. Pictures from the web are numerous, varied, and easily accessible and thus, could potentially address the small size issues of icon-based systems. We present results from two studies that investigate this potential and demonstrate that images can be as effective as icons when used as a replacement for English language communication. The first study uses elderly subjects to investigate the efficacy of images vs. icons in conveying word meaning; the second study examines the retention of word-level meaning by both images and icons with a population of aphasics. We conclude that images collected from the web are as functional as icons in conveying information and thus, are feasible to use in assistive technology that supports people with aphasia.","Xiaojuan Ma, Jordan Boyd-Graber, Sonya Nikolova, Perry Cook","aphasia, computerized visual communication (C-VIC), visual communication (VIC)",163,170
10.1145/1639642.1639673,ASSETS,2009,Better vocabularies for assistive communication aids,connecting terms using semantic networks and untrained annotators,"The difficulties of navigating vocabulary in an assistive communication device are exacerbated for individuals with lexical access disorders like those due to aphasia. We present the design and implementation of a vocabulary network based on WordNet, a resource that attempts to model human semantic memory, that enables users to find words easily. To correct for the sparsity of links among words, we augment WordNet with additional connections derived from human judgments of semantic similarity collected in an online experiment. We evaluate the resulting system, the visual vocabulary for aphasia (ViVA), and describe its potential to adapt to a user's profile and enable faster search and improved navigation.","Sonya Nikolova, Jordan Boyd-Graber, Christiane Fellbaum, Perry Cook","adaptive tools, aphasia, assistive communication, semantic networks, visual vocabularies",171,178
10.1145/1639642.1639674,ASSETS,2009,Let's stay in touch,"sharing photos for restoring social connectedness between rehabilitants, friends and family","A case study on the use of an existing photo sharing application in a spinal cord lesion rehabilitation centre is presented. The study focuses on enhancing social connectedness through sharing photos between rehabilitants and their families and friends. Four rehabilitants participated in this study for 6-7 weeks. Most photos sent related to sharing things in everyday life and keeping the rehabilitant informed about regular events. The combination of interviews and content analysis reveals that only a minority of the photos lead to follow-up communication about the contents of the photos. Rehabilitants were positively surprised how spontaneous photo sharing simplified the way they could reconnect to their friends and family, without the immediate need or obligation to engage in a (phone) conversation.","Margit Biemans, Betsy van Dijk, Pavan Dadlani, Aart van Halteren","photo sharing, rehabilitation, social connectedness",179,186
10.1145/1639642.1639675,ASSETS,2009,Talking points,the differential impact of real-time computer generated audio/visual feedback on speech-like & non-speech-like vocalizations in low functioning children with ASD,"Real-time computer feedback systems (CFS) have been shown to impact the communication of neurologically typical individuals. Promising new research appears to suggest the same for the vocalization of low functioning children with Autistic Spectrum Disorder (ASD). The distinction between speech-like versus non-speech-like vocalizations has rarely, if ever, been addressed in the HCI community. This distinction is critical as we strive to most effectively and efficiently facilitate speech development in children with ASD, while simultaneously helping decrease vocalizations that do not facilitate positive social interactions. This paper provided an extension of Hailpern et al. in 2009 by examining the influence of a computerized feedback system on both the speech-like and non-speech-like vocalizations of five nonverbal children with ASD. Results were largely positive, in that some form of computerized feedback was able to differentially facilitate speech-like vocalizations relative to nonspeech-like vocalizations in 4 of the 5 children. The main contribution of this work is in highlighting the importance of distinguishing between speech-like versus nonspeech-like vocalizations in the design of feedback systems focused on facilitating speech in similar populations.","Joshua Hailpern, Karrie Karahalios, Laura DeThorne, James Halle","accessibility, autism, children, feedback, speech, visualization, vocalization",187,194
10.1145/1639642.1639677,ASSETS,2009,Collaborative web accessibility improvement,challenges and possibilities,"Collaborative accessibility improvement has great potential to make the Web more adaptive in a timely manner by inviting users into the improvement process. The Social Accessibility Project is an experimental service for a new needs-driven improvement model based on collaborative metadata authoring technologies. In 10 months, about 18,000 pieces of metadata were created for 2,930 webpages through collaboration. We encountered many challenges as we sought to create a new mainstream approach. The productivity of the volunteer activities exceeded our expectation, but we found large and important problems in the screen reader users' lack of awareness of their own accessibility problems. In this paper, we first introduce examples, analyze some statistics from the pilot service and then discuss our findings and challenges. Three future directions including site-wide authoring are considered.","Hironobu Takagi, Shinya Kawanaka, Masatomo Kobayashi, Daisuke Sato, Chieko Asakawa","accessibility, collaboration, metadata, social computing, web",195,202
10.1145/1639642.1639678,ASSETS,2009,How much does expertise matter?,a barrier walkthrough study with experts and non-experts,"Manual accessibility evaluation plays an important role in validating the accessibility of Web pages. This role has become increasingly critical with the advent of the Web Content Accessibility Guidelines (WCAG) 2.0 and their reliance on user evaluation to validate certain conformance measures. However, the role of expertise, in such evaluations, is unknown and has not previously been studied. This paper sets out to investigate the interplay between expert and non-expert evaluation by conducting a Barrier Walkthrough (BW) study with 19 expert and 51 non-expert judges. The BW method provides an evaluation framework that can be used to manually assess the accessibility of Web pages for different user groups including motor impaired, hearing impaired, low vision, cognitive impaired, etc. We conclude that the level of expertise is an important factor in the quality of accessibility evaluation of Web pages. Expert judges spent significantly less time than non-experts; rated themselves as more productive and confident than non-experts; and ranked and rated pages differently against each type of disability. Finally, both effectiveness and reliability of the expert judges are significantly higher than non-expert judges.","Yeliz Yesilada, Giorgio Brajnik, Simon Harper","evaluation, expertise, guideline, web accessibility",203,210
10.1145/2700648.2809868,ASSETS,2015,The Tactile Graphics Helper,Providing Audio Clarification for Tactile Graphics Using Machine Vision,"Tactile graphics use raised lines, textures, and elevations to provide individuals with visual impairments access to graphical materials through touch. Tactile graphics are particularly important for students in science, technology, engineering, and mathematics (STEM) fields, where educational content is often conveyed using diagrams and charts. However, providing a student who has a visual impairment with a tactile graphic does not automatically provide the student access to the graphic's educational content. Instead, the student may struggle to decipher subtle differences between textures or line styles, and must deal with cramped and confusing placement of lines and braille. These format-related issues prevent students with visual impairments from accessing educational content in graphics independently, because they necessitate the students ask for sighted clarification. We propose a machine-vision based ""tactile graphics helper"" (TGH), which tracks a student's fingers as he/she explores a tactile graphic, and allows the student to gain clarifying audio information about the tactile graphic without sighted assistance. Using an embedded mixed-methods case study with three STEM university students with visual impairments, we confirmed that format-related issues prevent these students from accessing some graphical content independently, and established that TGH provides a promising approach for overcoming tactile-graphic format issues.","Giovanni Fusco, Valerie Morash","assistive devices, case study, finger tracking, machine vision, mixed-methods, tactile graphics",97,106
10.1145/2049536.2049558,ASSETS,2011,Evaluating quality and comprehension of real-time sign language video on mobile phones, ,"Video and image quality are often objectively measured using peak signal-to-noise ratio (PSNR), but for sign language video, human comprehension is most important. Yet the relationship of human comprehension to PSNR has not been studied. In this survey, we determine how well PSNR matches human comprehension of sign language video. We use very low bitrates (10-60 kbps) and two low spatial resolutions (192&#215;144 and 320&#215;240 pixels) which may be typical of video transmission on mobile phones using 3G networks. In a national online video-based user survey of 103 respondents, we found that respondents preferred the 320&#215;240 spatial resolution transmitted at 20 kbps and higher; this does not match what PSNR results would predict. However, when comparing perceived ease/difficulty of comprehension, we found that responses did correlate well with measured PSNR. This suggests that PSNR may not be suitable for representing subjective video quality, but can be reliable as a measure for comprehensibility of American Sign Language (ASL) video. These findings are applied to our experimental mobile phone application, MobileASL, which enables real-time sign language communication for Deaf users at low bandwidths over the U.S. 3G cellular network.","Jessica Tran, Joy Kim, Jaehong Chon, Eve Riskin, Richard Ladner, Jacob Wobbrock","american sign language, bitrate, deaf community, mobile phones, online survey, psnr, spatial resolution, video compression",115,122
10.1145/2982142.2982163,ASSETS,2016,LucentMaps,3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People,"Tactile maps support blind and visually impaired people in orientation and to familiarize with unfamiliar environments. Interactive approaches complement these maps with auditory feedback. However, commonly these approaches focus on blind people. We present an approach which incorporates visually impaired people by visually augmenting relevant parts of tactile maps. These audiovisual tactile maps can be used in conjunction with common tablet computers and smartphones. By integrating conductive elements into 3D printed tactile maps, they can be recognized by a single touch on the mobile device's display, which eases the handling for blind and visually impaired people. To allow multiple elevation levels in our transparent tactile maps, we conducted a study to reconcile technical and physiological requirements of off-the-shelf 3D printers, capacitive touch inputs and the human tactile sense. We propose an interaction concept for 3D printed audiovisual tactile maps, verify its feasibility and test it with a user study. Our discussion includes economic considerations crucial for a broad dissemination of tactile maps for both blind and visually impaired people.",Timo G&#246;tzelmann,"3d printing, accessibility, audio-tactile, blind, capacitive, capacitive sensing, functional, global, marker, orientation, tactile maps, tangible user interfaces, touch screen",81,90
10.1145/2049536.2049570,ASSETS,2011,Automatically generating tailored accessible user interfaces for ubiquitous services, ,"Ambient Assisted Living environments provide support to people with disabilities and elderly people, usually at home. This concept can be extended to public spaces, where ubiquitous accessible services allow people with disabilities to access intelligent machines such as information kiosks. One of the key issues in achieving full accessibility is the instantaneous generation of an adapted accessible interface suited to the specific user that requests the service. In this paper we present the method used by the EGOKI interface generator to select the most suitable interaction resources and modalities for each user in the automatic creation of the interface. The validation of the interfaces generated for four different types of users is presented and discussed.","Julio Abascal, Amaia Aizpurua, Idoia Cearreta, Borja Gamecho, Nestor Garay-Vitoria, Ra&#250;l Mi&#241;&#243;n","accessible user interfaces, adaptive systems, automatic user interface generation, ubiquitous computing",187,194
10.1145/2513383.2513433,ASSETS,2013,A haptic ATM interface to assist visually impaired users, ,"This paper outlines the design and evaluation of a haptic interface intended to convey non audio-visual directions to an ATM (Automated Teller Machine) user. The haptic user interface is incorporated into an ATM test apparatus on the keypad. The system adopts a well known 'clock face' metaphor and is designed to provide haptic prompts to the user in the form of directions to the current active device, e.g. card reader or cash dispenser. Results of an evaluation of the device are reported that indicate that users with varying levels of visual impairment are able to appropriately detect, distinguish and act on the prompts given to them by the haptic keypad. As well as reporting on how participants performed in the evaluation we also report the results of a semi structured interview designed to find out how acceptable participants found the technology for use on a cash machine. As a further contribution the paper also presents observations on how participants place their hands on the haptic device and compare this with their performance.","Brendan Cassidy, Gilbert Cockton, Lynne Coventry","acceptability, accessibility, automated teller machine, haptic, keypad, vibro-tactile, visual impairment",1,8
10.1145/3234695.3236362,ASSETS,2018,Towards Accessible Conversations in a Mobile Context for People who are Deaf and Hard of Hearing, ,"Prior work has explored communication challenges faced by people who are deaf and hard of hearing (DHH) and the potential role of new captioning and support technologies to address these challenges; however, the focus has been on stationary contexts such as group meetings and lectures. In this paper, we present two studies examining the needs of DHH people in moving contexts (e.g., walking) and the potential for mobile captions on head-mounted displays (HMDs) to support those needs. Our formative study with 12 DHH participants identifies social and environmental challenges unique to or exacerbated by moving contexts. Informed by these findings, we introduce and evaluate a proof-of-concept HMD prototype with 10 DHH participants. Results show that, while walking, HMD captions can support communication access and improve attentional balance between the speakers(s) and navigating the environment. We close by describing open questions in the mobile context space and design guidelines for future technology.","Dhruv Jain, Rachel Franz, Leah Findlater, Jackson Cannon, Raja Kushalnagar, Jon Froehlich","augmented reality, conversations, deaf and hard of hearing, head-mounted display, moving, real-time captioning",81,92
10.1145/2982142.2982157,ASSETS,2016,"Comparing Tactile, Auditory, and Visual Assembly Error-Feedback for Workers with Cognitive Impairments", ,"More and more industrial manufacturing companies are outsourcing assembly tasks to sheltered work organizations where cognitively impaired workers are employed. To facilitate these assembly tasks assistive systems have been introduced to provide cognitive assistance. While previous work found that these assistive systems have a great impact on the workers' performance in giving assembly instructions, these systems are further capable of detecting errors and notifying the worker of an assembly error. However, the topic of how assembly errors are presented to cognitively impaired workers has not been analyzed scientifically. In this paper, we close this gap by comparing tactile, auditory, and visual error feedback in a user study with 16 cognitively impaired workers. The results reveal that visual error feedback leads to a significantly faster assembly time compared to tactile error feedback. Further, we discuss design implications for providing error feedback for workers with cognitive impairments.","Thomas Kosch, Romina Kettner, Markus Funk, Albrecht Schmidt","assistive systems, augmented reality, cognitively impaired workers, error feedback, multimodal interfaces",53,60
10.1145/2700648.2809844,ASSETS,2015,Inclusion and Education,3D Printing for Integrated Classrooms,"Over 60% of adults with intellectual disabilities (ID) in the U.S. are unemployed; this is more than twice the unemployment rate of the general population [19]. Of the adults with ID who are employed, only half receive competitive wages alongside co-workers without disabilities. While the enactment of IDEA [20] has helped to promote access to education for people with ID and other disabilities, there are still obstacles to employment. Misconceptions about ability and lack of opportunities to learn and practice employability skills contribute to this problem. Our research explores employability and integration through the lens of 3D printing, an innovative technology touted as a means to self-employment. We successfully taught young adults with intellectual disabilities many technical skills required for 3D printing through an integrated, post-secondary course on 3D printing for entrepreneurship. In this paper we report on our methods for designing this course and discuss the benefits, challenges, and strategies for teaching 3D printing to an integrated cohort of students. We offer recommendations for educators and describe technology obstacles unique to this user demographic, and the impact of integrated, postsecondary courses on employment outcomes for students with ID.","Erin Buehler, William Easley, Samantha McDonald, Niara Comrie, Amy Hurst","3d modeling, 3d printing, cognitive impairment, disability, education, intellectual disability, special education",281,290
10.1145/2384916.2384942,ASSETS,2012,Online quality control for real-time crowd captioning, ,"Approaches for real-time captioning of speech are either expensive (professional stenographers) or error-prone (automatic speech recognition). As an alternative approach, we have been exploring whether groups of non-experts can collectively caption speech in real-time. In this approach, each worker types as much as they can and the partial captions are merged together in real-time automatically. This approach works best when partial captions are correct and received within a few seconds of when they were spoken, but these assumptions break down when engaging workers on-demand from existing sources of crowd work like Amazon's Mechanical Turk. In this paper, we present methods for quickly identifying workers who are producing good partial captions and estimating the quality of their input. We evaluate these methods in experiments run on Mechanical Turk in which a total of 42 workers captioned 20 minutes of audio. The methods introduced in this paper were able to raise overall accuracy from 57.8% to 81.22% while keeping coverage of the ground truth signal nearly unchanged.","Walter Lasecki, Jeffrey Bigham","captioning, deaf, hard of hearing, human computation",143,150
10.1145/2049536.2049568,ASSETS,2011,A mobile phone based personal narrative system, ,"Currently available commercial Augmentative and Alternative Communication (AAC) technology makes little use of computing power to improve the access to words and phrases for personal narrative, an essential part of social interaction. In this paper, we describe the development and evaluation of a mobile phone application to enable data collection for a personal narrative system for children with severe speech and physical impairments (SSPI). Based on user feedback from the previous project ""How was School today?"" we developed a modular system where school staff can use a mobile phone to track interaction with people and objects and user location at school. The phone also allows taking digital photographs and recording voice message sets by both school staff and parents/carers at home. These sets can be played back by the child for immediate narrative sharing similar to established AAC device interaction using sequential voice recorders. The mobile phone sends all the gathered data to a remote server. The data can then be used for automatic narrative generation on the child's PC based communication aid. Early results from the ongoing evaluation of the application in a special school with two participants and school staff show that staff were able to track interactions, record voice messages and take photographs. Location tracking was less successful, but was supplemented by timetable information. The participating children were able to play back voice messages and show photographs on the mobile phone for interactive narrative sharing using both direct and switch activated playback options.","Rolf Black, Annalu Waller, Nava Tintarev, Ehud Reiter, Joseph Reddington","accessibility, assistive technology, augmentative and alternative communication (aac), cerebral palsy, disability, language development, personal narrative",171,178
10.1145/2661334.2661380,ASSETS,2014,"""just let the cane hit it""",how the blind and sighted see navigation differently,"Sighted people often have the best of intentions when they want to help a blind person navigate, but their well meaning is also often coupled with a lack of knowledge and understanding about how a person navigates without vision. As a result what sighted people think is the right feedback is too often the wrong feedback to give to a person with a visual impairment. Understanding how to provide feedback to blind navigators is crucial to the design of assistive technologies for navigation. In our research investigating the design of a personal pedestrian navigation device, we observed firsthand the ways that sighted people seemingly misunderstand how many blind people navigate when using a white cane mobility aid. Throughout our qualitative end user studies that included focus groups and observations (including couple-based observations with a close companion) we gathered data that explicitly shows how the language and understanding of sighted vs. blind pedestrians differs greatly and even how it can be dangerous when people interfere in the wrong way. From our findings we discuss why it is difficult for a blind person to navigate like a sighted person to ensure designers are aware of the difficulties and designing with new training in mind, not simply designing from their own point of view. We also want to encourage advocacy and empathy amongst the sighted community towards this activity of walking around independently.","Michele Williams, Caroline Galbraith, Shaun Kane, Amy Hurst","blind navigation, empathy, white cane",217,224
10.1145/2700648.2809860,ASSETS,2015,Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users, ,"Technology to automatically synthesize linguistically accurate and natural-looking animations of American Sign Language (ASL) from an easy-to-update script would make it easier to add ASL content to websites and media, thereby increasing information accessibility for many people who are deaf. Researchers evaluate their sign language animation systems by collecting subjective judgments and comprehension-question responses from deaf participants. Through a survey (N=62) and multiple regression analysis, we identified relationships between (a) demographic and technology experience/attitude characteristics of participants and (b) the subjective and objective scores collected from them during the evaluation of sign language animation systems. This finding suggests that it would be important for researchers to collect and report these characteristics of their participants in publications about their studies, but there is currently no consensus in the field. We present a set of questions in ASL and English that can be used by researchers to measure these participant characteristics; reporting such data would enable researchers to better interpret and compare results from studies with different participant pools.","Hernisa Kacorri, Matt Huenerfauth, Sarah Ebling, Kasmira Patel, Mackenzie Willard","American sign language, accessibility technology for people who are deaf, animation, user study",147,154
10.1145/2982142.2982172,ASSETS,2016,Would You Be Mine,Appropriating Minecraft as an Assistive Technology for Youth with Autism,"Those with disabilities have long adopted, adapted, and appropriated collaborative systems to serve as assistive devices. In this paper, we present the results of a digital ethnography in a Minecraft virtual world for children with autism, specifically examining how this community has used do-it-yourself (DIY) making activities to transform the game into a variety of assistive technologies. Our results demonstrate how players and administrators ""mod"" the Minecraft system to support self-regulation and community engagement. This work highlights the ways in which we, as researchers concerned with accessible and equitable computing spaces, might reevaluate the scope of our inquiry, and how designers might encourage and support appropriation, enhancing users' experience and long-term adoption.","Kathryn Ringland, Christine Wolf, LouAnne Boyd, Mark Baldwin, Gillian Hayes","appropriation, assistive technology, autism, disability., diy, minecraft, modding, virtual worlds",33,41
10.1145/2982142.2982156,ASSETS,2016,A Computer Vision-Based System for Stride Length Estimation using a Mobile Phone Camera, ,"Conditions such as Parkinson's disease (PD), a chronic neurodegenerative disorder which severely affects the motor system, will be an increasingly common problem for our growing and aging population. Gait analysis is widely used as a noninvasive method for PD diagnosis and assessment. However, current clinical systems for gait analysis usually require highly specialized cameras and lab settings, which are expensive and not scalable. This paper presents a computer vision-based gait analysis system using a camera on a common mobile phone. A simple PVC mat was designed with markers printed on it, on which a subject can walk whilst being recorded by a mobile phone camera. A set of video analysis methods were developed to segment the walking video, detect the mat and feet locations, and calculate gait parameters such as stride length. Experiments showed that stride length measurement has a mean absolute error of 0.62 cm, which is comparable with the ""gold standard"" walking mat system GAITRite. We also tested our system on Parkinson's disease patients in a real clinical environment. Our system is affordable, portable, and scalable, indicating a potential clinical gait measurement tool for use in both hospitals and the homes of patients.","Wei Zhu, Boyd Anderson, Shenggao Zhu, Ye Wang","computer vision, gait analysis, mobile camera, movement disorder, parkinson's disease, video analysis",121,130
10.1145/2661334.2661356,ASSETS,2014,"Age, technology usage, and cognitive characteristics in relation to perceived disorientation and reported website ease of use", ,"Comparative studies including older and younger adults are becoming more common in HCI, generally used to compare how these two different age groups will approach a task. However, it is unclear whether user ""age"" is the underlying factor that differentiates between these two groups. To address this problem, an examination into the relationship between users' age, previous technology experience, and cognitive characteristics is conducted. Measures of perceived disorientation and reported ease of use are used to understand links that exist between these user characteristics and their effect on browsing experience. This is achieved through a lab-based information retrieval task, where participants visited a selection of websites in order to find answers to a series of questions and then self reported their feelings of perceived disorientation and website ease of use through a Likert-scored questionnaire. The presented research found that age accounts for as little as 1% of user browsing experience when performing information retrieval tasks. Further, it showed that cognitive ability and previous technology experience significantly affected perceived disorientation in these searches. These results argue for the inclusion of metrics regarding cognitive ability and previous technology experience when analyzing user satisfaction and performance in Internet based-studies.","Michael Crabb, Vicki Hanson","cognitive psychology, hci, older adults, search strategies, web search",193,200
10.1145/2513383.2513441,ASSETS,2013,Comparing native signers' perception of American Sign Language animations and videos via eye tracking, ,"Animations of American Sign Language (ASL) have accessibility benefits for signers with lower written-language literacy. Our lab has conducted prior evaluations of synthesized ASL animations: asking native signers to watch different versions of animations and answer comprehension and subjective questions about them. Seeking an alternative method of measuring users' reactions to animations, we are now investigating the use of eye tracking to understand how users perceive our stimuli. This study quantifies how the eye gaze of native signers varies when they view: videos of a human ASL signer or synthesized animations of ASL (of different levels of quality). We found that, when viewing videos, signers spend more time looking at the face and less frequently move their gaze between the face and body of the signer. We also found correlations between these two eye-tracking metrics and participants' responses to subjective evaluations of animation-quality. This paper provides methodological guidance for how to design user studies evaluating sign language animations that include eye tracking, and it suggests how certain eye-tracking metrics could be used as an alternative or complimentary form of measurement in evaluation studies of sign language animation.","Hernisa Kacorri, Allen Harper, Matt Huenerfauth","accessibility technology for people who are deaf, american sign language, animation, eye tracking, user study",1,8
10.1145/2513383.2513392,ASSETS,2013,Eyes-free yoga,an exergame using depth cameras for blind &#38; low vision exercise,"People who are blind or low vision may have a harder time participating in exercise classes due to inaccessibility, travel difficulties, or lack of experience. Exergames can encourage exercise at home and help lower the barrier to trying new activities, but there are often accessibility issues since they rely on visual feedback to help align body positions. To address this, we developed Eyes-Free Yoga, an exergame using the Microsoft Kinect that acts as a yoga instructor, teaches six yoga poses, and has customized auditory-only feedback based on skeletal tracking. We ran a controlled study with 16 people who are blind or low vision to evaluate the feasibility and feedback of Eyes-Free Yoga. We found participants enjoyed the game, and the extra auditory feedback helped their understanding of each pose. The findings of this work have implications for improving auditory-only feedback and on the design of exergames using depth cameras.","Kyle Rector, Cynthia Bennett, Julie Kientz","accessibility, audio feedback, exergames, eyes-free, health, kinect, video games, visual impairments, yoga",1,8
10.1145/2049536.2049574,ASSETS,2011,On the intelligibility of fast synthesized speech for individuals with early-onset blindness, ,"People with visual disabilities increasingly use text-to-speech synthesis as a primary output modality for interaction with computers. Surprisingly, there have been no systematic comparisons of the performance of different text-to-speech systems for this user population. In this paper we report the results of a pilot experiment on the intelligibility of fast synthesized speech for individuals with early-onset blindness. Using an open-response recall task, we collected data on four synthesis systems representing two major approaches to text-to-speech synthesis: formant-based synthesis and concatenative unit selection synthesis. We found a significant effect of speaking rate on intelligibility of synthesized speech, and a trend towards significance for synthesizer type. In post-hoc analyses, we found that participant-related factors, including age and familiarity with a synthesizer and voice, also affect intelligibility of fast synthesized speech.","Amanda Stent, Ann Syrdal, Taniya Mishra",text-to-speech,211,218
10.1145/2384916.2384920,ASSETS,2012,Thematic organization of web content for distraction-free text-to-speech narration, ,"People with visual disabilities, especially those who are blind, have digital content narrated to them by text-to-speech (TTS) engines (e.g., with the help of screen readers). Naively narrating web pages, particularly the ones consisting of several diverse pieces (e.g., news summaries, opinion pieces, taxonomy, ads), with TTS engines without organizing them into thematic segments will make it very difficult for the blind user to mentally separate out and comprehend the essential elements in a segment, and the effort to do so can cause significant cognitive stress. One can alleviate this difficulty by segmenting web pages into thematic pieces and then narrating each of them separately. Extant segmentation methods typically segment web pages using visual and structural cues. The use of such cues without taking into account the semantics of the content, tends to produce ""impure"" segments containing extraneous material interspersed with the essential elements. In this paper, we describe a new technique for identifying thematic segments by tightly coupling visual, structural, and linguistic features present in the content. A notable aspect of the technique is that it produces segments with very little irrelevant content. Another interesting aspect is that the clutter-free main content of a web page, that is produced by the Readability tool and the ""Reader"" feature of the Safari browser, emerges as a special case of the thematic segments created by our technique. We provide experimental evidence of the effectiveness of our technique in reducing clutter. We also describe a user study with 23 blind subjects of its impact on web accessibility.","Muhammad Islam, Faisal Ahmed, Yevgen Borodin, I.V. Ramakrishnan","blind users, clustering, screen readers, segmentation, singular value decomposition",17,24
10.1145/2384916.2384946,ASSETS,2012,"""So that's what you see""",building understanding with personalized simulations of colour vision deficiency,"Colour vision deficiencies (CVD) affect the everyday lives of a large number of people, but it is difficult for others - even friends and family members - to understand the experience of having CVD. Simulation tools can help provide this experience; however, current simulations are based on general models that have several limitations, and therefore cannot accurately reflect the perceptual capabilities of most individuals with reduced colour vision. To address this problem, we have developed a new simulation approach that is based on a specific empirical model of the actual colour perception abilities of a person with CVD. The resulting simulation is therefore a more exact representation of what a particular person with CVD actually sees. We tested the new approach in two ways. First, we compared its accuracy with that of the existing models, and found that the personalized simulations were significantly more accurate than the old method. Second, we asked pairs of participants (one with CVD, and one close friend or family member without CVD) to discuss images of everyday scenes that had been simulated with the CVD person's particular model. We found that the personalized simulations provided new insights into the details of the CVD person's experience. The personalized-simulation approach shows great promise for improving understanding of CVD (and potentially other conditions) for people with ordinary perceptual abilities.","David Flatla, Carl Gutwin","colour vision deficiency, colour vision simulation",167,174
10.1145/1878803.1878832,ASSETS,2010,Click on bake to get cookies,guiding word-finding with semantic associations,"It is challenging to navigate a dictionary consisting of thousands of entries in order to select appropriate words for building communication. This is particularly true for people with lexical access disorders like those present in aphasia. We make vocabulary navigation and word-finding easier by building a vocabulary network where links between words reflect human judgments of semantic relatedness. We report the results from a user study with people with aphasia that evaluated how our system (called ViVA) performs compared to a widely used vocabulary access system in which words are organized hierarchically into common categories and subcategories. The results indicate that word retrieval is significantly better with ViVA, but finding the first word to start a communication is still problematic and requires further investigation.","Sonya Nikolova, Marilyn Tremaine, Perry Cook","adaptive user interfaces, aphasia, assistive communication, semantic networks, visual vocabularies",155,162
10.1145/3234695.3236363,ASSETS,2018,Turn Right,Analysis of Rotation Errors in Turn-by-Turn Navigation for Individuals with Visual Impairments,"Navigation assistive technologies aim to improve the mobility of blind or visually impaired people. In particular, turn-by-turn navigation assistants provide sequential instructions to enable autonomous guidance towards a destination. A problem frequently addressed in the literature is to obtain accurate position and orientation of the user during such guidance. An orthogonal challenge, often overlooked in the literature, is how precisely navigation instructions are followed by users. In particular, imprecisions in following rotation instructions lead to rotation errors that can significantly affect navigation. Indeed, a relatively small error during a turn is amplified by the following frontal movement and can lead the user towards incorrect or dangerous paths. In this contribution, we study rotation errors and their effect on turn-by-turn guidance for individuals with visual impairments. We analyze a dataset of indoor trajectories of 11 blind participants guided along three routes through a multi-story shopping mall using NavCog, a turn-by-turn smartphone navigation assistant. We find that participants extend rotations by 17&#186; on average. The error is not proportional to the expected rotation; instead, it is accentuated for ""slight turns"" (22.5&#186;-60&#186;), while ""ample turns"" (60&#186;-120&#186;) are consistently approximated to 90&#186;. We generalize our findings as design considerations for engineering navigation assistance in real-world scenarios.","Dragan Ahmetovic, Uran Oh, Sergio Mascetti, Chieko Asakawa","navigation assistive technologies, orientation and mobility, visual impairments and blindness",333,339
10.1145/3308561.3353806,ASSETS,2019,DarkReader&#58; Bridging the Gap Between Perception and Reality of Power Consumption in Smartphones for Blind Users, ,"This paper presents a user study with 10 blind participants to understand their perception of power consumption in smartphones. We found that a widely used power saving mechanism for smartphones--pressing the power button to put the smartphone to sleep--has a serious usability issue for blind screen reader users. Among other findings, our study also unearthed several usage patterns and misconceptions of blind users that contribute to excessive battery drainage. Informed by the first user study, this paper proposes DarkReader, a screen reader developed in Android that bridges users' perception of power consumption to reality. DarkReader darkens the screen by truly turning it off, but allows users to interact with their smartphones. A second user study with 10 blind participants shows that participants perceived no difference in completion times in performing routine tasks using DarkReader and default screen reader. Yet DarkReader saves 24% to 52% power depending on tasks and screen brightness.","Jian Xu, Syed Masum Billah, Roy Shilkrot, Aruna Balasubramanian","android, battery, blind, brightness, curtain mode, ios, low-power, privacy, screen, screen readers, shoulder-surfing, talkback, vision impairment",96,104
10.1145/2700648.2809864,ASSETS,2015,The Invisible Work of Accessibility,How Blind Employees Manage Accessibility in Mixed-Ability Workplaces,"Over the past century, people who are blind and their allies have developed successful public policies and technologies in support of creating more accessible workplaces. However, simply creating accessible technologies does not guarantee that these will be available or adopted. Because much work occurs within shared workspaces, decisions about assistive technology use may be mediated by social interactions with, and expectations of, sighted coworkers. We present findings from a qualitative field study of five workplaces from the perspective of blind employees. Although all participants were effective employees, they expressed that working in a predominantly sighted office environment produces impediments to a blind person's independence and to their integration as an equal coworker. We describe strategies employed by our participants to create and maintain an accessible workplace and present suggestions for future technology that better supports blind workers as equal peers in the workplace.","Stacy Branham, Shaun Kane","assistive technology, blindness, collaborative accessibility, vision impairment, workplace",163,171
10.1145/3308561.3353778,ASSETS,2019,Defining Problems of Practices to Advance Inclusive Tactile Media Consumption and Production, ,"Tactile media are important resources for people who are blind or have low vision (BLV) to access visual or graphical information as well as to develop tactile acuity. In this paper we present findings from a social learning design that was guided by the question, ""What are the factors or issues that impact the tactile media consumption and production practices of BLV and other invested stakeholders?"" From data collected during three Tactile Arts and Graphics Symposia-that gathered (N=64) BLV and sighted practitioners, researchers, and educators invested making information and experiences accessible through touch-we identified 34 issues that fall under four problems of practice, that impact tactile media practices. Based on these four problems of practice (and 34 underlying issues), we share recommendations to support the development of socio-technical systems that improve tactile access to information and full inclusion of people who are BLV in tactile media consumption and production.","Abigale Stangl, Ann Cunningham, Lou Ann Blake, Tom Yeh","accessible art, blind, consumption, inclusion, low vision, problems of practice, production, social learning, tactile art, tactile graphics, tactile literacy., tactile media, visually impaired",329,341
10.1145/2700648.2809847,ASSETS,2015,Zebra Crossing Spotter,Automatic Population of Spatial Databases for Increased Safety of Blind Travelers,"In this paper we propose a computer vision-based technique that mines existing spatial image databases for discovery of zebra crosswalks in urban settings. Knowing the location of crosswalks is critical for a blind person planning a trip that includes street crossing. By augmenting existing spatial databases (such as Google Maps or OpenStreetMap) with this information, a blind traveler may make more informed routing decisions, resulting in greater safety during independent travel. Our algorithm first searches for zebra crosswalks in satellite images; all candidates thus found are validated against spatially registered Google Street View images. This cascaded approach enables fast and reliable discovery and localization of zebra crosswalks in large image datasets. While fully automatic, our algorithm could also be complemented by a final crowdsourcing validation stage for increased accuracy.","Dragan Ahmetovic, Roberto Manduchi, James Coughlan, Sergio Mascetti","autonomous navigation, crowdsourcing, orientation and mobility, satellite and street-level imagery, visual impairments and blindness",251,258
10.1145/3234695.3236343,ASSETS,2018,Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users, ,"Mobile, wearable, and other ubiquitous computing devices are increasingly creating a context in which conventional keyboard and screen-based inputs are being replaced in favor of more natural speech-based interactions. Digital personal assistants use speech to control a wide range of functionality, from environmental controls to information access. However, many deaf and hard-of-hearing users have speech patterns that vary from those of hearing users due to incomplete acoustic feedback from their own voices. Because automatic speech recognition (ASR) systems are largely trained using speech from hearing individuals, speech-controlled technologies are typically inaccessible to deaf users. Prior work has focused on providing deaf users access to aural output via real-time captioning or signing, but little has been done to improve users' ability to provide input to these systems' speech-based interfaces. Further, the vocalization patterns of deaf speech often make accurate recognition intractable for both automated systems and human listeners, making traditional approaches to mitigate ASR limitations, such as human captionists, less effective. To bridge this accessibility gap, we investigate the limitations of common speech recognition approaches and techniques---both automatic and human-powered---when applied to deaf speech. We then explore the effectiveness of an iterative crowdsourcing workflow, and characterize the potential for groups to collectively exceed the performance of individuals. This paper contributes a better understanding of the challenges of deaf speech recognition and provides insights for future system development in this space.","Raymond Fok, Harmanpreet Kaur, Skanda Palani, Martez Mott, Walter Lasecki","accessibility, automatic speech recognition, deaf and hard-of-hearing, deaf speech, human computation",57,67
10.1145/2049536.2049562,ASSETS,2011,Accessibility of 3D game environments for people with Aphasia,an exploratory study,"People with aphasia experience difficulties with all aspects of language and this can mean that their access to technology is substantially reduced. We report a study undertaken to investigate the issues that confront people with aphasia when interacting with technology, specifically 3D game environments. Five people with aphasia were observed and interviewed in twelve workshop sessions. We report the key themes that emerged from the study, such as the importance of direct mappings between users' interactions and actions in a virtual environment. The results of the study provide some insight into the challenges, but also the opportunities, these mainstream technologies offer to people with aphasia. We discuss how these technologies could be more supportive and inclusive for people with language and communication difficulties.","Julia Galliers, Stephanie Wilson, Sam Muscroft, Jane Marshall, Abi Roper, Naomi Cocks, Tim Pring","3d games, accessible interaction design, aphasia, stroke, virtual environments",139,146
10.1145/2982142.2982169,ASSETS,2016,Blind Photographers and VizSnap,A Long-Term Study,"This paper describes a long term user study in which 13 blind participants were asked to use a blind friendly iPhone app, VizSnap -- an app designed to assist blind people in organizing and browsing a photo library without sight -- for a total of two months. VizSnap records audio while the user is aiming the camera, and allows an optional voice memo to be recorded, to allow the user to give custom information to accompany the photo, as well as capturing time, date, and location the photo was taken. All this information is available to the user when browsing through VizSnap's photo library. The participants met with us every two weeks, in which we discuss general VizSnap usage, conduct a short user study with their photos, as well as upload all data that was gathered using VizSnap. The user study aims to determine whether accompanying audio, time, date, and location metadata assists in memory retrieval of photos by blind people. We found that in general, both ambient audio and voice memo are considered most helpful for memory retrieval.","Dustin Adams, Sri Kurniawan, Cynthia Herrera, Veronica Kang, Natalie Friedman","accessibility., blind users, mobile, photography",201,208
10.1145/2700648.2809846,ASSETS,2015,Exploring the Opportunities and Challenges with Exercise Technologies for People who are Blind or Low-Vision, ,"People who are blind or low-vision may have a harder time participating in exercise due to inaccessibility or lack of experience. We employed Value Sensitive Design (VSD) to explore the potential of technology to enhance exercise for people who are blind or low-vision. We conducted 20 semi-structured interviews about exercise and technology with 10 people who are blind or low-vision and 10 people who facilitate fitness for people who are blind or low-vision. We also conducted a survey with 76 people to learn about outsider perceptions of hypothetical exercise with people who are blind or low-vision. Based on our interviews and survey, we found opportunities for technology development in four areas: 1) mainstream exercise classes, 2) exercise with sighted guides, 3) rigorous outdoors activity, and 4) navigation of exercise spaces. Design considerations should include when and how to deliver auditory or haptic information based on exercise and context, and whether it is acceptable to develop less mainstream technologies if they enhance mainstream exercise. The findings of this work seek to inform the design of accessible exercise technologies.","Kyle Rector, Lauren Milne, Richard Ladner, Batya Friedman, Julie Kientz","accessibility, audio feedback, exercise, exergames, eyes-free, health, value sensitive design, visual impairments",203,214
10.1145/2982142.2982167,ASSETS,2016,Customizable 3D Printed Tactile Maps as Interactive Overlays, ,"Though tactile maps have been shown to be useful tools for visually impaired individuals, their availability has been limited by manufacturing and design costs. In this paper, we present a system that uses 3D printing to (1) make tactile maps more affordable to produce, (2) allow visually impaired individuals to independently design and customize maps, and (3) provide interactivity using widely available mobile devices. Our system consists of three parts: a web interface, a modeling algorithm, and an interactive touchscreen application. Our web interface, hosted at www.tactilemaps.net, allows visually impaired individuals to create maps of any location on the globe while specifying (1) what features to map, (2) how the features should be represented by textures, and (3) where to place markers and labels. Our modeling algorithm accommodates user specifications to create map models with (1) multiple layers of continuously varying textures and (2) markers of various geometric shapes or braille characters. Our interactive application uses a novel approach to 3D printing tactile maps using conductive filament to provide touchscreen overlays that allow users to dynamically interact with the maps on a wide range of mobile devices. This paper details the implementation of our system. We also present findings from a user study validating the usability of our mapping interface and the utility of the maps produced. Finally, we discuss the limitations of our current implementation and the plans we have to improve our system based on feedback from our user study and additional interviews.","Brandon Taylor, Anind Dey, Dan Siewiorek, Asim Smailagic","3d printing, tactile maps",71,79
10.1145/3132525.3132546,ASSETS,2017,Good Background Colors for Readers,A Study of People with and without Dyslexia,"The use of colors to enhance the reading of people with dyslexia have been broadly discussed and is often recommended, but evidence of the effectiveness of this approach is lacking. This paper presents a user study with 341 participants (89 with dyslexia) that measures the effect of using background colors on screen readability. Readability was measured via reading time and distance travelled by the mouse. Comprehension was used as a control variable. The results show that using certain background colors have a significant impact on people with and without dyslexia. Warm background colors, Peach, Orange and Yellow, significantly improved reading performance over cool background colors, Blue, Blue Grey and Green. These results provide evidence to the practice of using colored backgrounds to improve readability; people with and without dyslexia benefit, but people with dyslexia may especially benefit from the practice given the difficulty they have in reading in general.","Luz Rello, Jeffrey Bigham","background colors, dyslexia, readability, reading speed",72,80
10.1145/3132525.3132552,ASSETS,2017,Technology-Mediated Sight,A Case Study of Early Adopters of a Low Vision Assistive Technology,"A case study of early adopters of a head-mounted assistive device for low vision provides the basis for a sociotechnical analysis of technology-mediated sight. Our research complements recent work in HCI focused on designing, building, and evaluating the performance of assistive devices for low vision by highlighting psychosocial and adaptive aspects of digitally enhanced vision. Through a series of semi-structured interviews with users of the eSight 2.0 device and customer-facing employees of the eSight company, we sought to better understand the social and emotional impacts associated with adoption of this type of low-vision assistive technology. Four analytic themes emerged from our interviews: 1) assessing the value of assistive technology in real life, 2) negotiating social engagement, 3) boundaries of sight, and 4) attitudes toward and expectations of technology. We introduce the concept of multiplicities of vision to describe technology-mediated sight as being a form of skilled vision and neither fully-human nor fully-digital, but rather, assembled through a combination of social and technical affordances. We propose that instead of seeing low-vision users through a deficit model of sight, HCI designers have more to gain by viewing people with low vision as individuals with a distinct type of skilled vision that is both socially and technologically mediated.","Annuska Zolyomi, Anushree Shukla, Jaime Snyder","assistive technology, head-mounted systems, low vision, qualitative research",220,229
10.1145/3132525.3132533,ASSETS,2017,"The Effects of ""Not Knowing What You Don't Know"" on Web Accessibility for Blind Web Users", ,"Web accessibility and usability have been extensively studied for blind web users. The focus has generally been on making it technically possible for blind users to access content, or on helping to make the web more usable. This paper explores a challenge at the intersection of these two lenses, which is the effects of blind web users not knowing what they don't know. On the web, this often means that the user is having a problem completing a task, but does not know whether the problem is because the information is there and not accessible, whether the information is simply difficult to access, or whether the information is not present at all. We first discuss how this issue has manifested itself in other work in this space. We then present the results of a study with 30 sighted web users and 30 blind web users exploring the phenomenon, demonstrating that not knowing the source of a problem causes frustration and wastes time. We conclude with recommendations for future research to help understand and address this problem, as well as design implications for future technology that may assist non-visual web navigation.","Jeffrey Bigham, Irene Lin, Saiph Savage","accessibility, blind, screen reader, web",101,109
10.1145/3308561.3353807,ASSETS,2019,Gender and Help Seeking by Older Adults When Learning New Technologies, ,"A gender stereotype that has some basis in research is that men are more reluctant to ask for directions than women. We wanted to investigate whether this stereotype applies to technology-related contexts, affecting older adults' abilities to learn new technologies. To explore how help seeking and gender might relate for older adults, we conducted a controlled experiment with 36 individuals, of whom 18 identified as men and 18 identified as women, and observed how often they asked for help when learning new applications. We also conducted post-experiment interviews with participants. We found that although most participants stereotyped older men as being reluctant to ask for help in the interview, the gender difference was minimal in the experiment. Instead, individual differences had a greater effect: older participants took longer to complete tasks and participants with lower technology self-efficacy asked significantly more questions.","Rachel Franz, Leah Findlater, Barbara Barbosa Neves, Jacob Wobbrock","gender, help seeking, older adults, technology use",136,142
10.1145/2982142.2982177,ASSETS,2016,SlidePacer,A Presentation Delivery Tool for Instructors of Deaf and Hard of Hearing Students,"Following multimedia lectures in mainstream classrooms is challenging for deaf and hard-of-hearing (DHH) students, even when provided with accessibility services. Due to multiple visual sources of information (e.g. teacher, slides, interpreter), these students struggle to divide their attention among several simultaneous sources, which may result in missing important parts of the lecture; as a result, access to information is limited in comparison to their hearing peers, having a negative effect in their academic achievements. In this paper we propose a novel approach to improve classroom accessibility, which focuses on improving the delivery of multimedia lectures. We introduce SlidePacer, a tool that promotes coordination between instructors and sign language interpreters, creating a single instructional unit and synchronizing verbal and visual information sources. We conducted a user study with 60 participants on the effects of SlidePacer in terms of learning performance and gaze behaviors. Results show that SlidePacer is effective in providing increased access to multimedia information; however, we did not find significant improvements in learning performance. We finish by discussing our results and limitations of our user study, and suggest future research avenues that build on these insights.","Alessandra Brand&#227;o, Hugo Nicolau, Shreya Tadas, Vicki Hanson","deaf, interpreter, learning, lecture, multimedia, pace, presentation., visual sources",25,32
10.1145/2049536.2049546,ASSETS,2011,Navigation and obstacle avoidance help (NOAH) for older adults with cognitive impairment,a pilot study,"Many older adults with cognitive impairment are excluded from powered wheelchair use because of safety concerns. This leads to reduced mobility, and in turn, higher dependence on caregivers. In this paper, we describe an intelligent wheelchair that uses computer vision and machine learning methods to provide adaptive navigation assistance to users with cognitive impairment. We demonstrate the performance of the system in a user study with the target population. We show that the collision avoidance module of the system successfully decreases the number of collisions for all participants. We also show that the wayfinding module assists users with memory and vision impairments. We share feedback from the users on various aspects of the intelligent wheelchair system. In addition, we provide our own observations and insights on the target population and their use of intelligent wheelchairs. Finally, we suggest directions for future work.","Pooja Viswanathan, James Little, Alan Mackworth, Alex Mihailidis","cognitive impairment, collision avoidance, intelligent wheelchair, wayfinding assistance",43,50
10.1145/2661334.2661357,ASSETS,2014,The blind driver challenge,steering using haptic cues,"Loss of vision significantly impairs mobility, with blind individuals often relying on sighted individuals or public transportation to get around. Self-driving vehicles could significantly improve the mobility of blind people, but current legislation often requires a legal driver to be present in the vehicle who can take over in case of a malfunction. To enable blind people to eventually use a self-driving car independently, we present a steering interface that allows for steering a vehicle using haptic cues. User studies with six blind and sighted subjects identify what accuracy is required and possible using our interface to steer a vehicle on a track using a simulator. We investigate whether driving experience affects haptic steering performance and perform a qualitative study into the usability of our haptic steering interface.","Burkay Sucu, Eelke Folmer","haptics, mobility, steering., visual impairment",3,10
10.1145/2049536.2049548,ASSETS,2011,Understanding the computer skills of adult expert users with down syndrome,an exploratory study,"Recent survey research suggests that individuals with Down syndrome use computers for a variety of educational, communication, and entertainment activities. However, there has been no analysis of the actual computer knowledge and skills of employment-aged computer users with Down syndrome. We conducted an ethnographic observation that aims at examining the workplace-related computer skills of expert users with Down syndrome. The results show that expert users with Down syndrome have the ability to use computers for basic workplace tasks such as word processing, data entry, and communication.","Jonathan Lazar, Libby Kumin, Jinjuan Feng","assistive technology, cognitive impairment, down syndrome, employment, human-computer interaction, workplace technology",51,58
10.1145/2700648.2809857,ASSETS,2015,Evaluating Alternatives for Better Deaf Accessibility to Selected Web-Based Multimedia, ,"The proliferation of video and audio media on the Internet has created a distinct disadvantage for deaf Internet users. Despite technological and legislative milestones in recent decades in making television and movies more accessible, there has been less progress with online access. A major obstacle to providing captions for Internet media is the high cost of captioning and transcribing services. This paper reports on two studies that focused on multimedia accessibility for Internet users who were born deaf or became deaf at an early age. An initial study attempted to identify priorities for deaf accessibility improvement. A total of 20 deaf and hard-of-hearing participants were interviewed via videophone about their Internet usage and the issues that were the most frustrating. The most common theme was concern over a lack of accessibility for online news. In the second study, a total of 95 deaf and hard-of-hearing participants evaluated different caption styles, some of which were generated through automatic speech recognition. Results from the second study confirm that captioning online videos makes the Internet more accessible to the deaf users, even when the captions are automatically generated. However color-coded captions used to highlight confidence levels were found neither to be beneficial nor detrimental; yet when asked directly about the benefit of color-coding, participants strongly favored the concept.","Brent Shiver, Rosalee Wolfe","automatic speech recognition, captioning, deaf, multimedia accessibility, speech-to-text, web accessibility",231,238
10.1145/2700648.2809841,ASSETS,2015,Understanding the Challenges Faced by Neurodiverse Software Engineering Employees,Towards a More Inclusive and Productive Technical Workforce,"Technology workers are often stereotyped as being socially awkward or having difficulty communicating, often with humorous intent; however, for many technology workers with atypical cognitive profiles, such issues are no laughing matter. In this paper, we explore the hidden lives of neurodiverse technology workers, e.g., those with autism spectrum disorder (ASD), attention deficit hyperactivity disorder (ADHD), and/or other learning disabilities, such as dyslexia. We present findings from interviews with 10 neurodiverse technology workers, identifying the challenges that impede these employees from fully realizing their potential in the workplace. Based on the interview findings, we developed a survey that was taken by 846 engineers at a large software company. In this paper, we reflect on the differences between the neurotypical (N = 781) and neurodiverse (N = 59) respondents. Technology companies struggle to attract, develop, and retain talented software developers; our findings offer insight into how employers can better support the needs of this important worker constituency.","Meredith Morris, Andrew Begel, Ben Wiedermann","attention deficit hyperactivity disorder, autism spectrum disorder, neurodiversity, software development",173,184
10.1145/2384916.2384949,ASSETS,2012,Effect of presenting video as a baseline during an american sign language animation user study, ,"Animations of American Sign Language (ASL) have accessibility benefits for many signers with lower levels of written language literacy. Our lab has conducted several prior studies to evaluate synthesized ASL animations by asking native signers to watch different versions of animations and to answer comprehension and subjective questions about them. As an upper baseline, we used an animation of a virtual human carefully created by a human animator who is a native ASL signer. Considering whether to instead use videos of human signers as an upper baseline, we wanted to quantify how including a video upper baseline would affect how participants evaluate the ASL animations presented in a study. In this paper, we replicate a user study we conducted two years ago, with one difference: replacing our original animation upper baseline with a video of a human signer. We found that adding a human video upper baseline depressed the subjective Likert-scale scores that participants assign to the other stimuli (the synthesized animations) in the study when viewed side-by-side. This paper provides methodological guidance for how to design user studies evaluating sign language animations and facilitates comparison of studies that have used different upper baselines.","Pengfei Lu, Hernisa Kacorri","accessibility technology for people who are deaf, american sign language, animation, baseline, user study",183,190
10.1145/2513383.2513434,ASSETS,2013,Safe walking technology for people with dementia,what do <i>they</i> want?,"This paper presents an attempt to understand how safe walking technology can be designed to fit the needs of people with dementia. Taking inspiration from modern dementia care philosophy, and its emphasis on the individual with dementia, we have performed in-depth investigations of three persons' experiences of living with early-stage dementia. From interviews and co-design workshops with them and their family caregivers, we identified several factors that influence people with dementia's attitudes toward safe walking technology, and how they want the technology to assist them. Relevant factors include: The desire for control and self-management, the subjective experiences of symptoms, personal routines and skills, empathy for care-givers, and the local environment in which they live. Based on these findings, we argue there is a need to reconsider ""surveillance"" as a concept on which to base design of safe walking technology. We also discuss implications for design ethics.","Kristine Holb&#248;, Silje B&#248;thun, Yngve Dahl","assistive technology, dementia care, human values, participatory design",1,8
10.1145/1878803.1878837,ASSETS,2010,Leveraging proprioception to make mobile phones more accessible to users with visual impairments, ,"Accessing the advanced functions of a mobile phone is not a trivial task for users with visual impairments. They rely on screen readers and voice commands to discover and execute functions. In mobile situations, however, screen readers are not ideal because users may depend on their hearing for safety, and voice commands are difficult for a system to recognize in noisy environments. In this paper, we extend Virtual Shelves--an interaction technique that leverages proprioception to access application shortcuts--for visually impaired users. We measured the directional accuracy of visually impaired participants and found that they were less accurate than people with vision. We then built a functional prototype that uses an accelerometer and a gyroscope to sense its position and orientation. Finally, we evaluated the interaction and prototype by allowing participants to customize the placement of seven shortcuts within 15 regions. Participants were able to access shortcuts in their personal layout with 88.3% accuracy in an average of 1.74 seconds.","Frank Li, David Dearman, Khai Truong","accessibility, mobile devices, proprioception, visual impairments",187,194
10.1145/2513383.2513449,ASSETS,2013,"""Pray before you step out""",describing personal and situational blind navigation behaviors,"Personal navigation tools have greatly impacted the lives of people with vision impairments. As people with vision impairments often have different requirements for technology, it is important to understand users' ever-changing needs. We conducted a formative study exploring how people with vision impairments used technology to support navigation. Our findings from interviews with 30 adults with vision impairments included insights about experiences in Orientation & Mobility (O&M) training, everyday navigation challenges, helpful and unhelpful technologies, and the role of social interactions while navigating. We produced a set of categorical data that future technologists can use to identify user requirements and usage scenarios. These categories consist of <i>Personality</i> and <i>Scenario</i> attributes describing navigation behaviors of people with vision impairments. We demonstrate the usefulness of these attributes by introducing navigation-style personas backed by our data. This work demonstrates the complex choices individuals with vision impairments undergo when leaving their home, and the many factors that affect their navigation behavior.","Michele Williams, Amy Hurst, Shaun Kane","accessibility, assistive technology, navigation, persona, universal design, visually impaired",1,8
10.1145/3308561.3353799,ASSETS,2019,Revisiting Blind Photography in the Context of Teachable Object Recognizers, ,"For people with visual impairments, photography is essential in identifying objects through remote sighted help and image recognition apps. This is especially the case for teachable object recognizers, where recognition models are trained on user's photos. Here, we propose real-time feedback for communicating the location of an object of interest in the camera frame. Our audio-haptic feedback is powered by a deep learning model that estimates the object center location based on its proximity to the user's hand. To evaluate our approach, we conducted a user study in the lab, where participants with visual impairments (N=9) used our feedback to train and test their object recognizer in vanilla and cluttered environments. We found that very few photos did not include the object (2% in the vanilla and 8% in the cluttered) and the recognition performance was promising even for participants with no prior camera experience. Participants tended to trust the feedback even though they know it can be wrong. Our cluster analysis indicates that better feedback is associated with photos that include the entire object. Our results provide insights into factors that can degrade feedback and recognition performance in teachable interfaces.","Kyungjun Lee, Jonggi Hong, Simone Pimento, Ebrima Jarjue, Hernisa Kacorri","hand, object recognition, sonification, visual impairments",83,95
10.1145/2049536.2049557,ASSETS,2011,Assessing the deaf user perspective on sign language avatars, ,"Signing avatars have the potential to become a useful and even cost-effective method to make written content more accessible for Deaf people. However, avatar research is characterized by the fact that most researchers are not members of the Deaf community, and that Deaf people as potential users have little or no knowledge about avatars. Therefore, we suggest two well-known methods, focus groups and online studies, as a two-way information exchange between research and the Deaf community. Our aim was to assess signing avatar acceptability, shortcomings of current avatars and potential use cases. We conducted two focus group interviews (N=8) and, to quantify important issues, created an accessible online user study(N=317). This paper deals with both the methodology used and the elicited opinions and criticism. While we found a positive baseline response to the idea of signing avatars, we also show that there is a statistically significant increase in positive opinion caused by participating in the studies. We argue that inclusion of Deaf people on many levels will foster acceptance as well as provide important feedback regarding key aspects of avatar technology that need to be improved.","Michael Kipp, Quan Nguyen, Alexis Heloir, Silke Matthes","accessibility technology for deaf people, german sign language, sign language synthesis",107,114
10.1145/2661334.2661359,ASSETS,2014,Path-guided indoor navigation for the visually impaired using minimal building retrofitting, ,"One of the common problems faced by visually impaired people is of independent path-based mobility in an unfamiliar indoor environment. Existing systems do not provide active guidance or are bulky, expensive and hence are not socially apt. In this paper, we present the design of an omnipresent cellphone based active indoor wayfinding system for the visually impaired. Our system provides step-by-step directions to the destination from any location in the building using minimal additional infrastructure. The carefully calibrated audio, vibration instructions and the small wearable device helps the user to navigate efficiently and unobtrusively. Results from a formative study with five visually impaired individuals informed the design of the system. We then deployed the system in a building and field tested it with ten visually impaired users. The comparison of the quantitative and qualitative results demonstrated that the system is useful and usable, but can still be improved.",Dhruv Jain,"indoor navigation, visual impairment",225,232
10.1145/3132525.3132554,ASSETS,2017,Narratives of Older Adults with Mild Cognitive Impairment and Their Caregivers, ,"The design of assistive technology is dictated by the narratives surrounding a particular impairment and its impact on one's life. This in turn affects the perceptions of the users - both the role of technology as well as their own sense of identity. In the case of those with a mild cognitive impairment, a precursor to dementia, home-based technologies that are disability and change focused shape the identity of the very people they are to help - reifying their dependency and their loss of self. With this study, we set out to better understand the narratives of people living with a mild cognitive impairment as well as their partners that live with them and provide care. Within this investigation, we uncovered the predominance of a disease-focused narrative - one that laments loss of identity and the struggles of daily care. However, we also uncovered a different narrative centered on the role of technology to provide support within the dyads' life. These technology narratives were evidence of a need to support the biopsychosocial aspects of autonomy for both parties and improved relationships. We use our findings to further discuss the driving force behind design goals for home-based technologies for those with a mild cognitive impairment.","Galina Madjaroff, Helena Mentis","cognitive impairment, dementia, home-based technology",140,149
10.1145/1878803.1878840,ASSETS,2010,Introducing multimodal paper-digital interfaces for speech-language therapy, ,"After a stroke or brain injury, it may be more difficult to understand language and communicate with others. Speech-language therapy may help an individual regain language and cope with changes in their communication abilities. Our research examines the process of speech-language therapy with an emphasis on the practices of therapists working with adults with aphasia and apraxia of speech. This paper presents findings from field work undertaken to inform the design of a mixed paper-digital interface prototype using multimodal digital pens. We describe and analyze therapists' initial reactions to the system and present two case studies of use by older adults undergoing speech-language therapy. We discuss the utility of multimodal paper-digital interfaces to assist therapy and describe our vision of a system to help therapists independently create custom interactive paper materials for their clients.","Anne Piper, Nadir Weibel, James Hollan","communication, multimodal interaction, older adults, pen-based computing, speech-language therapy",203,210
10.1145/3234695.3236345,ASSETS,2018,Exploring Aural and Haptic Feedback for Visually Impaired People on a Track,A Wizard of Oz Study,"Access to a variety of exercises is important for maintaining a healthy lifestyle. This variety includes physical activity in public spaces. A 400-meter jogging track is not accessible because it provides solely visual cues for people to remain in their lane. As a first step toward making exercise spaces accessible, we conducted an ecologically valid Wizard of Oz study to compare the accuracy and user experience of human guide, verbal, wrist vibration, and head beat feedback while people walked around the track. The technology conditions did not affect accuracy, but the order of preference was human guide, verbal, wrist vibration, and head beat. Participants had a difficult time perceiving vibrations when holding their cane or guide dog, and lower frequency sounds made it difficult to focus on their existing navigation strategies.","Kyle Rector, Rachel Bartlett, Sean Mullan","accessibility, audio feedback, eyes-free, outdoor exercise, vibration feedback, visual impairments",295,306
10.1145/1878803.1878830,ASSETS,2010,Relating computer tasks to existing knowledge to improve accessibility for older adults, ,"Routine computer tasks are often difficult for older adult computer users to learn and remember. People tend to learn new tasks by relating new concepts to existing knowledge. However, even for 'basic' computer tasks there is little, if any, existing knowledge on which older adults can base their learning. This paper investigates a custom file management interface that was designed to aid discovery and learnability by providing interface objects that are familiar to the user. A study was conducted which examined the differences between older and younger computer users when undertaking routine file management tasks using the standard Windows desktop as compared with the custom interface. Results showed that older adult computer users requested help more than ten times as often as younger users when using a standard windows/mouse configuration, made more mistakes and also required significantly more confirmations than younger users. The custom interface showed improvements over standard Windows/mouse, with fewer confirmations and less help being required. Hence, there is potential for an interface that closely mimics the real world to improve computer accessibility for older adults, aiding self-discovery and learnability.","Nic Hollinworth, Faustina Hwang","human computer interaction, older adults",147,154
10.1145/2049536.2049564,ASSETS,2011,The interplay between web aesthetics and accessibility, ,"Visual aesthetics enhances user experience in the context of the World Wide Web (Web). Accordingly, many studies report positive relationships between Web aesthetics and facets of user experience like usability and credibility, but does this hold for accessibility also? This paper describes an empirical investigation towards this end. The aesthetic judgements of 30 sighted Web users were elicited to understand what types of Web design come across as being visually pleasing. Participants judged 50 homepages based on Lavie and Tractinsky's classical and expressive Web aesthetics framework. A cross-section of the homepages were then manually audited for accessibility compliance by 11 Web accessibility experts who used a heuristic evaluation technique known as the Barrier Walkthrough (BW) method to check for accessibility barriers that could affect people with visual impairments. Web pages judged on the classical dimension as being visually clean showed significant correlations with accessibility, suggesting that visual cleanness may be a suitable proxy measure for accessibility as far as people with visual impairments are concerned. Expressive designs and other aesthetic dimensions showed no such correlation, however, demonstrating that an expressive or aesthetically pleasing Web design is not a barrier to accessibility.","Grace Mbipom, Simon Harper","user experience, visual aesthetics, web accessibility",147,154
10.1145/3132525.3132556,ASSETS,2017,DroneNavigator,Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers,"Although a large number of navigation support systems for visually impaired people have been proposed in the past, navigating through unknown environments is still a major challenge for visually impaired travelers. Existing systems provide navigation information through headphones, speakers or tactile actuators. In this paper, we propose to use small lightweight quadcopters instead to provide navigation information for people with visual impairments. Using a leashed or free-floating quadcopter, the user is navigated by the distinct sound that the quadcopter emits and a haptic stimulus provided by the leash. In a user with 14 visually impaired participants, we compared leashed quadcopter navigation, free-floating quadcopter navigation, and traditional audio navigation. The results show that compared to audio navigation, participants navigate significantly faster with a free-floating quadcopter and make fewer navigation errors using the quadcopter navigation methods.","Mauro Avila Soto, Markus Funk, Matthias Hoppe, Robin Boldt, Katrin Wolf, Niels Henze","drones, navigation aid, quadcopter, visual impairments",300,304
10.1145/2700648.2809850,ASSETS,2015,A Spellchecker for Dyslexia, ,"Poor spelling is a challenge faced by people with dyslexia throughout their lives. Spellcheckers are therefore a crucial tool for people with dyslexia, but current spellcheckers do not detect real-word errors, which are a common type of errors made by people with dyslexia. Real-word errors are spelling mistakes that result in an unintended but real word, for instance, form instead of from. Nearly 20% of the errors that people with dyslexia make are real-word errors. In this paper, we introduce a system called Real Check that uses a probabilistic language model, a statistical dependency parser and Google n-grams to detect real-world errors. We evaluated Real Check on text written by people with dyslexia, and showed that it detects more of these errors than widely used spellcheckers. In an experiment with 34 people (17 with dyslexia), people with dyslexia corrected sentences more accurately and in less time with Real Check.","Luz Rello, Miguel Ballesteros, Jeffrey Bigham","dyslexia, real-word errors, spellchecker, spelling errors",39,47
10.1145/2661334.2661453,ASSETS,2014,Headlock,a wearable navigation aid that helps blind cane users traverse large open spaces,"Traversing large open spaces is a challenging task for blind cane users, as such spaces are often devoid of tactile features that can be followed. Consequently, in such spaces cane users may veer from their intended paths. Wearable devices have great potential for assistive applications for users who are blind as they typically feature a camera and support hands and eye free interaction. We present HEADLOCK; a navigation aid for an optical head-mounted display that helps blind users traverse large open spaces by letting them lock onto a salient landmark across the space, such as a door, and then providing audio feedback to guide the user towards the landmark. A user study with 8 blind users evaluated the usability and effectiveness of two types of audio feedback (sonification and text-to-speech) for guiding a user across an open space to a doorway. Qualitative results are reported, which may inform the design of assistive wearable technology for users who are blind.","Alexander Fiannaca, Ilias Apostolopoulous, Eelke Folmer","head-mounted display, mobility, navigation, sonification., veering, visual impairment, wearable computing",19,26
10.1145/2513383.2513440,ASSETS,2013,Exploring the use of speech input by blind people on mobile devices, ,"Much recent work has explored the challenge of nonvisual text entry on mobile devices. While researchers have attempted to solve this problem with gestures, we explore a different modality: speech. We conducted a survey with 169 blind and sighted participants to investigate how often, what for, and why blind people used speech for input on their mobile devices. We found that blind people used speech more often and input longer messages than sighted people. We then conducted a study with 8 blind people to observe how they used speech input on an iPod compared with the on-screen keyboard with VoiceOver. We found that speech was nearly 5 times as fast as the keyboard. While participants were mostly satisfied with speech input, editing recognition errors was frustrating. Participants spent an average of 80.3% of their time editing. Finally, we propose challenges for future work, including more efficient eyes-free editing and better error detection methods for reviewing text.","Shiri Azenkot, Nicole Lee","dictation, eyes-free, mobile devices, text entry",1,8
10.1145/3132525.3132537,ASSETS,2017,Development and Theoretical Evaluation of Optimized Phonemic Interfaces, ,"In this paper, optimized communication interfaces in which users select phonemes (sounds) instead of letters or whole words are presented and evaluated. Optimization is based on phoneme transition likelihoods (i.e., the probability of transitioning from one phoneme to another in a particular communication corpus), similar to letter-to-letter transition likelihoods used to optimize orthographic interfaces. However, it is unknown to what extent phoneme transition likelihoods vary by corpus, nor how optimizing based on different corpora affects the final interface efficiency. Here we used computational evaluations to compare phoneme transition likelihoods between various phonemic corpora and optimize phonemic interfaces with each corpus. Each interface's efficiency was evaluated against all the corpora. Phoneme-to-phoneme transitions were highly correlated across corpora (r = 0.7-0.86). Optimization based on phoneme-to-phoneme transition likelihoods improved efficiency by around 20-30% compared to random phonemic layouts, regardless of the corpus used to optimize the interface. Optimizations using different corpora were similar, varying only by 3-5%. We conclude that, if possible, future phonemic interfaces should be optimized via a corpus from the intended user's communication. If this is not possible, however, optimization still improved efficiency using all testing corpora, suggesting that optimizing via any relevant corpus is indicated over other layouts.","Gabriel Cler, Cara Stepp","augmentative and alternative communication (aac), corpus-based optimizations, fitts' law, minimal movement capabilities, phonemic interface",230,239
10.1145/2982142.2982173,ASSETS,2016,Should I Trust It When I Cannot See It?,Credibility Assessment for Blind Web Users,"As users become increasingly more reliant on online resources to satisfy their information needs, care is needed to ensure that these resources are credible in nature, especially if a decision is to be taken based upon the information accessed. The credibility of a web site is known to be heavily influenced by its visual appearance. However, for individuals who are blind, challenges are often faced accessing these visual cues when using assistive technologies. In this paper, we describe an observational study to examine the strategies and workarounds developed by individuals who are blind to perform credibility assessments. These are compared with those used by sighted users. Findings from the study have highlighted the relationship between accessibility and credibility. The features used to form assessments non-visually have also been identified. Insights from the study can be used to support the design of highly credible interfaces for blind screen reader users.","Ali Abdolrahmani, Ravi Kuber","accessibility, blind, visually-impaired, web credibility",191,199
10.1145/1878803.1878824,ASSETS,2010,An evaluation of video intelligibility for novice american sign language learners on a mobile device, ,"Language immersion from birth is crucial to a child's language development. However, language immersion can be particularly challenging for hearing parents of deaf children to provide as they may have to overcome many difficulties while learning sign language. We intend to create a mobile device-based system to help hearing parents learn sign language. The first step is to understand what level of detail (i.e., resolution) is necessary for novice signers to learn from video of signs. In this paper we present the results of a study designed to evaluate the ability of novices learning sign language to ascertain the details of a particular sign based on video presented on a mobile device. Four conditions were presented. Three conditions involve manipulation of video resolution (low, medium, and high). The fourth condition employs insets showing the sign handshapes along with the high resolution video. Subjects were tested on their ability to emulate the given sign over 80 signs commonly used between parents and their young children. Although participants noticed a reduction in quality in the low resolution condition, there was no significant effect of condition on ability to generate the sign. Sign difficulty had a significant correlation with ability to correctly reproduce the sign. Although the inset handshape condition did not improve the participants' ability to emulate the signs correctly, participant feedback provided insight into situations where insets would be more useful, as well as further suggestions to improve video intelligibility. Participants were able to reproduce even the most complex signs tested with relatively high accuracy.","Kimberly Weaver, Thad Starner, Harley Hamilton","american sign language, computer assisted language learning, mobile devices",107,114
10.1145/1878803.1878825,ASSETS,2010,A web-based user survey for evaluating power saving strategies for deaf users of mobileASL, ,"MobileASL is a video compression project for two-way, real-time video communication on cell phones, allowing Deaf people to communicate in the language most accessible to them, American Sign Language. Unfortunately, running MobileASL quickly depletes a full battery charge in a few hours. Previous work on MobileASL investigated a method called <i>variable frame rate (VFR)</i> to increase the battery duration. We expand on this previous work by creating two new power saving algorithms, <i>variable spatial resolution (VSR)</i>, and the application of both VFR and VSR. These algorithms extend the battery life by altering the temporal and/or spatial resolutions of video transmitted on MobileASL. We found that implementing only VFR extended the battery life from 284 minutes to 307 minutes; implementing only VSR extended the battery life to 306 minutes, and implementing both VFR and VSR extended the battery life to 315 minutes. We evaluated all three algorithms by creating a linguistically accessible online survey to investigate Deaf people's perceptions of video quality when these algorithms were applied. In our survey results, we found that VFR produces perceived video choppiness and VSR produces perceived video blurriness; however, a surprising finding was that when both VFR and VSR are used together, they largely ameliorate the choppiness and blurriness perceived, <i>i.e.</i>, they each improve the use of the other. This is a useful finding because using VFR and VSR together saves the most battery life.","Jessica Tran, Tressa Johnson, Joy Kim, Rafael Rodriguez, Sheri Yin, Eve Riskin, Richard Ladner, Jacob Wobbrock","american sign language, battery power consumption, deaf community, deaf culture, encoding algorithms, mobile phones, video compression, web-based user survey",115,122
10.1145/3132525.3132547,ASSETS,2017,Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment, ,"Mobile accessibility is often a property considered at the level of a single mobile application (app), but rarely on a larger scale of the entire app ""ecosystem,"" such as all apps in an app store, their companies, developers, and user influences. We present a novel conceptual framework for the accessibility of mobile apps inspired by epidemiology. It considers apps within their ecosystems, over time, and at a population level. Under this metaphor, ""inaccessibility"" is a set of diseases that can be viewed through an epidemiological lens. Accordingly, our framework puts forth notions like risk and protective factors, prevalence, and health indicators found within a population of apps. This new framing offers terminology, motivation, and techniques to reframe how we approach and measure app accessibility. It establishes how app accessibility can benefit from multi-factor, longitudinal, and population-based analyses. Our epidemiology-inspired conceptual framework is the main contribution of this work, intended to provoke thought and inspire new work enhancing app accessibility at a systemic level. In a preliminary exercising of our framework, we perform an analysis of the prevalence of common determinants or accessibility barriers. We assess the health of a stratified sample of 100 popular Android apps using Google's Accessibility Scanner. We find that 100% of apps have at least one of nine accessibility errors and examine which errors are most common. A preliminary analysis of the frequency of co-occurrences of multiple errors in a single app is also presented. We find 72% of apps have five or six errors, suggesting an interaction among different errors or an underlying influence.","Anne Ross, Xiaoyi Zhang, James Fogarty, Jacob Wobbrock","accessibility assessment, app accessibility, conceptual framework, epidemiology, mobile accessibility, mobile computing",2,11
10.1145/2661334.2661361,ASSETS,2014,A large user pool for accessibility research with representative users, ,"A critical element of accessibility research is the exploration and evaluation of ideas with representative users. However, it is often difficult to recruit these users, particularly in a timely manner. In this paper we report on the establishment of a large user pool created to facilitate accessibility research through recruiting sizeable numbers of older adults potentially interested in taking part in research studies about technology. Lessons learned from creating and maintaining this pool of individuals are reported.","Marianne Dee, Vicki Hanson","research participation, user studies",35,42
10.1145/1878803.1878838,ASSETS,2010,Autonomous navigation through the city for the blind, ,"Autonomous navigation in the city has become a necessity for people with visual disabilities, due to the fact that they now enjoy a higher degree of social insertion. As such, several technological solutions seek to assist with this autonomy. In this work, we present a study on the effect of the use of an easy-to-access, audio-based GPS software program on navigation through open spaces, and in particular on the stimulation of orientation and mobility skills in blind people. Results show that the use of the audio-based GPS software allowed blind users to be able to get to various destinations without the need for prior information on the environment, favoring the navigation of blind people in unfamiliar contexts, stimulating the use of different orientation and mobility skills, and finally providing help to users that habitually navigate spaces in the city only in the company of other people.","Jaime S&#225;nchez, Natalia de la Torre","assistive technology, mobile technology, orientation and mobility, people who are blind",195,202
10.1145/3234695.3236349,ASSETS,2018,Interactiles,3D Printed Tactile Interfaces to Enhance Mobile Touchscreen Accessibility,"The absence of tactile cues such as keys and buttons makes touchscreens difficult to navigate for people with visual impairments. Increasing tactile feedback and tangible interaction on touchscreens can improve their accessibility. However, prior solutions have either required hardware customization or provided limited functionality with static overlays. Prior investigation of tactile solutions for large touchscreens also may not address the challenges on mobile devices. We therefore present Interactiles, a low cost, portable, and unpowered system that enhances tactile interaction on Android touchscreen phones. Interactiles consists of 3D-printed hardware interfaces and software that maps interaction with that hardware to manipulation of a mobile app. The system is compatible with the built-in screen reader without requiring modification of existing mobile apps. We describe the design and implementation of Interactiles, and we evaluate its improvement in task performance and the user experience it enables with people who are blind or have low vision.","Xiaoyi Zhang, Tracy Tran, Yuqian Sun, Ian Culhane, Shobhit Jain, James Fogarty, Jennifer Mankoff","3D printing, mobile accessibility, tactile interfaces, touchscreens, visual impairments",131,142
10.1145/2384916.2384947,ASSETS,2012,Evaluation of dynamic image pre-compensation forcomputer users with severe refractive error, ,"Visual distortion and blurring impede the efficient interaction between computers and their users. Visual problems can be caused by eye diseases, severe refractive errors or combinations of both. Several image enhancement methods based on contrast sensitivity have been used to help people with eye diseases (e.g., age-related macular degeneration and cataracts), whereas few methods have been designed for people with severe refractive errors. This paper describes a new pre-compensation method to counter the visual blurring caused by the severe refractive errors of a specific computer user. It preprocesses the pictorial information through dynamic pre-compensation in advance, aiming to present customized images on the basis of the ocular aberrations of the specific computer user. The new method improves the previous static pre-compensation method by updating the aberration data according to pupil size variations, in real-time. The real-time aberration data enable us to generate better suited pre-compensated images, as the pre-compensation model is updated dynamically. An empirical study was conducted to evaluate the efficiency of the new pre-compensation method, through an icon recognition test. From the results of statistical analysis, we found that participants achieved significantly higher accuracy levels in recognizing the icons with dynamic pre-compensation, than when viewing the original icons. The accuracy is also significantly boosted when the icons were processed with dynamic pre-compensation method, in comparison with the previous static pre-compensation method.","Jian Huang, Armando Barreto, Malek Adjouadi","icon recognition, image enhancement, image pre-compensation, ocular aberration, refractive error",175,182
10.1145/2700648.2809843,ASSETS,2015,Tracked Speech-To-Text Display,Enhancing Accessibility and Readability of Real-Time Speech-To-Text,"Deaf and Hard of Hearing (DHH) students are under-served and under-represented in education in part because they miss spoken classroom information, even with aural-to-visual accommodations, such as a real-time speech to text Display (SD). Most SD systems utilize a trained typist to transcribe the speech into text (speech-to-text) onto a display. Still, these students encounter significant but subtle barriers in following speech-to-text displays, especially when detailed visuals are used or when the speaker is fast or uses uncommon words. Hearing students can simultaneously watch the visuals and listen to the spoken explanation, while DHH students constantly look away from the SD to search and observe details in the classroom visuals. As a result, they spend less time watching the visuals and gain less information than their hearing peers. They can also fall behind in reading the speech-text. We discuss the implementation and evaluation of a real-time Tracked Speech-To-Text Display (TSD), that addresses these subtle barriers presented by SD systems. The TSD system minimizes the student's viewing distance between the Speech-to-Text Display and the speaker. This is done through tracking the presenter and displaying the speech-text at a fixed distance above the presenter. Our evaluation showed that students significantly preferred TSD over SD and reported that it was easier to follow the lecture. They liked being able to see both the teacher and speech-to-text, and being able to set the number of displayed lines.","Raja Kushalnagar, Gary Behm, Aaron Kelstone, Shareef Ali","accessible technology, deaf and hard of hearing users",223,230
10.1145/3234695.3236347,ASSETS,2018,Understanding the Power of Control in Autonomous Vehicles for People with Vision Impairment, ,"Autonomy and control are important themes in design for people with disabilities. With the rise in research in autonomous vehicle design, we investigate perceived differences in control for people with vision impairments in the use of semi- and fully autonomous vehicles. We conducted focus groups with 15 people with vision impairments. Each focus group included a design component asking participants to design voice-based and tactile solutions to problems identified by the group. We contribute a new perspective of independence in the context of control. We discuss the importance of driving for blind and low vision people, describe differences in perceptions of autonomous vehicles based on level of autonomy, and the use of assistive technology in vehicle operation and information gathering. Our findings guide the design of accessible autonomous transportation systems and existing navigation and orientation systems for people with vision impairments.","Robin Brewer, Vaishnav Kameswaran","autonomous vehicles, blind, control, low vision",185,197
10.1145/2049536.2049561,ASSETS,2011,Developing accessible TV applications, ,"The development of TV applications nowadays excludes users with certain impairments from interacting with and accessing the same type of contents as other users do. Developers are also not interested in developing new or different versions of applications targeting different user characteristics. In this paper we describe a novel adaptive accessibility approach on how to develop accessible TV applications, without requiring too much additional effort from the developers. Integrating multimodal interaction, adaptation techniques and the use of simulators in the design process, we show how to adapt User Interfaces to the individual needs and limitations of elderly users. For this, we rely on the identification of the most relevant impairment configurations among users in practical user-trials, and we draw a relation with user specific characteristics. We provide guidelines for more accessible and centered TV application development.","Jos&#233; Coelho, Carlos Duarte, Pradipta Biswas, Patrick Langdon","accessible applications, elderly, guide, multimodal, simulation",131,138
10.1145/3234695.3236348,ASSETS,2018,Interdependence as a Frame for Assistive Technology Research and Design, ,"In this paper, we describe interdependence for assistive technology design, a frame developed to complement the traditional focus on independence in the Assistive Technology field. Interdependence emphasizes collaborative access and people with disabilities' important and often understated contribution in these efforts. We lay the foundation of this frame with literature from the academic discipline of Disability Studies and popular media contributed by contemporary disability justice activists. Then, drawing on cases from our own work, we show how the interdependence frame (1) synthesizes findings from a growing body of research in the Assistive Technology field and (2) helps us orient to additional technology design opportunities. We position interdependence as one possible orientation to, not a prescription for, research and design practice--one that opens new design possibilities and affirms our commitment to equal access for people with disabilities.","Cynthia Bennett, Erin Brady, Stacy Branham","assistive technology design, interdependence",161,173
10.1145/3234695.3236360,ASSETS,2018,Volunteer-Based Online Studies With Older Adults and People with Disabilities, ,"There are few large-scale empirical studies with people with disabilities or older adults, mainly because recruiting partici&#173;pants with specific characteristics is even harder than recruit&#173;ing young and/or non-disabled populations. Analyzing four online experiments on LabintheWild with a total of 355,656 participants, we show that volunteer-based online experiments that provide personalized feedback attract large numbers of participants with diverse disabilities and ages and allow ro&#173;bust studies with these populations that replicate and extend the findings of prior laboratory studies. To find out what mo&#173;tivates people with disabilities to take part, we additionally analyzed participants' feedback and forum entries that discuss LabintheWild experiments. The results show that participants use the studies to diagnose themselves, compare their abilities to others, quantify potential impairments, self-experiment, and share their own stories -- findings that we use to inform design guidelines for online experiment platforms that adequately support and engage people with disabilities.","Qisheng Li, Krzysztof Gajos, Katharina Reinecke","elderly people, online experimentation, people with disabilities, volunteers",229,241
10.1145/2049536.2049556,ASSETS,2011,Evaluating importance of facial expression in american sign language and pidgin signed english animations, ,"Animations of American Sign Language (ASL) and Pidgin Signed English (PSE) have accessibility benefits for many signers with lower levels of written language literacy. In prior experimental studies we conducted evaluating animations of ASL, native signers gave informal feedback in which they critiqued the insufficient and inaccurate facial expressions of the virtual human character. While face movements are important for conveying grammatical and prosodic information in human ASL signing, no empirical evaluation of their impact on the understandability and perceived quality of ASL animations had previously been conducted. To quantify the suggestions of deaf participants in our prior studies, we experimentally evaluated ASL and PSE animations with and without various types of facial expressions, and we found that their inclusion does lead to measurable benefits for the understandability and perceived quality of the animations. This finding provides motivation for our future work on facial expressions in ASL and PSE animations, and it lays a novel methodological groundwork for evaluating the quality of facial expressions for conveying prosodic or grammatical information.","Matt Huenerfauth, Pengfei Lu, Andrew Rosenberg","accessibility technology for people who are deaf, american sign language, facial expression, pidgin signed english",99,106
10.1145/1878803.1878813,ASSETS,2010,Testability and validity of WCAG 2.0,the expertise effect,"Web Content Accessibility Guidelines 2.0 (WCAG 2.0) require that success criteria be tested by human inspection. Further, testability of WCAG 2.0 criteria is achieved if 80% of knowledgeable inspectors agree that the criteria has been met or not. In this paper we investigate the very core WCAG 2.0, being their ability to determine web content accessibility conformance. We conducted an empirical study to ascertain the testability of WCAG 2.0 success criteria when experts and non-experts evaluated four relatively complex web pages; and the differences between the two. Further, we discuss the validity of the evaluations generated by these inspectors and look at the differences in validity due to expertise. In summary, our study, comprising 22 experts and 27 non-experts, shows that approximately 50% of success criteria fail to meet the 80% agreement threshold; experts produce 20% false positives and miss 32% of the true problems. We also compared the performance of experts against that of non-experts and found that agreement for the non-experts dropped by 6%, false positives reach 42% and false negatives 49%. This suggests that in many cases WCAG 2.0 conformance cannot be tested by human inspection to a level where it is believed that at least 80% of knowledgeable human evaluators would agree on the conclusion. Why experts fail to meet the 80% threshold and what can be done to help achieve this level are the subjects of further investigation.","Giorgio Brajnik, Yeliz Yesilada, Simon Harper","evaluation, expertise, guideline, web accessibility",43,50
10.1145/3234695.3236338,ASSETS,2018,Designing an Animated Character System for American Sign Language, ,"Sign languages lack a standard written form, preventing millions of Deaf people from accessing text in their primary language. A major barrier to adoption is difficulty learning a system which represents complex 3D movements with stationary symbols. In this work, we leverage the animation capabilities of modern screens to create the first animated character system prototype for sign language, producing text that combines iconic symbols and movement. Using animation to represent sign movements can increase resemblance to the live language, making the character system easier to learn. We explore this idea through the lens of American Sign Language (ASL), presenting 1) a pilot study underscoring the potential value of an animated ASL character system, 2) a structured approach for designing animations for an existing ASL character system, and 3) a design probe workshop with ASL users eliciting guidelines for the animated character system design.","Danielle Bragg, Raja Kushalnagar, Richard Ladner","american sign language (asl), animation, character systems, deaf, design, literacy, reading and writing, si5s",282,294
10.1145/3132525.3132549,ASSETS,2017,Designing Interactions for 3D Printed Models with Blind People, ,"Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.","Lei Shi, Yuhang Zhao, Shiri Azenkot","elicitation, exploration behaviors, interactive 3d printed models, visually impairments",200,209
10.1145/2384916.2384938,ASSETS,2012,Understanding the role of age and fluid intelligence in information search, ,"In this study, we explore the role of age and fluid intelligence on the behavior of people looking for information in a real-world search space. Analyses of mouse moves, clicks, and eye movements provide a window into possible differences in both task strategy and performance, and allow us to begin to separate the influence of age from the correlated but isolable influence of cognitive ability. We found little evidence of differences in strategy between younger and older participants matched on fluid intelligence. Both performance and strategy differences were found between older participants having higher versus lower fluid intelligence, however, suggesting that cognitive factors, rather than age per se, exert the dominant influence. This underscores the importance of measuring and controlling for cognitive abilities in studies involving older adults.","Shari Trewin, John Richards, Vicki Hanson, David Sloan, Bonnie John, Cal Swart, John Thomas","age, cognition, eye-gaze, fluid intelligence, information search",119,126
10.1145/2700648.2809845,ASSETS,2015,"""But, I don't take steps""",Examining the Inaccessibility of Fitness Trackers for Wheelchair Athletes,"Wearable fitness devices have demonstrated the capacity to improve overall physical activity, which can lead to physical and mental health improvements as well as quality of life gains. Although wheelchair athletes who participate in adaptive sports are interested in using wearable fitness trackers to capture their activity, we have observed low adoption of wearable fitness trackers among wheelchair athletes. We interviewed five wheelchair athletes and three physical and occupational therapists to explore fitness activities, experience with wearable technology, and potential uses for wearable fitness devices. None of the wheelchair athletes we interviewed had previously used any wearable fitness devices, however four out of five were interested in tracking their physical activity. We present five thematic areas helpful for thinking about wearable computing systems and accessibility challenges that arise based on incorrect assumptions about the athletic community. We highlight opportunities for improving the impact and accessibility of fitness tracking technologies for wheelchair athletes. These opportunities include improving the analysis of data from existing sensors, instrumenting the custom equipment used by adaptive sport athletes, and revising the language used in the presentation of fitness data to create a more inclusive community of users.","Patrick Carrington, Kevin Chang, Helena Mentis, Amy Hurst","adaptive sports, fitness, wearable, wheelchair athletes",193,201
10.1145/3234695.3236337,ASSETS,2018,BrowseWithMe,An Online Clothes Shopping Assistant for People with Visual Impairments,"Our interviews with people who have visual impairments show clothes shopping is an important activity in their lives. Unfortunately, clothes shopping web sites remain largely inaccessible. We propose design recommendations to address online accessibility issues reported by visually impaired study participants and an implementation, which we call BrowseWithMe, to address these issues. BrowseWithMe employs artificial intelligence to automatically convert a product web page into a structured representation that enables a user to interactively ask the BrowseWithMe system what the user wants to learn about a product (e.g., What is the price? Can I see a magnified image of the pants?). This enables people to be active solicitors of the specific information they are seeking rather than passive listeners of unparsed information. Experiments demonstrate BrowseWithMe can make online clothes shopping more accessible and produce accurate image descriptions.","Abigale Stangl, Esha Kothari, Suyog Jain, Tom Yeh, Kristen Grauman, Danna Gurari","accessibility, artificial intelligence, human computer interaction",107,118
10.1145/2513383.2513437,ASSETS,2013,UbiBraille,designing and evaluating a vibrotactile Braille-reading device,"Blind people typically resort to audio feedback to access information on electronic devices. However, this modality is not always an appropriate form of output. Novel approaches that allow for private and inconspicuous interaction are paramount. In this paper, we present a vibrotactile reading device that leverages the users' Braille knowledge to read textual information. UbiBraille consists of six vibrotactile actuators that are used to code a Braille cell and communicate single characters. The device is able to simultaneously actuate the users' index, middle, and ring fingers of both hands, providing fast and mnemonic output. We conducted two user studies on UbiBraille to assess both character and word reading performance. Character recognition rates ranged from 54% to 100% and were highly character- and user-dependent. Indeed, participants with greater expertise in Braille reading/writing were able to take advantage of this knowledge and achieve higher accuracy rates. Regarding word reading performance, we investigated four different vibrotactile timing conditions. Participants were able to read entire words and obtained recognition rates up to 93% with the most proficient ones being able achieve a rate of 1 character per second.","Hugo Nicolau, Jo&#227;o Guerreiro, Tiago Guerreiro, Lu&#237;s Carri&#231;o","Braille, blind, finger, reading, vibrotactile, wearable",1,8
10.1145/2661334.2661370,ASSETS,2014,Design and evaluation of a networked game to supportsocial connection of youth with cerebral palsy, ,"Youth with cerebral palsy (CP) can experience social isolation, in part due to mobility limitations associated with CP. We show that networked video games can provide a venue for social interaction from the home. We address the question of how to design networked games that enhance social play among people with motor disabilities. We present Liberi, a networked game custom-designed for youth with CP. Liberi is designed to allow frictionless group formation, to balance for differences in player abilities, and to support a variety of play styles. A ten-week home-based study with ten participants showed the game to be effective in fostering social interaction among youth with CP.","Hamilton Hernandez, Mallory Ketcheson, Adrian Schneider, Zi Ye, Darcy Fehlings, Lauren Switzer, Virginia Wright, Shelly Bursick, Chad Richards, T.C. Nicholas Graham","cerebral palsy, game accessibility, video game design",161,168
10.1145/2049536.2049554,ASSETS,2011,We need to communicate!,helping hearing parents of deaf children learn american sign language,"Language immersion from birth is crucial to a child's language development. However, language immersion can be particularly challenging for hearing parents of deaf children to provide as they may have to overcome many difficulties while learning American Sign Language (ASL). We are in the process of creating a mobile application to help hearing parents learn ASL. To this end, we have interviewed members of our target population to gain understanding of their motivations and needs when learning sign language. We found that the most common motivation for parents learning ASL is better communication with their children. Parents are most interested in acquiring more fluent sign language skills through learning to read stories to their children.","Kimberly Weaver, Thad Starner","american sign language, computer assisted language learning, mobile devices",91,98
10.1145/3308561.3353800,ASSETS,2019,Autoethnography of a Hard of Hearing Traveler, ,"Travel experiences offer a diverse view into an individual's interactions with different cultures, societies, and places. In this paper, we present a 2.5-year autoethnographic travel account of a hard of hearing individual-Jain. Through retrospective journals and field notes, we reveal the tensions and nuances in his travel, including the magnified difficulty of social conversations, issues with navigating unfamiliar environments and cultural contexts, and changes in the relationship to personal assistive technologies. By exploring the longitudinal travel experiences of a single individual, we uncover evocative and personal insights rarely available through participant-based research methods. Based on these lived experiences and post hoc reflections, we present two design explorations of personalized technology the autoethnographer created for aiding his travel. Finally, we offer reflections for customized travel technologies for deaf and hard of hearing users, and methodological guidelines for performing first-person research in the context of disability.","Dhruv Jain, Audrey Desjardins, Leah Findlater, Jon Froehlich","accessibility, autobiographical design, autoethnography, deaf, hard of hearing, personalized technology, travel",236,248
10.1145/2049536.2049572,ASSETS,2011,Improving calibration time and accuracy for situation-specific models of color differentiation, ,"Color vision deficiencies (CVDs) cause problems in situations where people need to differentiate the colors used in digital displays. Recoloring tools exist to reduce the problem, but these tools need a model of the user's color-differentiation ability in order to work. Situation-specific models are a recent approach that accounts for all of the factors affecting a person's CVD (including genetic, acquired, and environmental causes) by using calibration data to form the model. This approach works well, but requires repeated calibration - and the best available calibration procedure takes more than 30 minutes. To address this limitation, we have developed a new situation-specific model of human color differentiation (called ICD-2) that needs far fewer calibration trials. The new model uses a color space that better matches human color vision compared to the RGB space of the old model, and can therefore extract more meaning from each calibration test. In an empirical comparison, we found that ICD-2 is 24 times faster than the old approach, and had small but significant gains in accuracy. The efficiency of ICD-2 makes it feasible for situation-specific models of individual color differentiation to be used in the real world.","David Flatla, Carl Gutwin","adaptation tools, color blindness, color differentiation, color vision deficiency (cvd), modeling",195,202
10.1145/3308561.3353804,ASSETS,2019,RoboGraphics,Dynamic Tactile Graphics Powered by Mobile Robots,"Tactile graphics are a common way to present information to people with vision impairments. Tactile graphics can be used to explore a broad range of static visual content but aren't well suited to representing animation or interactivity. We introduce a new approach to creating dynamic tactile graphics that combines a touch screen tablet, static tactile overlays, and small mobile robots. We introduce a prototype system called RoboGraphics and several proof-of-concept applications. We evaluated our prototype with seven participants with varying levels of vision, comparing the RoboGraphics approach to a flat screen, audio-tactile interface. Our results show that dynamic tactile graphics can help visually impaired participants explore data quickly and accurately.","Darren Guinness, Annika Muehlbradt, Daniel Szafir, Shaun Kane","accessibility, blindness, education, robots, tactile, tangible user interfaces",318,328
10.1145/2513383.2517032,ASSETS,2013,Audio-visual speech understanding in simulated telephony applications by individuals with hearing loss, ,"We present a study into the effects of the addition of a video channel, video frame rate, and audio-video synchrony, on the ability of people with hearing loss to understand spoken language during video telephone conversations. Analysis indicates that higher frame rates result in a significant improvement in speech understanding, even when audio and video are not perfectly synchronized. At lower frame rates, audio-video synchrony is critical: if the audio is perceived 100 ms ahead of video, understanding drops significantly; if on the other hand the audio is perceived 100 ms behind video, understanding does not degrade versus perfect audio-video synchrony. These findings are validated in extensive statistical analysis over two within-subjects experiments with 24 and 22 participants, respectively.","Linda Kozma-Spytek, Paula Tucker, Christian Vogler","audio-video synchronization, frame rate, hard of hearing, hearing loss, lipreading, speech understanding, telecommunications accessibility",1,8
10.1145/3308561.3353793,ASSETS,2019,Leveraging Participation,Supporting Skills Development of Young Adults with Intellectual Disability Using Social Media,"Young adults with intellectual disability are keen users of social media. However, there is little understanding about how their skills and participation in social media might be leveraged to support further skills development. Employing a participatory approach through workshops with eleven participants and interviews with eight parents, we investigated what skills young adults desire and how they might be able to leverage their participation in YouTube and Facebook to develop these skills. We found that young adults want to develop social skills of such as playing sports or learning languages, but that leveraging social media participation to do this goes beyond their typical use, and requires both collaborative support and accessible design. Based on these findings, we propose and discuss a collaboration-in-the-loop framework that integrates support through personal networks and an accessible user interface design. We conclude with a reflection on designing to leverage participation, interests and competencies to support people with intellectual disability.","Andrew Bayor, Laurianne Sitbon, Bernd Ploderer, Filip Bircanin, Stewart Koplick, Margot Brereton","competencies, intellectual disability, participation, skills development, social media, social skills, techshops",143,155
10.1145/2700648.2809865,ASSETS,2015,ForeSee,A Customizable Head-Mounted Vision Enhancement System for People with Low Vision,"Most low vision people have functional vision and would likely prefer to use their vision to access information. Recently, there have been advances in head-mounted displays, cameras, and image processing technology that create opportunities to improve the visual experience for low vision people. In this paper, we present ForeSee, a head-mounted vision enhancement system with five enhancement methods: Magnification, Contrast Enhancement, Edge Enhancement, Black/White Reversal, and Text Extraction; in two display modes: Full and Window. ForeSee enables users to customize their visual experience by selecting, adjusting, and combining different enhancement methods and display modes in real time. We evaluated ForeSee by conducting a study with 19 low vision participants who performed near- and far-distance viewing tasks. We found that participants had different preferences for enhancement methods and display modes when performing different tasks. The Magnification Enhancement Method and the Window Display Mode were popular choices, but most participants felt that combining several methods produced the best results. The ability to customize the system was key to enabling people with a variety of different vision abilities to improve their visual experience.","Yuhang Zhao, Sarit Szpiro, Shiri Azenkot","accessibility, augmented reality glasses, vision customization, visual impairment",239,249
10.1145/2700648.2809866,ASSETS,2015,Online News Videos,The UX of Subtitle Position,"Millions of people rely on subtitles when watching video content. The current change in media viewing behaviour involving computers has resulted in a large proportion of people turning to online sources as opposed to regular television for news information. This work analyses the user experience of viewing subtitled news videos presented as part of a web page. A lab-based user experiment was carried out with frequent subtitle users, focusing on determining whether changes in video dimension and subtitle location could affect the user experience attached to viewing subtitled content. A significant improvement in user experience was seen when changing the subtitle location from the standard position of within a video at the bottom to below the video clip. Additionally, participants responded positively when given the ability to change the position of subtitles in real time, allowing for a more personalised viewing experience. This recommendation for an alternative subtitle positioning that can be controlled by the user is unlike current subtitling practice. It provides evidence that further user-based research examining subtitle usage outside of the traditional television interface is required.","Michael Crabb, Rhianne Jones, Mike Armstrong, Chris Hughes","experiment methodology, laboratory experiment, subtitles, user experience",215,222
10.1145/2661334.2661371,ASSETS,2014,Buildings and users with visual impairment,uncovering factors for accessibility using BIT-Kit,"In this paper, we report on the experiences of visually impaired users in navigating buildings. We focus on an investigation of the way-finding experiences by 10 participants with varying levels of visual ability, as they undertook a way-finding task in an unfamiliar public building. Through applying the BIT-Kit framework in this preliminary user study, we were able to uncover 54 enabling and disabling interactions within the case study building. While this building adhered to building legislation, our findings identified a number of accessibility problems including, issues associated with using doors, hazards caused by building finishes, and difficulty in knowing what to do in the case of an emergency evacuation. This user study has demonstrated a disparity between design guidance and the accessibility needs of building users. It has uncovered evidence to enable architects to begin to design for the real needs of users who have a range of visual impairment. Furthermore, it has instigated discussion of how BIT-Kit's evidence could be incorporated into digital modelling tools currently used in architectural practice.","Lesley McIntyre, Vicki Hanson","accessibility, architecture, buildings, methods, visual impairment, way-finding",59,66
10.1145/3308561.3353808,ASSETS,2019,X-Ray,Screenshot Accessibility via Embedded Metadata,"Screenshots are frequently shared on social media, via personal communications, and in academic papers. Unfortunately, existing screenshot tools strip away semantics useful for making the content accessible, leaving only pixels. For example, a screenshot of a table removes the structural information useful for conveying it. We quantify the scale of the problem via a study of academic papers, showing that a large number of images included in academic papers are screenshots, and validate this via qualitative interviews with researchers about their figure generation process. We then introduce X-Ray, a system that captures and embeds the semantics of the underlying content into images. Using the X-Ray screenshot tool, semantic information is captured and stored in the Exif data of the resulting image, allowing it to ""tag along"" as the image is shared and reposted. We demonstrate that our approach retains accessibility for screen reader users via a study with five blind participants. More generally, our approach suggests a method for embedding accessibility metadata into otherwise inaccessible formats, enabling them to retain the more accessible representations that are present at capture time.","Sujeath Pareddy, Anhong Guo, Jeffrey Bigham","accessibility, alt text, large-scale analysis, pdf, runtime modification, screen readers, screenshot, vision impairment",389,395
10.1145/2384916.2384935,ASSETS,2012,Learning non-visual graphical information using a touch-based vibro-audio interface, ,"This paper evaluates an inexpensive and intuitive approach for providing non-visual access to graphic material, called a vibro-audio interface. The system works by allowing users to freely explore graphical information on the touchscreen of a commercially available tablet and synchronously triggering vibration patterns and auditory information whenever an on-screen visual element is touched. Three studies were conducted that assessed legibility and comprehension of the relative relations and global structure of a bar graph (Exp 1), Pattern recognition via a letter identification task (Exp 2), and orientation discrimination of geometric shapes (Exp 3). Performance with the touch-based device was compared to the same tasks performed using standard hardcopy tactile graphics. Results showed similar error performance between modes for all measures, indicating that the vibro-audio interface is a viable multimodal solution for providing access to dynamic visual information and supporting accurate spatial learning and the development of mental representations of graphical material.","Nicholas Giudice, Hari Prasath Palani, Eric Brenner, Kevin Kramer","accessibility (blind and visually-impaired), android programming, assistive technology, audio cues, graphs and diagrams, haptic cues, information graphics",103,110
10.1145/2661334.2661379,ASSETS,2014,Usability issues with 3D user interfaces for adolescents with high functioning autism, ,"Most literature on the usability of 3D user interfaces (3DUI) in ASD therapy consists of a series of case studies based on games for rehabilitation. These games have been largely successful. However, it is difficult to generalize the results of these specific case studies. The usability of atomic 3DUI interactions (e.g., rotation, translation) with respect to adolescents with ASD has not yet been evaluated. Adolescents with ASD often have enhanced spatial cognitive abilities and less efficient hand-eye coordination. Our main research question is ""Do adolescents with ASD perform 3DUI tasks differently than typically developed adolescents and if so, why?"" To address this question, we present a matched pair user study including adolescents without ASD (i.e. as controls) paired with adolescents who had a high ASD severity score, but were still considered high functioning. Our results give insight into the usability of 3DUI for adolescents with ASD and provide generalizable guidelines for future 3DUI applications for children with autism.","Chao Mei, Lee Mason, John Quarles","3d user interfaces, autism spectrum disorder, user studies.",99,106
10.1145/2700648.2809859,ASSETS,2015,Comparing Methods of Displaying Language Feedback for Student Videos of American Sign Language, ,"Deaf children benefit from early exposure to language, and higher levels of written language literacy have been measured in deaf adults who were raised in homes using American Sign Language (ASL). Prior work has established that new parents of deaf children benefit from technologies to support learning ASL. As part of a project to design a tool to automatically analyze a video of a students' signing and provide immediate feedback about fluent and non-fluent aspects of their movements, we conducted a study to compare multiple methods of conveying feedback to ASL students, using videos of their signing. Through a Wizard-of-Oz study, we compared three types of feedback in regard to users' subjective judgments of system quality and the degree students' signing improved (as judged by an ASL instructor who analyzed recordings of students' signing before and after they viewed each type of feedback). We found that displaying videos to students of their signing, augmented with feedback messages about their errors or correct ASL usage, yielded higher subjective scores and greater signing improvement. Students gave higher subjective scores to a version in which pop-up messages appeared overlaid on the student's video to indicate errors or correct ASL usage.","Matt Huenerfauth, Elaine Gale, Brian Penly, Mackenzie Willard, Dhananjai Hariharan","american sign language, education, feedback, user study",139,146
10.1145/2513383.2513438,ASSETS,2013,Do you see what I see?,designing a sensory substitution device to access non-verbal modes of communication,"The inability to access non-verbal cues is a setback for people who are blind or visually impaired. A visual-to-auditory Sensory Substitution Device (SSD) may help improve the quality of their lives by transforming visual cues into auditory cues. In this paper, we describe the design and development of a robust and real-time SSD called <i>i</i>FEPS -- improved Facial Expression Perception through Sound. The implementation of the <i>i</i>FEPS evolved over time through a participatory design process. We conducted both subjective and objective experiments to quantify the usability of the system. Evaluation with 14 subjects (7 blind + 7 blind-folded) shows that the users were able to perceive the facial expressions in most of the time. In addition, the overall subjective usability of the system was found to be scoring 4.02 in a 5 point Likert scale.","M. Tanveer, A. Anam, Mohammed Yeasin, Majid Khan","blind, facial expression, participatory design, sensory substitution",1,8
10.1145/3308561.3353782,ASSETS,2019,shapeCAD&#58; An Accessible 3D Modelling Workflow for the Blind and Visually-Impaired Via 2.5D Shape Displays, ,"Affordable rapid 3D printing technologies have become key enablers of the Maker Movement by giving individuals the ability to create physical finished products. However, existing computer-aided design (CAD) tools that allow authoring and editing of 3D models are mostly visually reliant and limit access to people with blindness and visual impairment (BVI). Through a series of co-design sessions with three blind users of mixed programming ability, we identify accessibility challenges in existing 3D modelling scripting tools and design interactions to support dynamic feedback of scripts using a 2.5D tactile shape display. With these insights, we implement shapeCAD. Interacting with shapeCAD, BVI users are able to leverage the low resolution output from a 2.5D shape display to complement programming of 3D models. shapeCAD allows users to haptically explore and modify existing models, and to author new models. We further validate usability and user experience through an evaluation with five BVI programmers. In a short period of time, novices were able to design a range of new objects. BVI users can bring a valuable perspective to design and it is imperative to increase accessibility in tools that enable this community to also participate as designers.","Alexa Siu, Son Kim, Joshua Miele, Sean Follmer","2.5d shape displays, accessible 3d printing, accessible authoring tools, haptics, tactile displays, tactile graphics",342,354
10.1145/2384916.2384943,ASSETS,2012,Designing for individuals,usable touch-screen interaction through shared user models,"Mobile touch-screen devices are becoming increasingly popular across a diverse range of users. Whilst there is a wealth of information and utilities available via downloadable apps, there is still a large proportion of users with visual and motor impairments who are unable to use the technology fully due to their interaction needs. In this paper we present an evaluation of the use of shared user modelling and adaptive interfaces to improve the accessibility of mobile touch-screen technologies. By using abilities based information collected through application use and continually updating the user model and interface adaptations, it is easy for users to make applications aware of their needs and preferences. Three smart phone apps were created for this study and tested with 12 adults who had diverse visual and motor impairments. Results indicated significant benefits from the shared user models that can automatically adapt interfaces, across applications, to address usability needs.","Kyle Montague, Vicki Hanson, Andy Cobley","adaptive interfaces, mobile touch screens, shared user modelling",151,158
10.1145/1878803.1878811,ASSETS,2010,Towards a tool for keystroke level modeling of skilled screen reading, ,"Designers often have no access to individuals who use screen reading software, and may have little understanding of how their design choices impact these users. We explore here whether cog-nitive models of auditory interaction could provide insight into screen reader usability. By comparing human data with a tool-generated model of a practiced task performed using a screen reader, we identify several requirements for such models and tools. Most important is the need to represent parallel execution of hearing with thinking and acting. Rules for placement of cogni-tive operators that were developed for visual user interfaces may not be applicable in the auditory domain. Other mismatches be-tween the data and the model were attributed to the extremely fast listening rate and differences between the typing patterns of screen reader usage and the model's assumptions. This work in-forms the development of more accurate models of auditory inter-action. Tools incorporating such models could help designers create user interfaces that are well tuned for screen reader users, without the need for modeling expertise.","Shari Trewin, Bonnie John, John Richards, Cal Swart, Jonathan Brezin, Rachel Bellamy, John Thomas","accessibility, keyboard navigation, klm, screen reader, usability",27,34
10.1145/2384916.2384919,ASSETS,2012,Capture,a desktop display-centric text recorder,"As more and more information is designed for human visual consumption through computer displays, the need to capture and process display-centric content is becoming increasingly important, especially for visually impaired users. We present <i>Capture</i>, a novel display-centric text recorder that facilitates real-time access to onscreen text and its structure and contextual information, including data associated with both foreground and background windows. <i>Capture</i> provides an intelligent caching architecture that integrates with the standard accessibility framework available on modern operating systems to continuously track onscreen text and metadata. This enables fast, semantic information recording without any modifications to applications, window systems, or operating system kernels. The recorded data is useful for a variety of problem domains, including assistive technologies, desktop search, auditing, and predictive graphical user interfaces. We have implemented a <i>Capture</i> prototype on Linux with the GNOME Accessibility Toolkit. Our results on real desktop applications demonstrate that <i>Capture</i> provides low runtime overhead and much more complete recording of onscreen text than modern desktop screen readers used for visually impaired users.","Oren Laadan, Andrew Shu, Jason Nieh","accessibility, assistive technology, screen capture",9,16
10.1145/2700648.2809849,ASSETS,2015,User Participation When Users have Mental and Cognitive Disabilities, ,"Persons with cognitive or mental disabilities have difficulties participating in or are excluded from IT development and assessments exercises due to the problems finding good ways to efficiently collaborate on equal terms. In this paper we describe how we worked closely together with persons that have mental and cognitive disabilities in order to test and develop methods for participation in assessments and in processes for developing, Information and Communication Technology (ICT) products and services. More than 100 persons with mental and cognitive disabilities participated in the study (people with diagnoses such as depression, anxiety disorder, bipolarity, and schizophrenia). To explore the conditions for a more equal and fair participation we have developed and elaborated a set of methods, tools and approaches. By combining scientific research methods with well-established methods for empowerment and participation we have developed methods that are cost effective and that easily can be incorporated in existing processes. We believe that our approach have taken steps to implement possibilities for persons with mental and cognitive disabilities to take part where user participation is needed in order not to discriminate or exclude but also to improve the overall quality of the end result. The results clearly show that it is possible to include persons with mental and cognitive disabilities. A mixed method -- mixed tool approach can increase the possibility for participation. The results also show that the quality of the analysis phase increases if the collaborative approach is extended to also embrace the data analysis phase.","Stefan Johansson, Jan Gulliksen, Ann Lantz","accessibility, cognitive disability, inclusion, mental disability, mental problems, user participation",69,76
10.1145/2700648.2809862,ASSETS,2015,ChartMaster,A Tool for Interacting with Stock Market Charts using a Screen Reader,"Stock market charts guide investors in making financial decisions. Online stock market charts are largely interactive, driven by real-time financial data. However, these are not easily accessible via a screen reader. To enable screen reader users to query and effectively use interactive online stock market charts, we are developing a tool called ChartMaster. In this paper, we describe an early study conducted with sixteen visually impaired persons, most of whom were financial novices, for co-designing the interaction interface for C hartMaster. An inclusive design exercise was undertaken to discover alternative interfaces using non-visual modalities to interact with stock market charts. A user-centered process of co-design using HCI methods was carried out to iteratively evaluate and refine three input solutions: audio input, text input and dropdown menu. While the users ultimately declared the dropdown menu to be the most useful of the three solutions, they wanted all possible options to choose from based on task contexts and personal preferences. User feedback confirmed that a one-size-fits-all design is not ideal for accommodating diverse user needs within the widest possible range of contexts. It was also found that the ChartMaster tool with dropdown menu interface holds potential educational value for financial novices.","Hong Zou, Jutta Treviranus","accessibility, financial literacy, inclusive design, multi-modal, non-visual, screen reader, stock market charts, usability, visual impairment",107,116
10.1145/3234695.3239330,ASSETS,2018,"What My Eyes Can't See, A Robot Can Show Me",Exploring the Collaboration Between Blind People and Robots,"Blind people rely on sighted peers and different assistive technologies to accomplish everyday tasks. In this paper, we explore how assistive robots can go beyond information-giving assistive technologies (e.g., screen readers) by physically collaborating with blind people. We first conducted a set of focus groups to assess how blind people perceive and envision robots. Results showed that, albeit having stereotypical concerns, participants conceive the integration of assistive robots in a broad range of everyday life scenarios and are welcoming of this type of technology. In a second study, we asked blind participants to collaborate with two versions of a robot in a Tangram assembly task: one robot would only provide static verbal instructions whereas the other would physically collaborate with participants and adjust the feedback to their performance. Results showed that active collaboration had a major influence on the successful performance of the task. Participants also reported higher perceived warmth, competence and usefulness when interacting with the physically assistive robot. Overall, we provide preliminary results on the usefulness of assistive robots and the possible role these can hold in fostering a higher degree of autonomy for blind people.","Mayara Bonani, Raquel Oliveira, Filipa Correia, Andr&#233; Rodrigues, Tiago Guerreiro, Ana Paiva","blind people, collaboration, human-robot interaction",15,27
10.1145/3132525.3132530,ASSETS,2017,Imagining Artificial Intelligence Applications with People with Visual Disabilities using Tactile Ideation, ,"There has been a surge in artificial intelligence (AI) technologies co-opted by or designed for people with visual disabilities. Researchers and engineers have pushed technical boundaries in areas such as computer vision, natural language processing, location inference, and wearable computing. But what do people with visual disabilities imagine as their own technological future? To explore this question, we developed and carried out tactile ideation workshops with participants in the UK and India. Our participants generated a large and diverse set of ideas, most focusing on ways to meet needs related to social interaction. In some cases, this was a matter of recognizing people. In other cases, they wanted to be able to participate in social situations without foregrounding their disability. It was striking that this finding was consistent across UK and India despite substantial cultural and infrastructural differences. In this paper, we describe a new technique for working with people with visual disabilities to imagine new technologies that are tuned to their needs and aspirations. Based on our experience with these workshops, we provide a set of social dimensions to consider in the design of new AI technologies: social participation, social navigation, social maintenance, and social independence. We offer these social dimensions as a starting point to forefront users' social needs and desires as a more deliberate consideration for assistive technology design.","Cecily Morrison, Edward Cutrell, Anupama Dhareshwar, Kevin Doherty, Anja Thieme, Alex Taylor","accessibility, ai, artificial intelligence, blind, design, ideation, multicultural, visually impaired",81,90
10.1145/3234695.3239329,ASSETS,2018,"'Wow! You're Wearing a Fitbit, You're a Young Boy Now!""",Socio-Technical Aspirations for Children with Autism in India,"In this paper, we build a case for incorporating socio-technical aspirations of different stakeholders, e.g. parents, care-givers, and therapists, to motivate technology acceptance and adoption for children with autism. We base this on findings from two studies at a special school in New Delhi. First, with six children with autism, their parents and therapists we explored whether fitness bands motivate children with autism in India to increase their physical activity. Second, with five parents and specialists at the same school, we conducted interviews to understand their expectations from and current usage of technology. Previous work defines a culture-based framework for assistive technology design with three dimensions: lifestyle, socio-technical infrastructure, and monetary and informational resources. To this framework we propose a fourth dimension of socio-technical aspirations. We discuss the implications of the proposed fourth dimension to the existing framework.","Sumita Sharma, Krishnaveni Achary, Harmeet Kaur, Juhani Linna, Markku Turunen, Blessin Varkey, Jaakko Hakulinen, Sanidhya Daeeyya","autism in india, children with autism, socio-technical aspirations, technology acceptance and adoption",174,184
10.1145/1878803.1878809,ASSETS,2010,Towards accessible touch interfaces, ,"Touch screen mobile devices bear the promise of endless leisure, communication, and productivity opportunities to motor-impaired people. Indeed, users with residual capacities in their upper extremities could benefit immensely from a device with no demands regarding strength. However, the precision required to effectively select a target without physical cues creates problems to people with limited motor abilities. Our goal is to thoroughly study mobile touch screen interfaces, their characteristics and parameterizations, thus providing the tools for informed interface design for motor-impaired users. We present an evaluation performed with 15 tetraplegic people that allowed us to understand the factors limiting user performance within a comprehensive set of interaction techniques (<i>Tapping, Crossing, Exiting and Directional Gesturing</i>) and parameterizations (<i>Position, Size and Direction</i>). Our results show that for each technique, accuracy and precision vary across different areas of the screen and directions, in a way that is directly dependent on target size. Overall, <i>Tapping</i> was both the preferred technique and among the most effective. This proves that it is possible to design inclusive unified interfaces for motor-impaired and able-bodied users once the correct parameterization or adaptability is assured.","Tiago Guerreiro, Hugo Nicolau, Joaquim Jorge, Daniel Gon&#231;alves","evaluation, interaction techniques, mobile device, tetraplegic, touch screen",19,26
10.1145/2384916.2384932,ASSETS,2012,How do professionals who create computing technologies consider accessibility?, ,"In this paper, we present survey findings about how user experience (UX) and human-computer interaction (HCI) professionals, who create information and communication technologies (ICTs), reported considering accessibility in their work. Participants (N = 199) represented a wide range of job titles and nationalities. We found that most respondents (87%, N = 173) reported that accessibility was important or very important in their work; however, when considerations for accessibility were discussed in an open-ended question (N =185) the scope was limited. Additionally, we found that aspects of empathy and professional experience were associated with how accessibility considerations were reported. We also found that many respondents indicated that decisions about accessibility were not in their control. We argue that a better understanding about how accessibility is considered by professionals has implications for academic programs in HCI and UX as to how well programs are preparing students to consider and advocate for inclusive design.","Cynthia Putnam, Kathryn Wozniak, Mary Jo Zefeldt, Jinghui Cheng, Morgan Caputo, Carl Duffield","accessibility, diverse users, inclusive design, professions",87,94
10.1145/3308561.3353798,ASSETS,2019,Deep Learning for Automatically Detecting Sidewalk Accessibility Problems Using Streetscape Imagery, ,"Recent work has applied machine learning methods to automatically find and/or assess pedestrian infrastructure in online map imagery (e.g., satellite photos, streetscape panoramas). While promising, these methods have been limited by two interrelated issues: small training sets and the choice of machine learning model. In this paper, aided by the recently released Project Sidewalk dataset of 300,000+ image-based sidewalk accessibility labels, we present the first examination of deep learning to automatically assess sidewalks in Google Street View (GSV) panoramas. Specifically, we investigate two application areas: automatically validating crowdsourced labels and automatically labeling sidewalk accessibility issues. For both tasks, we introduce and use a residual neural network (ResNet) modified to support both image and non-image (contextual) features (e.g., geography). We present an analysis of performance, the effect of our non-image features and training set size, and cross-city generalizability. Our results significantly improve on prior automated methods and, in some cases, meet or exceed human labeling performance.","Galen Weld, Esther Jang, Anthony Li, Aileen Zeng, Kurtis Heimerl, Jon Froehlich","accessibility, computer vision, neural networks, sidewalks",196,209
10.1145/3234695.3236339,ASSETS,2018,Investigating Cursor-based Interactions to Support Non-Visual Exploration in the Real World, ,"The human visual system processes complex scenes to focus attention on relevant items. However, blind people cannot visually skim for an area of interest. Instead, they use a combination of contextual information, knowledge of the spatial layout of their environment, and interactive scanning to find and attend to specific items. In this paper, we define and compare three cursor-based interactions to help blind people attend to items in a complex visual scene: window cursor (move their phone to scan), finger cursor (point their finger to read), and touch cursor (drag their finger on the touchscreen to explore). We conducted a user study with 12 participants to evaluate the three techniques on four tasks, and found that: window cursor worked well for locating objects on large surfaces, finger cursor worked well for accessing control panels, and touch cursor worked well for helping users understand spatial layouts. A combination of multiple techniques will likely be best for supporting a variety of everyday tasks for blind users.","Anhong Guo, Saige McVea, Xu Wang, Patrick Clary, Ken Goldman, Yang Li, Yu Zhong, Jeffrey Bigham","accessibility, blind, computer vision, cursor, interaction, mobile devices, non-visual exploration, visually impaired",3,14
10.1145/2700648.2809840,ASSETS,2015,Faster Text-to-Speeches,Enhancing Blind People's Information Scanning with Faster Concurrent Speech,"Blind people rely mostly on the auditory feedback of screen readers to consume digital information. Still, how fast can information be processed remains a major problem. The use of faster speech rates is one of the main techniques to speed-up the consumption of digital information. Moreover, recent experiments have suggested the use of concurrent speech as a valid alternative when scanning for relevant information. In this paper, we present an experiment with 30 visually impaired participants, where we compare the use of faster speech rates against the use of concurrent speech. Moreover, we combine these two approaches by gradually increasing the speech rate with one, two and three voices. Results show that concurrent voices with speech rates slightly faster than the default rate, enable a significantly faster scanning for relevant content, while maintaining its comprehension. In contrast, to keep-up with concurrent speech timings, <i>One-Voice</i> requires larger speech rate increments, which cause a considerable loss in performance. Overall, results suggest that the best compromise between efficiency and the ability to understand each sentence is the use of <i>Two-Voices</i> with a rate of 1.75*<i>default-rate</i> (approximately 278 WPM).","Jo&#227;o Guerreiro, Daniel Gon&#231;alves","auditory perception, blind, cocktail party effect, concurrent speech, scanning, screen reader, skimming, speech rate, text-to-speech, visually impaired",3,11
10.1145/3308561.3353785,ASSETS,2019,Understanding Mental Ill-health as Psychosocial Disability,Implications for Assistive Technology,"Psychosocial disability involves actual or perceived impairment due to a diversity of mental, emotional, or cognitive experiences. While assistive technology for psychosocial disabilities has been understudied in communities such as ASSETS, advances in computing have opened up a number of new avenues for assisting those with psychosocial disabilities beyond the clinic. However, these tools continue to emerge primarily within the framework of ""treatment,"" emphasizing resolution or improvement of mental health symptoms. This work considers what it means to adopt a social model lens from disability studies and incorporate the expertise of assistive technology researchers in relation to mental health. Our investigation draws on interviews conducted with 18 individuals who have complex health needs that include mental health symptoms. This work highlights the potential role for assistive technology in supporting psychosocial disability outside of a clinical or medical framework.","Kathryn Ringland, Jennifer Nicholas, Rachel Kornfield, Emily Lattie, David Mohr, Madhu Reddy","anxiety, depression, mental health, psychosocial disability, social model of disability",156,170
10.1145/2700648.2809853,ASSETS,2015,Using In-Situ Projection to Support Cognitively Impaired Workers at the Workplace, ,"Today's working society tries to integrate more and more impaired workers into everyday working processes. One major scenario for integrating impaired workers is in the assembly of products. However, the tasks that are being assigned to cognitively impaired workers are easy tasks that consist of only a small number of assembly steps. For tasks with a higher number of steps, cognitively impaired workers need instructions to help them with assembly. Although supervisors provide general support and assist new workers while learning new assembly steps, sheltered work organizations often provide additional printed pictorial instructions that actively guide the workers. To further improve continuous instructions, we built a system that uses in-situ projection and a depth camera to provide context-sensitive instructions. To explore the effects of in-situ instructions, we compared them to state-of-the-art pictorial instructions in a user study with 15 cognitively impaired workers at a sheltered work organization. The results show that using in-situ instructions, cognitively impaired workers can assemble more complex products up to 3 times faster and with up to 50% less errors. Further, the workers liked the in-situ instructions provided by our assistive system and would use it for everyday assembly.","Markus Funk, Sven Mayer, Albrecht Schmidt","assistance for impaired workers, assistive system, augmented reality, in-situ projection",185,192
10.1145/2982142.2982179,ASSETS,2016,WeAllWalk,An Annotated Data Set of Inertial Sensor Time Series from Blind Walkers,"We introduce WeAllWalk, a data set of inertial sensor time series collected from blind walkers using a long cane or a guide dog. Blind participants walked through fairly long and complex indoor routes that included obstacles to be avoided and doors to be opened. Inertial data was recorded by two iPhone 6s carried by our participants in their pockets and carefully annotated. Ground truth heel strike times were measured by two small inertial sensor units clipped to the participants' shoes. We also show comparative examples of application of step counting and turn detection algorithms to selected data from WeAllWalk.","German Flores, Roberto Manduchi","inertial sensing, step counting., wayfinding",141,150
10.1145/3132525.3132527,ASSETS,2017,Using Participatory Design with Proxies with Children with Limited Communication, ,"Including children with communication disorders in the participatory design and evaluation of digital technologies is challenging, as communication between designers and users - an important part of this approach - can be impacted. Using Participatory Design with Proxies (PDwP) supports the inclusion of input from different stakeholders, such as parents, teachers and Speech-Language Pathologists (SLPs), who can provide valuable insight and feedback to augment direct input from users, which may be severely limited. We describe how we used PDwP to design a digital living media system that motivates children with disabilities to use digital therapeutic and learning applications and supports communication and collaboration between users. We employed an iterative design process to fabricate three functional prototypes and used them as design probes in participants' home and school settings. We present lessons learned in the form of the strengths and shortcomings of using PDwP for designing systems for children with limited communication abilities.","Foad Hamidi, Melanie Baljko, Isabel G&#243;mez","children with disabilities, digital living media systems, participatory design with proxies",250,259
10.1145/3234695.3236351,ASSETS,2018,HoloLearn,Wearable Mixed Reality for People with Neurodevelopmental Disorders (NDD),"Our research explores the potential of wearable Mixed Reality (MR) for people with Neuro-Developmental Disorders (NDD). The paper presents HoloLearn, a MR application designed in cooperation with NDD experts and implemented using HoloLens technology. The goal of HoloLearn is to help people with NDD learn how to perform simple everyday tasks in domestic environments and improve autonomy. An original feature of the system is the presence of a virtual assistant devoted to capture the user's attention and to give her/him hints during task execution in the MR environment. We performed an exploratory study involving 20 subjects with NDD to investigate the acceptability and usability of HoloLearn and its potential as a therapeutic tool. HoloLearn was well-accepted by the participants and the activities in the MR space were perceived as enjoyable, despite some usability problems associated to HoloLens interaction mechanism. More extensive and long term empirical research is needed to validate these early results, but our study suggests that HoloLearn could be adopted as a complement to more traditional interventions. Our work, and the lessons we learned, may help designers and developers of future MR applications devoted to people with NDD and to other people with similar needs.","Beatrice Aruanno, Franca Garzotto, Emanuele Torelli, Francesco Vona","augmented reality, holograms, hololens, mixed reality, neuro-developmental disorders, virtual assistant",40,51
10.1145/2982142.2982164,ASSETS,2016,Improving Real-Time Captioning Experiences for Deaf and Hard of Hearing Students, ,"We take a qualitative approach to understanding deaf and hard of hearing (DHH) students' experiences with real-time captioning as an access technology in mainstream university classrooms. We consider both existing human-based captioning as well as new machine-based solutions that use automatic speech recognition (ASR). We employed a variety of qualitative research methods to gather data about students' captioning experiences including in-class observations, interviews, diary studies, and usability evaluations. We also conducted a co-design workshop with 8 stakeholders after our initial research findings. Our results show that accuracy and reliability of the technology are still the most important issues across captioning solutions. However, we additionally found that current captioning solutions tend to limit students' autonomy in the classroom and present a variety of user experience shortcomings, such as complex setups, poor feedback and limited control over caption presentation. Based on these findings, we propose design requirements and recommend features for real-time captioning in mainstream classrooms.","Saba Kawas, George Karalis, Tzu Wen, Richard Ladner","access technology, automatic speech recognition, co-design, deaf and hard of hearing, human factors., inclusive classrooms, real-time captions",15,23
10.1145/3132525.3132528,ASSETS,2017,BrailleSketch,A Gesture-based Text Input Method for People with Visual Impairments,"In this paper, we present BrailleSketch, a gesture-based text input method on touchscreen smartphones for people with visual impairments. To input a letter with BrailleSketch, a user simply sketches a gesture that passes through all dots in the corresponding Braille code for that letter. BrailleSketch allows users to place their fingers anywhere on the screen to begin a gesture and draw the Braille code in many ways. To encourage users to type faster, BrailleSketch does not provide immediate letter-level audio feedback but instead provides word-level audio feedback. It uses an auto-correction algorithm to correct typing errors. Our evaluation of the method with ten participants with visual impairments who each completed five typing sessions shows that BrailleSketch supports a text entry speed of 14.53 word per min (wpm) with 10.6% error. Moreover, our data suggest that the speed had not begun to plateau yet by the last typing session and can continue to improve. Our evaluation also demonstrates the positive effect of the reduced audio feedback and the auto-correction algorithm.","Mingzhe Li, Mingming Fan, Khai Truong","blind, braille, gesture, mobile devices, people with visual impairments, sketch, text input",12,21
10.1145/3234695.3236359,ASSETS,2018,"""It Looks Beautiful but Scary""",How Low Vision People Navigate Stairs and Other Surface Level Changes,"Walking in environments with stairs and curbs is potentially dangerous for people with low vision. We sought to understand what challenges low vision people face and what strategies and tools they use when navigating such surface level changes. Using contextual inquiry, we interviewed and observed 14 low vision participants as they completed navigation tasks in two buildings and through two city blocks. The tasks involved walking in- and outdoors, across four staircases and two city blocks. We found that surface level changes were a source of uncertainty and even fear for all participants. Besides the white cane that many participants did not want to use, participants did not use technology in the study. Participants mostly used their vision, which was exhausting and sometimes deceptive. Our findings highlight the need for systems that support surface level changes and other depth-perception tasks; they should consider low vision people's distinct experiences from blind people, their sensitivity to different lighting conditions, and leverage visual enhancements.","Yuhang Zhao, Elizabeth Kupferstein, Doron Tal, Shiri Azenkot","curbs, depth perception, low vision, mobility, navigation, stairs, surface level changes",307,320
10.1145/2384916.2384924,ASSETS,2012,Design recommendations for tv user interfaces for older adults,findings from the eCAALYX project,"While guidelines for designing websites and iTV applications for older adults exist, no previous work has suggested how to best design TV user interfaces (UIs) that are accessible to older adults. Building upon pertinent guidelines from related areas, this paper presents thirteen recommendations for designing UIs for TV applications for older adults. These recommendations are the result of iterative design, testing, and development of a TV-based health system for older adults that aims to provide a holistic solution to improve quality of life for older adults with chronic conditions by fostering their autonomy and reducing hospitalization costs. The authors' work and experience shows that widely known UI design guidelines unsurprisingly apply to the design of TV-based applications for older adults, but acquire a crucial importance in this context.","Francisco Nunes, Maureen Kerwin, Paula Alexandra Silva","design recommendations, older adults, tv, user interface design",41,48
10.1145/2049536.2049545,ASSETS,2011,Situation-based indoor wayfinding system for the visually impaired, ,"This paper presents an indoor wayfinding system to help the visually impaired finding their way to a given destination in an unfamiliar environment. The main novelty is the use of the user's situation as the basis for designing color codes to explain the environmental information and for developing the wayfinding system to detect and recognize such color codes. Actually, people would require different information according to their situations. Therefore, situation-based color codes are designed, including location-specific codes and guide codes. These color codes are affixed in certain locations to provide information to the visually impaired, and their location and meaning are then recognized using the proposed wayfinding system. Consisting of three steps, the proposed wayfinding system first recognizes the current situation using a vocabulary tree that is built on the shape properties of images taken of various situations. Next, it detects and recognizes the necessary codes according to the current situation, based on color and edge information. Finally, it provides the user with environmental information and their path through an auditory interface. To assess the validity of the proposed wayfinding system, we have conducted field test with four visually impaired, then the results showed that they can find the optimal path in real-time with an accuracy of 95%.","Eunjeong Ko, Jin Ju, Eun Kim","2d color code, situation awareness, speeded-up robust feature (surf), visually impaired people, vocabulary tree., wayfinding system",35,42
10.1145/3308561.3353789,ASSETS,2019,Typing Slowly but Screen-Free,Exploring Navigation over Entirely Auditory Keyboards,"Accessible onscreen keyboards require people who are blind to keep out their phone at all times to search for visual affordances they cannot see. Is it possible to re-imagine text entry without a reference screen? To explore this question, we introduce screenless keyboards as aural flows (keyflows): rapid auditory streams of Text-To-Speech (TTS) characters controllable by hand gestures. In a study, 20 screen-reader users experienced keyflows to perform initial text entry. Typing took inordinately longer than current screen-based keyboards, but most participants preferred screen-free text entry to current methods, especially for short messages on-the-go. We model navigation strategies that participants enacted to aurally browse entirely auditory keyboards and discuss their limitation and benefits for daily access. Our work points to trade-offs in user performance and user experience for situations when blind users may trade typing speed with the benefit of being untethered from the screen.","Reeti Mathur, Aishwarya Sheth, Parimal Vyas, Davide Bolchini","accessible text entry, aural navigation, screen-reader users",427,439
10.1145/2700648.2809863,ASSETS,2015,How 3D Virtual Humans Built by Adolescents with ASD Affect Their 3D Interactions, ,"Training games have many potential benefits for autism spectrum disorder (ASD) intervention, such as increasing motivation and improving the abilities of performing daily living activities, due to their ability to simulate real world scenarios. A more motivating game may stimulate users to play the game more, and it may also result in users performing better in the game. Incorporating users' interests into the game could be a good way to build a motivating game, especially for users with ASD. We propose a Customizable Virtual Human (CVH) which enables users with ASD to easily customize a virtual human and then interact with the CVH in a 3D interaction task. Previous work has shown that users with ASD may have less efficient hand-eye coordination in performing 3D interaction tasks than users without ASD. We developed a hand-eye coordination training game - Imagination Soccer - and presented a user study on adolescents with high functioning ASD to investigate the effects of CVHs. We compare the differences of participants' 3D interaction performances, game performances and user experiences (i.e. presence, involvement, and flow) under CVH and Non-customizable Virtual Human (with randomly generated appearances) conditions. As expected, the results indicated that for users with ASD, CVHs could effectively motivate them to play the game more, and offer a better user experience. Surprisingly, results also showed that the CVHs improved performance in the hand-eye-coordination task users had higher success rate and blocked more soccer balls with the CVH than with a non-customizable virtual human.","Chao Mei, Lee Mason, John Quarles","3D interactions, autism spectrum disorder, customizable virtual human",155,162
10.1145/2049536.2049542,ASSETS,2011,Towards a framework to situate assistive technology design in the context of culture, ,"We present the findings from a cross-cultural study of the expectations and perceptions of individuals with autism and other intellectual disabilities (AOID) in Kuwait, Pakistan, South Korea, and the United States. Our findings exposed cultural nuances that have implications for the design of assistive technologies. We develop a framework, based on three themes; 1) lifestyle; 2) socio-technical infrastructure; and 3) monetary and informational resources within which the cultural implications and opportunities for assistive technology were explored. The three key contributions of this work are: 1) the development of a framework that outlines how culture impacts perceptions and expectations of individuals with social and intellectual disabilities; 2) a mapping of how this framework leads to implications and opportunities for assistive technology design; 3) the presentation of concrete examples of how these implications impact the design of three emerging assistive technologies.","Fatima Boujarwah,  Nazneen, Hwajung Hong, Gregory Abowd, Rosa Arriaga","assistive technology, autism spectrum disorders, culture",19,26
10.1145/2513383.2513446,ASSETS,2013,Physical accessibility of touchscreen smartphones, ,"This paper examines the use of touchscreen smartphones, focusing on physical access. Using interviews and observations, we found that participants with dexterity impairment considered a smartphone both useful and usable, but tablet devices offer several important advantages. Cost is a major barrier to adoption. We describe usability problems that are not addressed by existing accessibility options, and observe that the dexterity demands of important accessibility features made them unusable for many participants. Despite participants' enthusiasm for both smartphones and tablet devices, their potential is not yet fully realized for this population.","Shari Trewin, Cal Swart, Donna Pettick","accessibility, mobile devices, motor impairment, personalization, touch-screen interaction",1,8
10.1145/2661334.2661374,ASSETS,2014,The gest-rest,a pressure-sensitive chairable input pad for power wheelchair armrests,"Interacting with touch screen-based computing devices can be difficult for individuals with mobility impairments that affect their hands, arms, neck, or head. These problems may be especially difficult for power wheelchair users, as the frame of their wheelchair may obstruct the users' range of motion and reduce their ability to reach objects in the environment. The concept of chairable input devices refers to input devices that are designed to fit with the form of an individual's wheelchair, much like wearable technology fits with an individual's clothing. In this paper, we introduce a new <i>chairable input device</i>, the Gest-Rest, which provides a pressure-sensitive input surface that fits over a standard power wheelchair armrest. The Gest-Rest enables users to perform traditional touch screen gestures, such as press and flick, as well as pressure-based gestures such as squeezing and punching. The Gest-Rest enables multiple inputs, unlike most switches, and does not substantially change the shape of the wheelchair armrest. We present a formative evaluation in which nine wheelchair users and three clinicians tested multiple gestures using the Gest-Rest prototype, and provided recommendations for integrating the Gest-Rest with computing applications. Our study showed that our motor impaired participants were each able to perform multiple gestures using the prototype, but had some difficulty with the pre-set sensitivity settings, and would thus benefit from a more robust gesture recognizer.","Patrick Carrington, Amy Hurst, Shaun Kane","accessibility, chairable, gesture, input, mobile computing, power wheelchair, pressure input, touch",201,208
10.1145/2049536.2049549,ASSETS,2011,The vlogging phenomena,a deaf perspective,"Highly textual websites present barriers to Deaf people, primarily using American Sign Language for communication. Deaf people have been posting ASL content in form of vlogs to YouTube and specialized websites such as Deafvideo.TV. This paper presents some of the first insights into the use of vlogging technology and techniques among the Deaf community. The findings suggest that there are differences between YouTube and Deafvideo.TV due to differences between mainstream and specialized sites. Vlogging technology seems to influence use of styles that are not found or are used differently in face-to-face communications. Examples include the alteration of vloggers' signing space to convey different meanings on screen.","Ellen Hibbard, Deb Fels","access, deaf, sign language, technology, video, vlog, website",59,66
10.1145/2700648.2809858,ASSETS,2015,Social Media Platforms for Low-Income Blind People in India, ,"We present the first analysis of the use and non-use of social media platforms by low-income blind users in rural and peri-urban India. Using a mixed-methods approach of semi-structured interviews and observations, we examine the benefits received by low-income blind people from Facebook, Twitter and WhatsApp and investigate constraints that impede their social media participation. We also present a detailed analysis of how low-income blind people used a voice-based social media platform deployed in India that received significant traction from low-income people in rural and peri-urban areas. In eleven-weeks of deployment, fifty-three blind participants in our sample collectively placed 4784 voice calls, contributed 1312 voice messages, cast 33,909 votes and listened to the messages 46,090 times. Using a mixed-methods analysis of call logs, qualitative interviews, and phone surveys, we evaluate the strengths and weaknesses of the platform and benefits it offered to low-income blind people.","Aditya Vashistha, Edward Cutrell, Nicola Dell, Richard Anderson","HCI4D, ICT4D, blind, interactive voice response system, social media",259,272
10.1145/1878803.1878841,ASSETS,2010,A tool to promote prolonged engagement in art therapy,design and development from arts therapist requirements,"This paper describes the development of a tool that assists arts therapists working with older adults with dementia. Participation in creative activities is becoming accepted as a method for improving quality of life. This paper presents the design of a novel tool to increase the capacity of creative arts therapists to engage cognitively impaired older adults in creative activities. The tool is a creative arts touch-screen interface that presents a user with activities such as painting, drawing, or collage. It was developed with a user-centered design methodology in collaboration with a group of creative arts therapists. The tool is customizable by therapists, allowing them to design and build personalized therapeutic/goal-oriented creative activities for each client. In this paper, we evaluate the acceptability of the tool by arts therapists (our primary user group). We perform this evaluation qualitatively with a set of one-on-one interviews with arts therapists who work specifically with persons with dementia. We show how their responses during interviews support the idea of a customizable assistance tool. We evaluate the tool in simulation by showing a number of examples, and by demonstrating customizable components.","Jesse Hoey, Krists Zutis, Valerie Leuty, Alex Mihailidis","art therapy, computer vision, dementia, markov decision process, user modeling",211,218
10.1145/2661334.2661366,ASSETS,2014,Tactile graphics with a voice,using QR codes to access text in tactile graphics,"Textbook figures are often converted into a tactile format for access by blind students. These figures are not truly accessible unless the text within the figures is also made accessible. A common solution to access text in a tactile image is to use embossed Braille. We have developed an alternative to Braille that uses QR codes for students who want tactile graphics, but prefer the text in figures be spoken, rather than in Braille. Tactile Graphics with a Voice (TGV) allows text within tactile graphics to be accessible by using a talking QR code reader app on a smartphone. To evaluate TGV, we performed a longitudinal study where ten blind and low vision participants were asked to complete tasks using three alternative picture taking guidance techniques: 1) no guidance, 2) verbal guidance, and 3) finger pointing guidance. Our results show that TGV is an effective way to access text in tactile graphics, especially for those blind users who are not fluent in Braille. In addition, guidance preferences varied with each of the guidance techniques being preferred by at least one participant.","Catherine Baker, Lauren Milne, Jeffrey Scofield, Cynthia Bennett, Richard Ladner","access technology, blind, camera, non-visual feedback, qr codes., tactile graphics, visually impaired",75,82
10.1145/2982142.2982170,ASSETS,2016,Real-Time Mobile Personalized Simulations of Impaired Colour Vision, ,"Colour forms an essential element of day-to-day life for most people, but at least 5% of the world have Impaired Colour Vision (ICV) - seeing fewer colours than everyone else. Those with typical colour vision find it difficult to understand how people with ICV perceive colour, leading to misunderstanding and challenges for people with ICV. To help improve understanding, personalized simulations of ICV have been developed, but are computationally demanding (so limited to static images), which limits the value of these simulations. To address this, we extended personalized ICV simulations to work in real time on a mobile device to allow people with typical colour vision greater freedom in exploring ICV. To validate our approach, we compared our real-time simulation technique to an existing adjustable simulation technique and found general agreement between the two. We then deployed three real-time personalized ICV simulations to nine people with typical colour vision, encouraging them to take photos of interesting colour situations. In just over one week, participants recorded over 450 real-world images of situations where their simulation presented a distinct challenge for their respective ICV participant. Through a questionnaire and discussion of photos with participants, we found that our solution provides a valuable mechanism for building understanding of ICV for people with typical colour vision.","Rhouri MacAlpine, David Flatla","colour vision deficiency, colourblindness, impaired colour vision, mobile personalized simulation",181,189
10.1145/2661334.2661363,ASSETS,2014,From screen reading to aural glancing,towards instant access to key page sections,"Whereas glancing at a web page is crucial for navigation, screen readers force users to listen to content serially. This hampers efficient browsing of complex pages and maintains an accessibility divide between sighted and screen-reader users. To address this problem, we adopt a three-pronged strategy: (1) in a user study, we identified key page-level navigation problems that screen-reader users face while browsing a complex site; (2) through a crowd-sourcing system, we prioritized the most relevant sections of different page types necessary to support basic tasks; (3) we introduced DASX, a navigation approach that augments the ability of screen-reader users to ""aurally glance"" at a complex page by accessing at any time the most relevant page sections. In a preliminary evaluation, DASX markedly reduced the gap in page navigation efficiency between screen-reader and sighted users. Our contribution provides the groundwork for rethinking access strategies that strongly tie aural navigation to user's tasks.","Prathik Gadde, Davide Bolchini","blind users, direct web access, ecommerce web applications, fast browsing, screen-reader users, voice browsing, web navigation",67,74
10.1145/2700648.2809869,ASSETS,2015,Collaborative Creation of Digital Tactile Graphics, ,"The Tangram Workstation is a collaborative system for creating tactile graphics. A transcriber composing a tactile graphic from a visual source is supported by a non-visual reviewer on a two-dimensional tactile pin-matrix device on which he can observe and adapt the work of his sighted team member. We present the results of an evaluation with eight teams, each consisting of a transcriber and a blind reviewer. Overall, quality of tactile graphics can be improved by a collaborative approach. In most cases blind users recommended changes on tactile graphics even when they have been prepared by professional sighted editors. The study also showed that the blind reviewer is able to do simple editing tasks independently with our workstation.","Jens Bornschein, Denise Prescher, Gerhard Weber","blind users, collaboration, drawing-application, evaluation, pin-matrix device, tactile graphics",117,126
10.1145/2384916.2384918,ASSETS,2012,Back navigation shortcuts for screen reader users, ,"When screen reader users need to back track pages to re-find previously visited content, they are forced to listen to some portion of each unwanted page to recognize it. This makes aural back navigation inefficient, especially on large websites. To address this problem, we introduce topic- and list-based back: two navigation strategies that provide back browsing shortcuts by leveraging the conceptual structure of content-rich websites. Both are manifested in Webtime, an accessible website on the history of the Web. A controlled study (N=10) conducted at the Indiana School for the Blind and Visually Impaired compared topic- and list-based back to traditional back mechanisms while participants completed fact-finding tasks. Topic- and list-based back significantly decreased time-on-task and number of backtracked pages; the navigation shortcuts were also associated with positive improvements in perceived cognitive effort and navigation experience. The proposed strategies can operate as a supplement to current back mechanisms in information-rich websites.","Romisa Rohani Ghahari, Mexhid Ferati, Tao Yang, Davide Bolchini","assistive technology, back navigation, information architecture, screen reader users",1,8
10.1145/3308561.3353797,ASSETS,2019,Reading Between the Guidelines,How Commercial Voice Assistant Guidelines Hinder Accessibility for Blind Users,"Voice-Activated Personal Assistants (VAPAs)-like Apple Siri and Amazon Alexa - have rapidly become common features on mobile devices and in homes of millions of people around the world. They have proven to be particularly valuable to people with disabilities, chiefly among people with visual impairments. Yet, we still know relatively little about the fundamental metaphors and guidelines for designing voice assistants, and how they might empower and constrain visually impaired users. To address this need, we conducted a qualitative document review of VAPA design guidelines published by top commercial vendors Amazon, Google, Microsoft, Apple and Alibaba. We found that guidelines have many commonalities that surface an underlying assumption that VAPA interfaces should be modeled after human-human conversation. We draw on prior work about needs of people with visual impairments to critique this taken-for-granted human-human conversation metaphor and offer amendments to prevailing design guidelines that can make this now-pervasive platform more fully achieve its potential to become universally usable.","Stacy Branham, Antony Rishin Mukkath Roy","accessibility, blindness, conversation, design guidelines, voice assistant",446,458
10.1145/2384916.2384922,ASSETS,2012,Basic senior personas,a representative design tool covering the spectrum of European older adults,"The persona method is a powerful approach to focus on needs and characteristics of target users, keeping complex user data,numbers and diagrams alive during the whole design cycle.However, the development of prosperous personas requires a considerable amount of time, effort and specific skills. This paper introduces the development of a set of 30 basic senior personas, covering a broad range of characteristics of European older adults, following a quantitative development approach. The aim of this tool is to support researchers and developers in extending empathy for their target users when developing ICT solutions for the benefit of older adults. The main innovation lies in the representativeness of the basic senior personas. The personas build on multifaceted quantitative data from a single source including micro-level information from roughly 12,500 older individuals living in different European countries. The resulting personas may be applied in their basic form but are extendable to specific contexts. Also, the suggested tool addresses the drawbacks of current existing personas describing older adults: being representative and cost-efficient. The basic senior personas, a filter tool, a manual and templates for ""persona marketing"" articles are available for free online under http://elderlypersonas.cure.at.","Bernhard W&#246;ckl, Ulcay Yildizoglu, Isabella Buber, Belinda Aparicio Diaz, Ernst Kruijff, Manfred Tscheligi","ambient assisted living, basic personas, older adults with diverse capabilities, persona method, persona re-usage, personas layering, user centered design methods, user requirements",25,32
10.1145/2661334.2661367,ASSETS,2014,Text-to-speeches,evaluating the perception of concurrent speech by blind people,"Over the years, screen readers have been an essential tool for assisting blind users in accessing digital information. Yet, its sequential nature undermines blind people's ability to efficiently find relevant information, despite the browsing strategies they have developed. We propose taking advantage of the <i>Cocktail Party Effect</i>, which states that people are able to focus on a single speech source among several conversations, but still identify relevant content in the background. Therefore, oppositely to one sequential speech channel, we hypothesize that blind people can leverage concurrent speech channels to quickly get the gist of digital information. In this paper, we present an experiment with 23 participants, which aims to understand blind people's ability to search for relevant content listening to two, three or four concurrent speech channels. Our results suggest that it is easy to identify the relevant source with two and three concurrent talkers. Moreover, both two and three sources may be used to understand the relevant source content depending on the task intelligibility demands and user characteristics.","Jo&#227;o Guerreiro, Daniel Gon&#231;alves","blind, cocktail party effect, concurrent speech, scanning, screen reader, skimming, visually impaired",169,176
10.1145/2661334.2661369,ASSETS,2014,Tablet-based activity schedule for children with autism in mainstream environment, ,"Including children with Autism Spectrum Disorders (ASD) in mainstreamed environments creates a need for new interventions whose efficacy must be assessed in situ. This paper presents a tablet-based application for activity schedules that has been designed following a participatory design approach involving mainstream teachers, special-education teachers and school aides. This applications addresses two domains of activities: classroom routines and verbal communications. We assessed the efficiency of our application with a study involving 10 children with ASD in mainstream inclusion (5 children are equipped and 5 are not equipped). We show that (1) the use of the application is rapidly self-initiated (after two months for almost all the participants) and that (2) the tablet-supported routines are differently executed over time according to the activity domain conditions. Importantly, compared to the control children, the equipped children exhibited more classroom and communication routines correctly performed after three month of intervention.","Charles Fage, L&#233;onard Pommereau, Charles Consel, &#201;milie Balland, H&#233;l&#232;ne Sauz&#233;on","activity schedules, autism, educative inclusion in mainstreamed environment, idiosyncratic multimedia contents., participatory design, tablet application",145,152
10.1145/3308561.3353801,ASSETS,2019,Am I Too Old to Drive?,Opinions of Older Adults on Self-Driving Vehicles,"Fully autonomous or ""self-driving"" vehicles are an emerging technology that may hold significant mobility potential for both disabled persons and for older adults unable to operate a conventional motor vehicle. It can be argued, however, that the needs, preferences and concerns of older adults and disabled persons regarding this technology have been insufficiently explored. Using focus group methodology, this study explores the sentiments of 39 older adults (55+) regarding self driving vehicle technology. Discussions from the focus groups revealed that although participants believed that self-driving vehicles can enhance their mobility and independence, they were concerned about their reliability and safety. Participants expressed additional concerns regarding their ability to purchase such a vehicle and the training required to operate it. Opinions were mixed regarding the consideration of older adults in the design of the technology.","Earl Huff, Natalie DellaMaria, Brianna Posadas, Julian Brinkley","autonomous vehicles, older adults, self-driving vehicles",500,509
10.1145/3132525.3132529,ASSETS,2017,"""But, I Don't Want/Need a Power Wheelchair""",Toward Accessible Power Assistance for Manual Wheelchairs,"Power assist devices help manual wheelchair users to propel their wheelchair thus increasing their independence and reducing the risk of upper limb injuries due to excessive use. These benefits can be invaluable for people that already have upper limb joint pain and reduced muscular strength. However, it is not clear if the way that assistance is provided by such devices is what manual wheelchair users need and expect. 12 manual wheelchair users were interviewed to understand: the situations in which they find it difficult to propel their wheelchairs; situations they considered paramount to have power assistance; their experience or knowledge of power assist devices; and likes and dislikes of commercially available power assist devices. Finally, they were asked to comment on their ideal form factor of a power assist device. Users have suggested improvements of the devices' accessibility and visualized new ways in which they could interact with the technology. These interactions involve ""chairable"" devices independent from, but not excluding, wearable devices and mobile applications. We have identified the need of monitoring emotions and the need for designing an open source do-it-yourself wheelchair propelling assistance device which we believe is required equally in developed and in developing countries.","Dafne Zuleima Morgado Ramirez, Catherine Holloway","accessibility, assistive technology, human-centered, interaction design, interviews, manual wheelchair, participatory design, power assist device",120,129
10.1145/3308561.3353809,ASSETS,2019,Delivering Sign Language in a Live Planetarium Show Using Head-Mounted Displays and Infrared Light, ,Sign language narration is difficult to view at live planetarium shows because the room is dark and the signer is not located near images projected onto the planetarium dome. We have designed and implemented a system using head-mounted displays (HMDs) and infrared light to support viewing real-time sign language narration of live planetarium shows. Results from a series of 3 studies involving 29 students who are deaf or hard-of-hearing suggest that viewing properly configured video of sign language narration in an HMD may increase learning in this setting. Participants expressed no single preference regarding signer position in the video feed but did indicate that the relative brightness of the HMD must be tuned to match the apparent brightness of images broadcast on the planetarium dome. We also identified other issues related to HMD fit and the appearance of the signer in the video.,"Michael Jones, M. Lawler","head mounted display, planetarium, sign language",396,401
10.1145/2661334.2661377,ASSETS,2014,BraillePlay,educational smartphone games for blind children,"There are many educational smartphone games for children, but few are accessible to blind children. We present BraillePlay, a suite of accessible games for smartphones that teach Braille character encodings to promote Braille literacy. The BraillePlay games are based on VBraille, a method for displaying Braille characters on a smartphone. BraillePlay includes four games of varying levels of difficulty: VBReader and VBWriter simulate Braille flashcards, and VBHangman and VBGhost incorporate Braille character identification and recall into word games. We evaluated BraillePlay with a longitudinal study in the wild with eight blind children. Through logged usage data and extensive interviews, we found that all but one participant were able to play the games independently and found them enjoyable. We also found evidence that some children learned Braille concepts. We distill implications for the design of games for blind children and discuss lessons learned.","Lauren Milne, Cynthia Bennett, Richard Ladner, Shiri Azenkot","accessibility, blind, children, educational games",137,144
10.1145/2982142.2982162,ASSETS,2016,Uncovering Challenges and Opportunities for 3D Printing Assistive Technology with Physical Therapists, ,"Physical therapists have a history of modifying and making assistive technology (AT) to fit the unique needs of their patients. However, lack of materials, time, and access to training can restrict what they can create. While 3D printing has the opportunity to empower physical therapists to develop highly customized, economical, and timely assistive technology; little is known about the feasibility of using 3D printing in a clinical setting, and how to teach and engage physical therapists in physical prototyping. We collaborated with physical therapy professors and students at a medical university to integrate 3D printing and AT design into a graduate-level physical therapy class. Our investigation showed 3D printing is a viable tool for clinical production of AT. We found opportunities and barriers to 3D printing in the physical therapy field, and we present four considerations relevant to integrating 3D printing into clinical practice: 1) exploring augmentations versus novel AT designs, 2) improvements to novice 3D modeling software, 3) adjusting for prototype fidelity, and 4) selecting 3D printing materials. This paper contributes knowledge toward the understanding of practical applications of 3D printing in a clinical setting and teaching 3D modeling to non-engineers.","Samantha McDonald, Niara Comrie, Erin Buehler, Nicholas Carter, Braxton Dubin, Karen Gordes, Sandy McCombe-Waller, Amy Hurst","3d modeling, 3d printing, assistive technology, digital fabrication, education, physical fabrication, physical therapy.",131,139
10.1145/2513383.2517029,ASSETS,2013,"Visual complexity, player experience, performance and physical exertion in motion-based games for older adults", ,"Motion-based video games can have a variety of benefits for the players and are increasingly applied in physical therapy, rehabilitation and prevention for older adults. However, little is known about how this audience experiences playing such games, how the player experience affects the way older adults interact with motion-based games, and how this can relate to therapy goals. In our work, we decompose the player experience of older adults engaging with motion-based games, focusing on the effects of manipulations of the game representation through the visual channel (<i>visual complexity</i>), since it is the primary interaction modality of most games and since vision impairments are common amongst older adults. We examine the effects of different levels of visual complexity on player experience, performance, and exertion in a study with fifteen participants. Our results show that visual complexity affects the way games are perceived in two ways: First, while older adults do have preferences in terms of visual complexity of video games, notable effects were only measurable following drastic variations. Second, perceived exertion shifts depending on the degree of visual complexity. These findings can help inform the design of motion-based games for therapy and rehabilitation for older adults.","Jan Smeddinck, Kathrin Gerling, Saranat Tiemkeo","accessibility, design, entertainment, motion-based games, older adults, serious games, visual complexity",1,8
10.1145/1878803.1878834,ASSETS,2010,Broadening accessibility through special interests,a new approach for software customization,"Individuals diagnosed with autism spectrum disorder (ASD) often fixate on narrow, restricted interests. These interests can be highly motivating, but they can also create attentional myopia, preventing individuals from pursuing a broad range of activities. Interestingly, researchers have found that preferred interests can be used to help individuals with ASD branch out and participate in educational, therapeutic, or social situations they might otherwise shun. When interventions are modified, such that an individual's interest is properly represented, task adherence and performance can increase. While this strategy has seen success in the research literature, it is difficult to implement on a large scale and therefore has not been widely adopted. This paper describes a software approach designed to solve this problem. The approach facilitates customization, allowing users to easily embed images of almost any special interest into computer-based interventions. Specifically, we describe an algorithm that will: (1) retrieve any image from the Google image database; (2) strip it of its background; and (3) embed it seamlessly into Flash-based computer programs. To evaluate our algorithm, we employed it in a naturalistic setting with eleven individuals (nine diagnosed with ASD and two diagnosed with other developmental disorders). We also tested its ability to retrieve and process examples of preferred interests previously reported in the ASD literature. The results indicate that our method was an easy and efficient way for users to customize our software programs. While we believe this model is uniquely suited for individuals with ASD, we also foresee this approach being useful for anyone that might like a quick and simple way to personalize software programs.","Robert Morris, Connor Kirschbaum, Rosalind Picard","autism, preferred interests, software and technology design",171,178
10.1145/2049536.2049552,ASSETS,2011,Humsher,a predictive keyboard operated by humming,"This paper presents Humsher -- a novel text entry method operated by the non-verbal vocal input, specifically the sound of humming. The method utilizes an adaptive language model for text prediction. Four different user interfaces are presented and compared. Three of them use dynamic layout in which n-grams of characters are presented to the user to choose from according to their probability in given context. The last interface utilizes static layout, in which the characters are displayed alphabetically and a modified binary search algorithm is used for an efficient selection of a character. All interfaces were compared and evaluated in a user study involving 17 able-bodied subjects. Case studies with four disabled people were also performed in order to validate the potential of the method for motor-impaired users. The average speed of the fastest interface was 14 characters per minute, while the fastest user reached 30 characters per minute. Disabled participants were able to type at 14 -- 22 characters per minute after seven sessions.","Ondrej Polacek, Zdenek Mikovec, Adam Sporka, Pavel Slavik","adaptive language model, assistive technology, non-verbal vocal interface, predictive keyboard, text input",75,82
10.1145/2049536.2049573,ASSETS,2011,Supporting blind photography, ,"Blind people want to take photographs for the same reasons as others -- to record important events, to share experiences, and as an outlet for artistic expression. Furthermore, both automatic computer vision technology and human-powered services can be used to give blind people feedback on their environment, but to work their best these systems need high-quality photos as input. In this paper, we present the results of a large survey that shows how blind people are currently using cameras. Next, we introduce EasySnap, an application that provides audio feedback to help blind people take pictures of objects and people and show that blind photographers take better photographs with this feedback. We then discuss how we iterated on the portrait functionality to create a new application called PortraitFramer designed specifically for this function. Finally, we present the results of an in-depth study with 15 blind and low-vision participants, showing that they could pick up how to successfully use the application very quickly.","Chandrika Jayant, Hanjie Ji, Samuel White, Jeffrey Bigham","blind, camera, photography, visually impaired",203,210
10.1145/3132525.3132550,ASSETS,2017,Interviews and Observation of Blind Software Developers at Work to Understand Code Navigation Challenges, ,"Integrated Development Environments (IDEs) play an important role in the workflow of many software developers, e.g. providing syntactic highlighting or other navigation aids to support the creation of lengthy codebases. Unfortunately, such complex visual information is difficult to convey with current screen-reader technologies, thereby creating barriers for programmers who are blind, who are nevertheless using IDEs. To better understand their usage strategies and challenges, we conducted an exploratory study to investigate the issue of code navigation by developers who are blind. We observed 28 blind programmers using their preferred coding tool while they performed various programming activities, in particular while they navigated through complex codebases. Participants encountered many navigation difficulties when using their preferred coding software with assistive technologies (e.g., screen readers). During interviews, participants reported dissatisfaction with the accessibility of most IDEs due to the heavy use of visual abstractions. To compensate, participants used multiple input methods and workarounds to navigate through code comfortably and reduce complexity, but these approaches often reduced their speed and introduced mistakes, thereby reducing their efficiency as programmers. Our findings suggest an opportunity for researchers and the software industry to improve the accessibility and usability of code navigation for blind developers in IDEs.","Khaled Albusays, Stephanie Ludi, Matt Huenerfauth","accessibility, blind programmers, code navigation difficulties, programming challenges, user studies",91,100
10.1145/2700648.2809851,ASSETS,2015,Disability and Technology,A Critical Realist Perspective,"Assistive technology (AT) as a field explores the design, use and evaluation of computing technology that aims to benefit people with disabilities. The majority of the work consequently takes the functional needs of people with disabilities as starting point and matches those with technological opportunity spaces. With this paper, we argue that the underlying philosophical position implied in this approach can be seen as reductionist as the disabled experience is arguably richer and often more complex as can be projected from the functional limitations of people. Thinkers and activists in Disability Studies have conceptualised disability in various ways and more recently, critical realism was proposed as a philosophical position through which the many different facets of the disabled experience could be incorporated. In this paper, we explore the possibility of using a critical realist perspective to guide designers in developing technology for people with disabilities and thereby aim to contribute to the philosophical underpinnings of AT. After a brief review of historical conceptualisations of disability, we introduce the critical realist argument and discuss its appeal for understanding disability and the possible roles technology can have in this context. Subsequently, we aim to translate this philosophical and moral debate into a research agenda for AT and exemplify how it can be operationalised by presenting the OutsideTheBox project as a case study.",Christopher Frauenberger,"critical realism, disability studies, philosophy of science",89,96
10.1145/2049536.2049566,ASSETS,2011,Monitoring accessibility,large scale evaluations at a Geo political level,"Once we assumed that Web accessibility is a right, we implicitly state the necessity of a governance of it. Beyond any regulation, institutions must provide themselves with suitable tools to control and support accessibility on typically large scale scenarios of content and resources. No doubt, the economic impact and effectiveness of these tools affect accessibility level. In this paper, we propose an application to effectively monitor Web accessibility from a geo-political point of view, by referring resources to the specific (category of) institutions which are in charge of it and to the geographical places they are addressed to. Snapshots of such a macro level spatial-geo-political analysis can be used to effectively focus investments and skills where they are actually necessary.","Silvia Mirri, Ludovico Muratori, Paola Salomoni","accessibility evaluation, automated evaluation, web accessibility",163,170
10.1145/1878803.1878820,ASSETS,2010,Evaluating a tool for improving accessibility to charts and graphs, ,"We discuss factors in the design and evaluation of natural language-driven assistive technologies that generate descriptions of, and allow interaction with, graphical representations of numerical data. In particular, we provide data in favor of 1) screen-reading technologies as a usable, useful, and cost-effective means of interacting with graphs. The data also show that by carrying out evaluation of Assistive Technologies on populations other than the target communities, certain subtleties of navigation and interaction may be lost or distorted.","Leo Ferres, Gitte Lindgaard, Livia Sumegi","accessibility (blind and visually-impaired), natural language generation and interaction, statistical graphs and diagrams",83,90
10.1145/2513383.2513453,ASSETS,2013,Improved inference and autotyping in EEG-based BCI typing systems, ,"The RSVP Keyboard&#8482; is a brain-computer interface (BCI)-based typing system for people with severe physical disabilities, specifically those with locked-in syndrome (LIS). It uses signals from an electroencephalogram (EEG) combined with information from an n-gram language model to select letters to be typed. One characteristic of the system as currently configured is that it does not keep track of past EEG observations, i.e., observations of user intent made while the user was in a different part of a typed message. We present a principled approach for taking <i>all</i> past observations into account, and show that this method results in a 20% increase in simulated typing speed under a variety of conditions on realistic stimuli. We also show that this method allows for a principled and improved estimate of the probability of the backspace symbol, by which mis-typed symbols are corrected. Finally, we demonstrate the utility of automatically typing likely letters in certain contexts, a technique that achieves increased typing speed under our new method, though not under the baseline approach.","Andrew Fowler, Brian Roark, Umut Orhan, Deniz Erdogmus, Melanie Fried-Oken","brain computer interfaces, language models, text entry",1,8
10.1145/3308561.3353811,ASSETS,2019,&#956;Graph&#58; Haptic Exploration and Editing of 3D Chemical Diagrams, ,"People with visual impairments or blindness (VIB) encounter difficulties in exploring graphical representations that are widely used for the study of STEM subjects. In particular, graphs are used to represent many different scientific notations: flowcharts, automata, cognitive maps, and more. Among these, structural chemical formulae are characterized by a complex, often 3-dimensional structure, which makes them hard to access and author with traditional assistive tools. We propose MuGraph, a multimodal system that combines haptic and speech feedback to enable people with VIB to explore and edit structural chemical formulae. Two main contributions are presented: (i) a novel, non-visual interaction paradigm for exploring graphs and its implementation in the MuGraph system, and (ii) an extensive evaluation of the proposed system with 10 participants with VIB showing that MuGraph is thoroughly accessible and that the haptic feedback enhances understanding of the geometric properties of a graph.","Cristian Bernareggi, Dragan Ahmetovic, Sergio Mascetti","graph accessibility, stem education, visual impairment",312,317
10.1145/3308561.3353803,ASSETS,2019,A Community-Centered Design Framework for Robot-Assisted Feeding Systems, ,"Robot-assisted feeding (RAF) systems offer enormous potential benefits to community-centered care-giving environments. However, developers of RAF technologies often focus on evaluating their standard transactional functionality, omitting the impact of such technologies in contexts that extend past the interaction of the robot and food receiver. RAF technologies have complex social, cultural and self-identity implications, since a ""meal"" extends well beyond the simple provisioning of nourishment. To better understand these implications we conducted a contextual inquiry in an assisted-living community with five potential care recipients and five caregivers, as well as interviews with fifteen domain experts including occupational therapists and feeding specialists. Based on our findings from these studies, we developed a new framework for RAF technologies that formulates this vital task as a community-centered relational service. We then use this framework to qualitatively and quantitatively assess three existing feeding systems and identify areas of improvement. Our work reveals new insights about stakeholders of RAF technologies and provides a roadmap for technology developers to better serve the needs of these stakeholders.","Tapomayukh Bhattacharjee, Maria Cabrera, Anat Caspi, Maya Cakmak, Siddhartha Srinivasa","assistive feeding, assistive robotics",482,494
10.1145/2661334.2661381,ASSETS,2014,Enhancing caption accessibility through simultaneous multimodal information,visual-tactile captions,"Captions (subtitles) for television and movies have greatly enhanced accessibility for Deaf and hard of hearing (DHH) consumers who do not understand the audio, but can otherwise follow by reading the captions. However, these captions fail to fully convey auditory information, due to <i>simultaneous delivery of aural and visual content</i>, and <i>lack of standardization in representing non-speech information</i>. Viewers cannot simultaneously watch the movie scenes and read the visual captions; instead they have to switch between the two and inevitably lose information and context in watching the movies. In contrast, hearing viewers can simultaneously listen to the audio and watch the scenes. Most auditory non-speech information (NSI) is not easily represented by words, e.g., the description of a ring tone, or the sound of something falling. We enhance captions with tactile and visual-tactile feedback. For the former, we transform auditory NSI into its equivalent tactile representation and convey it simultaneously with the captions. For the latter, we visually identify the location of the NSI. This approach can benefit DHH viewers by conveying more aural content to the viewer's visual and tactile senses simultaneously than visual-only captions alone. We conducted a study, which compared DHH viewer responses between video with captions, tactile captions, and visual-tactile captions. The viewers significantly benefited from visual-tactile and tactile captions.","Raja Kushalnagar, Gary Behm, Joseph Stanislow, Vasu Gupta","aural-to-tactile information, caption readability, deaf and hard of hearing users, multi-modal interfaces",185,192
10.1145/3132525.3132538,ASSETS,2017,Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface, ,"Precisely guiding a blind person's hand can be useful for a range of applications from tracing printed text to learning and understanding shapes and gestures. In this paper, we evaluate wrist-worn haptics as a directional hand guide. We implemented and evaluated the following haptic wristband variations: (1) four versus eight vibromotor designs; (2) vibration from only a single motor at a time versus from two adjacent motors using interpolation. To evaluate our designs, we conducted two studies: Study 1 (N=13, 2 blind) showed that participants could non-visually find targets and trace paths more quickly and accurately with single-motor feedback than with interpolated feedback, particularly when only four motors were used. Study 2 (N=14 blind or visually impaired participants) found that single-motor feedback with four motors was faster, more accurate, and most preferred compared to similar feedback with eight motors. We derive implications for the design of wrist-worn directional haptic feedback and discuss future work.","Jonggi Hong, Alisha Pradhan, Jon Froehlich, Leah Findlater","accessibility, blind user, haptic feedback, haptic wristband, wearable computing",210,219
10.1145/3132525.3132536,ASSETS,2017,Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction, ,"For screenreader users who are blind or visually impaired (VI), today's mobile devices, while reasonably accessible, are not necessarily efficient. This inefficiency may be especially problematic for microinteractions, which are brief but high-frequency interactions that take only a few seconds for sighted users to complete (e.g., checking the weather or for new messages). One potential solution to support efficient non-visual microinteractions is on-body input, which appropriates the user's own body as the interaction medium. In this paper, we address two related research questions: How well are microinteractions currently supported for VI users' How should on-body interaction be designed to best support microinteractions for this user group? We conducted two studies: (1) an online survey to compare current microinteraction use between VI and sighted users (N=117); and (2) an in-person study where 12 VI screenreader users qualitatively evaluated a real-time on-body interaction system that provided three contrasting input designs. Our findings suggest that efficient microinteractions are not currently well-supported for VI users, at least using manual input, which highlights the need for new interaction approaches. On-body input offers this potential and the qualitative evaluation revealed tradeoffs with different on-body interaction techniques in terms of perceived efficiency, learnability, social acceptability, and ability to use on the go.","Uran Oh, Lee Stearns, Alisha Pradhan, Jon Froehlich, Leah Findlater","microinteraction, mobile, on-body interaction, visual impairments, wearable technology",22,31
10.1145/2661334.2661368,ASSETS,2014,Evaluating the accessibility of line graphs through textual summaries for visually impaired users, ,This paper presents the methodology for generating textual summaries of line graphs in the SIGHT (<i>Summarizing Information GrapHics Textually</i>) system and the evaluation of line graph summaries produced by SIGHT. The system is designed to deliver the high-level knowledge conveyed by informational graphics present in online popular media articles (newspaper and magazines) to individuals who do not have visual access to the image. It works by producing and delivering a concise summary of the graph's content including the most important visual features present in the graphic. The system is briefly described; the evaluation compares the utility of the generated textual summaries to visually viewing the graphic in order to answer important questions about the line graph.,"Priscilla Moraes, Gabriel Sina, Kathleen McCoy, Sandra Carberry","accessibility, information graphics, natural language generation, visual impairments",83,90
10.1145/2384916.2384939,ASSETS,2012,Elderly text-entry performance on touchscreens, ,"Touchscreen devices have become increasingly popular. Yet they lack of tactile feedback and motor stability, making it difficult effectively typing on virtual keyboards. This is even worse for elderly users and their declining motor abilities, particularly hand tremor. In this paper we examine text-entry performance and typing patterns of elderly users on touch-based devices. Moreover, we analyze users' hand tremor profile and its relationship to typing behavior. Our main goal is to inform future designs of touchscreen keyboards for elderly people. To this end, we asked 15 users to enter text under two device conditions (mobile and tablet) and measured their performance, both speed- and accuracy-wise. Additionally, we thoroughly analyze different types of errors (insertions, substitutions, and omissions) looking at touch input features and their main causes. Results show that omissions are the most common error type, mainly due to cognitive errors, followed by substitutions and insertions. While tablet devices can compensate for about 9% of typing errors, omissions are similar across conditions. Measured hand tremor largely correlates with text-entry errors, suggesting that it should be approached to improve input accuracy. Finally, we assess the effect of simple touch models and provide implications to design.","Hugo Nicolau, Joaquim Jorge","elderly, mobile, tablet, text-entry, touchscreen, tremor",127,134
10.1145/2982142.2982168,ASSETS,2016,How People with Low Vision Access Computing Devices,Understanding Challenges and Opportunities,"Low vision is a pervasive condition in which people have difficulty seeing even with corrective lenses. People with low vision frequently use mainstream computing devices, however how they use their devices to access information and whether digital low vision accessibility tools provide adequate support remains understudied. We addressed these questions with a contextual inquiry study. We observed 11 low vision participants using their smartphones, tablets, and computers when performing simple tasks such as reading email. We found that participants preferred accessing information visually than aurally (e.g., screen readers), and juggled a variety of accessibility tools. However, accessibility tools did not provide them with appropriate support. Moreover, participants had to constantly perform multiple gestures in order to see content comfortably. These challenges made participants inefficient-they were slow and often made mistakes; even tech savvy participants felt frustrated and not in control. Our findings reveal the unique needs of low vision people, which differ from those of people with no vision and design opportunities for improving low vision accessibility tools.","Sarit Szpiro, Shafeka Hashash, Yuhang Zhao, Shiri Azenkot","accessibility, computing devices, contextual inquiry, low vision",171,180
10.1145/2661334.2661365,ASSETS,2014,ABC and 3D,opportunities and obstacles to 3D printing in special education environments,"Consumer-grade digital fabrication such as 3D printing is on the rise, and we believe it can be leveraged to great benefit in the arena of special education. Although 3D printing is beginning to infiltrate mainstream education, little to no research has explored 3D printing in the context of students with special support needs. We present a formative study exploring the use of 3D printing at three locations serving populations with varying ability, including individuals with cognitive, motor, and visual impairments. We found that 3D design and printing performs three functions in special education: developing 3D design and printing skills encourages STEM engagement; 3D printing can support the creation of educational aids for providing accessible curriculum content; and 3D printing can be used to create custom adaptive devices. In addition to providing opportunities to students, faculty, and caregivers in their efforts to integrate 3D printing in special education settings, our investigation also revealed several concerns and challenges. We present our investigation at three diverse sites as a case study of 3D printing in the realm of special education, discuss obstacles to efficient 3D printing in this context, and offer suggestions for designers and technologists.","Erin Buehler, Shaun Kane, Amy Hurst","3d printing, assistive technology, children, cognitive impairment, developmental disability, digital fabrication, rapid prototyping, special education, visual impairment",107,114
10.1145/3308561.3353773,ASSETS,2019,VERSE,Bridging Screen Readers and Voice Assistants for Enhanced Eyes-Free Web Search,"People with visual impairments often rely on screen readers when interacting with computer systems. Increasingly, these individuals also make extensive use of voice-based virtual assistants (VAs). We conducted a survey of 53 people who are legally blind to identify the strengths and weaknesses of both technologies, and the unmet opportunities at their intersection. We learned that virtual assistants are convenient and accessible, but lack the ability to deeply engage with content (e.g., read beyond the first few sentences of an article), and the ability to get a quick overview of the landscape (e.g., list alternative search results and suggestions). In contrast, screen readers allow for deep engagement with content (when content is accessible), and provide fine-grained navigation and control, but at the cost of reduced walk-up-and-use convenience. Based on these findings, we implemented VERSE (Voice Exploration, Retrieval, and SEarch), a prototype that extends a VA with screen-reader-inspired capabilities, and allows other devices (e.g., smartwatches) to serve as optional input accelerators. In a usability study with 12 blind screen reader users we found that VERSE meaningfully extended VA functionality. Participants especially valued having access to multiple search results and search verticals.","Alexandra Vtyurina, Adam Fourney, Meredith Morris, Leah Findlater, Ryen White","non-visual search, voice search",414,426
10.1145/1878803.1878828,ASSETS,2010,Note-taker 2.0,the next step toward enabling students who are legally blind to take notes in class,"In-class note-taking is a vital learning activity in secondary and post-secondary classrooms. The process of note-taking helps students stay focused on the instruction, forces them to cognitively process what is being presented, and better retain what has been taught, even if they never refer to their notes after the class. However, note-taking is difficult for students with low vision, or who are legally blind for two reasons. First, they are less able to see what is being presented at the front of them room, and second, they must repeatedly switch between the far-sight task of viewing the front of the room, and the near-sight task of taking notes. This paper describes ongoing research aimed at developing a portable assistive device (called the Note-Taker) that a student can take to class, to assist in the process of taking notes. It describes the principles that have guided the development of the proof-of-concept Note-Taker prototype and the Note-Taker 2.0 prototype. Initial testing of those prototypes has been encouraging, but some significant problems remain to be solved. Proposed solutions are currently being implemented, and appear to be effective. If ongoing usability testing confirms their effectiveness, they will be implemented on the planned Note-Taker 3.0 prototype.","David Hayden, Liqing Zhou, Michael Astrauskas, John Black","lecture notes, legal blindness, low vision, note-taker, note-taking",131,138
10.1145/2513383.2513442,ASSETS,2013,Touchplates,low-cost tactile overlays for visually impaired touch screen users,"Adding tactile feedback to touch screens can improve their accessibility to blind users, but prior approaches to integrating tactile feedback with touch screens have either offered limited functionality or required extensive (and typically expensive) customization of the hardware. We introduce <i>touchplates</i>, carefully designed tactile guides that provide tactile feedback for touch screens in the form of physical guides that are overlaid on the screen and recognized by the underlying application. Unlike prior approaches to integrating tactile feedback with touch screens, touchplates are implemented with simple plastics and use standard touch screen software, making them versatile and inexpensive. Touchplates may be customized to suit individual users and applications, and may be produced on a laser cutter, 3D printer, or made by hand. We describe the design and implementation of touchplates, a ""starter kit"" of touchplates, and feedback from a formative evaluation with 9 people with visual impairments. Touchplates provide a low-cost, adaptable, and accessible method of adding tactile feedback to touch screen interfaces.","Shaun Kane, Meredith Morris, Jacob Wobbrock","accessibility, blindness, guides, hardware, touch screens, touchplates, visual impairments",1,8
10.1145/3234695.3236361,ASSETS,2018,Design of an Augmented Reality Magnification Aid for Low Vision Users, ,"Augmented reality (AR) systems that enhance visual capabilities could make text and other fine details more accessible for low vision users, improving independence and quality of life. Prior work has begun to investigate the potential of assistive AR, but recent advancements enable new AR visualizations and interactions not yet explored in the context of assistive technology. In this paper, we follow an iterative design process with feedback and suggestions from seven visually impaired participants, designing and testing AR magnification ideas using the Microsoft HoloLens. Participants identified several advantages to the concept of head-worn magnification (e.g., portability, privacy, ready availability), and to our AR designs in particular (e.g., a more natural reading experience and the ability to multitask). We discuss the strengths and weaknesses of this AR magnification approach and summarize lessons learned throughout the process.","Lee Stearns, Leah Findlater, Jon Froehlich","assistive technology, augmented reality, low vision, magnification, visual impairment, wearables",28,39
10.1145/1878803.1878823,ASSETS,2010,Modeling and synthesizing spatially inflected verbs for American sign language animations, ,"Animations of American Sign Language (ASL) have accessibility benefits for many signers with lower levels of written language literacy. This paper introduces a novel method for modeling and synthesizing ASL animations based on movement data collected from native signers. This technique allows for the synthesis of animations of signs (in particular, inflecting verbs, which are frequent in ASL) whose performance is affected by the arrangement of locations in 3D space that represent entities under discussion. Mathematical models of hand movement are trained on examples of signs produced by a human animator. Animations of ASL synthesized from the model were judged to be of similar quality to animations produced by a human animator, and these animations led to higher comprehension scores (than baseline approaches limited to selecting signs from a finite dictionary) in an evaluation study conducted with 18 native signers. This novel technique is applicable to ASL or other sign languages. It can significantly increase the repertoire of generation systems and can partially automate the work of humans using scripting systems.","Matt Huenerfauth, Pengfei Lu","American sign language, accessibility technology for people who are deaf, animation, natural language generation",99,106
10.1145/2513383.2513444,ASSETS,2013,An empirical study of issues and barriers to mainstream video game accessibility, ,"A gap between the academic human-computer interaction community and the game development industry has led to games not being as thoroughly influenced by accessibility standards as most other facets of information and communication technology. As a result, individuals with disabilities are unable to fully, if at all, engage with many commercial games. This paper presents the findings of a pair of complementary empirical studies intended to understand the current state of game accessibility in a grounded, real-world context and identify issues and barriers. The first study involved an online survey of 55 gamers with disabilities to elicit information about their play habits, experiences, and accessibility issues. The second study consisted of a series of semi-structured interviews with individuals from the game industry to better understand accessibility's situation in their design and development processes. Through quantitative and qualitative thematic analysis, we derive high-level insights from the data, such as the prevalence of assistive technology incompatibility and the value of middleware for implementing accessibility standardization. Finally, we discuss specific implications and how these insights can be used to define future work which may help to narrow the gap.","John Porter, Julie Kientz","access technology, accessibility, video games",1,8
10.1145/3132525.3132551,ASSETS,2017,Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior, ,"Overselectivity is a learning challenge that is largely unaddressed in the assistive technology community. Screening and intervention, done by specialists, is time-intensive and requires substantial training. Little to no treatments are available to the broader population of preliterate, minimally verbal individuals. In this work, we examine the impact of an iPad game based on the tenets of behavioral therapy to mitigate overselectivity. We developed software-based techniques and evaluated the system using established methods from the field of Special Education. We present the results of a deployment in a special education school that demonstrates that an assistive tablet game is a feasible means of addressing overselectivity, and we present generalizable technological features drawn from evidenced-based therapies to consider in future assistive technologies. We suggest that designers of assistive technology systems, particularly those who address physical, cognitive, and behavioral difficulties for preliterate AAC users, should consider overselectivity as a potential co-occurring condition.","LouAnne Boyd, Kathryn Ringland, Heather Faucett, Alexis Hiniker, Kimberley Klein, Kanika Patel, Gillian Hayes","aac, assistive technology, autism, children, language development, multiple cue responding, overselectivity, tablet games",240,249
10.1145/2513383.2513439,ASSETS,2013,AphasiaWeb,a social network for individuals with aphasia,"With the rise of social networks like Facebook and Twitter, it might seem that our opportunity to communicate with others is limited only by our access to smart phones and computers. However, most social networks are not designed with complete accessibility in mind. In particular, these networks' chronological organization of news items, abundant feature sets, and busy presentation can make these tools unusable to individuals with aphasia, an acquired language disorder that compromises an individual's ability to speak, write, and recognize language. This is unfortunate, as one of the primary means of managing aphasia is to keep individuals in community. To counter this, we have developed AphasiaWeb, a social network designed exclusively for keeping individuals with aphasia and their friends and families connected. In this paper we describe the social network and share findings from a two-month trial program conducted with a local aphasia support group.","Hannah Miller, Heather Buhr, Chris Johnson, Jerry Hoepner","aphasia, social networks",1,8
10.1145/2049536.2049569,ASSETS,2011,Blind people and mobile touch-based text-entry,acknowledging the need for different flavors,"The emergence of touch-based mobile devices brought fresh and exciting possibilities. These came at the cost of a considerable number of novel challenges. They are particularly apparent with the blind population, as these devices lack tactile cues and are extremely visually demanding. Existing solutions resort to assistive screen reading software to compensate the lack of sight, still not all the information reaches the blind user. Good spatial ability is still required to have notion of the device and its interface, as well as the need to memorize buttons' position on screen. These abilities, as many other individual attributes as age, age of blindness onset or tactile sensibility are often forgotten, as the blind population is presented with the same methods ignoring capabilities and needs. Herein, we present a study with 13 blind people consisting of a touch screen text-entry task with four different methods. Results show that different capability levels have significant impact on performance and that this impact is related with the different methods' demands. These variances acknowledge the need of accounting for individual characteristics and giving space for difference, towards inclusive design.","Jo&#227;o Oliveira, Tiago Guerreiro, Hugo Nicolau, Joaquim Jorge, Daniel Gon&#231;alves","blind, individual differences, mobile, text-entry, touch screens",179,186
10.1145/1878803.1878812,ASSETS,2010,Accessibility by demonstration,enabling end users to guide developers to web accessibility solutions,"Few web developers have been explicitly trained to create accessible web pages, and are unlikely to recognize subtle accessibility and usability concerns that disabled people face. Evaluating web pages with assistive technology can reveal problems, but this software takes time to install and its complexity can be overwhelming. To address these problems, we introduce a new approach for accessibility evaluation called <i>Accessibility by Demonstration (ABD)</i>. ABD lets assistive technology users retroactively record accessibility problems at the time they experience them as human-readable macros and easily send those recordings and the software necessary to replay them to others. This paper describes an implementation of ABD as an extension to the WebAnywhere screen reader, and presents an evaluation with 15 web developers not experienced with accessibility showing that interacting with these recordings helped them understand and fix some subtle accessibility problems better than existing tools.","Jeffrey Bigham, Jeremy Brudvik, Bernie Zhang","blind users, evaluation, web accessibility, web usability",35,42
10.1145/3132525.3132541,ASSETS,2017,Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings, ,"Recent advances in Automatic Speech Recognition (ASR) have made this technology a potential solution for transcribing audio input in real-time for people who are Deaf or Hard of Hearing (DHH). However, ASR is imperfect; users must cope with errors in the output. While some prior research has studied ASR-generated transcriptions to provide captions for DHH people, there has not been a systematic study of how to best present captions that may include errors from ASR software nor how to make use of the ASR system's word-level confidence. We conducted two studies, with 21 and 107 DHH participants, to compare various methods of visually presenting the ASR output with certainty values. Participants answered subjective preference questions and provided feedback on how ASR captioning could be used with confidence display markup. Users preferred captioning styles with which they were already most familiar (that did not display confidence information), and they were concerned about the accuracy of ASR systems. While they expressed interest in systems that display word confidence during captions, they were concerned that text appearance changes may be distracting. The findings of this study should be useful for researchers and companies developing automated captioning systems for DHH users.","Larwan Berke, Christopher Caulfield, Matt Huenerfauth","automatic speech recognition, communication, deaf and hard of hearing, feedback, real-time captions, user study",155,164
10.1145/2384916.2384931,ASSETS,2012,Web accessibility as a side effect, ,"This paper explores evidence for the conjecture that improvements in Web accessibility have arisen, in part, as side effects of changes in Web technology and associated shifts in the way Web pages are designed and coded. Drawing on an earlier study of Web accessibility trends over the past 14 years, it discusses several possible indirect contributors to improving accessibility including the use of new browser capabilities to create more sophisticated page layouts, a growing concern with improved page rank in search results, and a shift toward cross-device content design. Understanding these examples may inspire the creation of additional technologies with incidental accessibility benefits.","John Richards, Kyle Montague, Vicki Hanson",web accessibility,79,86
10.1145/2982142.2982174,ASSETS,2016,Online Learning System to Help People with Developmental Disabilities Reinforce Basic Skills, ,"We present the development and evaluation of an online learning system for people with developmental disabilities (DD) of all ages in collaboration with Imagine! and Hope Services, two not-for-profit organizations that provide care services to people with DD. The system was implemented as an HTML5-based web application for iPad. It includes activities that aim to support and improve the process through which people with DD of all ages reinforce basic skills such as recognizing numbers, letters, money, shapes, and colors. User evaluations suggest that a system such as ours will be: 1) helpful in supporting people with DD reinforce basic skills, and 2) well-received by users.","Lourdes Morales-Villaverde, Karina Caro, Taylor Gotfrid, Sri Kurniawan","basic skills, developmental disability, ipad, online learning, web application",43,51
10.1145/3308561.3353784,ASSETS,2019,CHIMELIGHT&#58; Augmenting Instruments in Interactive Music Therapy for Children with Neurodevelopmental Disorders, ,"In this paper, we propose a new mobile system to support therapists for teaching and tracking socio-communicative behaviors in children with neurodevelopmental disorders during music therapy sessions. The CHIMELIGHT system was designed to deal with the current issues in conventional therapies, such as the difficulty in both evaluating the performance and maintaining engagement of these children during therapeutic activities. The system evaluated movements made by a child with neurodevelopmental disorders playing a musical instrument while delivering contingent visual feedback based on real-time motion analysis. A set of metrics was implemented to evaluate the performance during the therapy activity and quantify specific target behaviors. An evaluation study performed during music therapy group sessions showed that the CHIMELIGHT-delivered visual feedback increased the engagement of children in the activity and decreased targeted negative behaviors. In some participants, we observed potential changes in their positive behaviors. Interviews and questionnaires provided to therapists showed that the developed system was effective for supporting evidence-based music therapy. Accordingly, our research enables new methods for both interactive therapy and mediation of the interaction between therapists and children with neurodevelopmental disorders.","Joana Lobo, Soichiro Matsuda, Izumi Futamata, Ryoichi Sakuta, Kenji Suzuki","assistive technology, interactive therapy, iot, music therapy, neurodevelopmental disorders, social playware, socio-communicative behaviors",124,135
10.1145/1878803.1878819,ASSETS,2010,Designing auditory cues to enhance spoken mathematics for visually impaired users, ,"Visual mathematic notation provides a succinct and unambiguous description of the structure of mathematical formulae in a manner that is difficult to replicate through the linear channels of synthesized speech and Braille. It is proposed that the use of auditory cues can enhance accessibility to mathematical material and reduce common ambiguities encountered through spoken mathematics. However, the use of additional complex hierarchies of non-speech sounds to represent the structure and scope of equations may be cognitively demanding to process. This can detract from the users' understanding of the mathematical content. In this paper, a new system is presented, which uses a mixture of non-speech auditory cues, modified speech (spearcons) and binaural spatialization to disambiguate the structure of mathematical formulae. A design study, involving an online survey with 56 users, was undertaken to evaluate an existing set of auditory cues and to brainstorm alternative ideas and solutions from users before implementing modified designs and conducting a separate controlled evaluation. It is proposed that by involving a wide number of users in the creative design process, intuitive auditory cues will be implemented with the potential to enhance spoken mathematics for visually impaired users.","Emma Murphy, Enda Bates, D&#243;nal Fitzpatrick","accessibility, design methods for user interfaces, mathematics, non-speech sound, spearcons, visually impaired users",75,82
10.1145/3234695.3236355,ASSETS,2018,Behavioral Changes in Speakers who are Automatically Captioned in Meetings with Deaf or Hard-of-Hearing Peers, ,"Deaf and hard of hearing (DHH) individuals face barriers to communication in small-group meetings with hearing peers; we examine generation of captions on mobile devices by automatic speech recognition (ASR). While ASR output displays errors, we study whether such tools benefit users and influence conversational behaviors. An experiment was conducted where DHH and hearing individuals collaborated in discussions in three conditions (without an ASR-based application, with the application, and with a version indicating words for which the ASR has low confidence). An analysis of audio recordings, from each participant across conditions, revealed significant differences in speech features. When using the ASR-based automatic captioning application, hearing individuals spoke more loudly, with improved voice quality (harmonics-to-noise ratio), with a non-standard articulation (changes in F1 and F2 formants), and at a faster rate. Identifying non-standard speech in this setting has implications on the composition of data used for ASR training/testing, which should be representative of its usage context. Understanding these behavioral influences may also enable designers of ASR captioning systems to leverage these effects, to promote communication success.","Matthew Seita, Khaled Albusays, Sushant Kafle, Michael Stinson, Matt Huenerfauth","accessibility, automatic speech recognition, communication, deaf and hard of hearing, speaking behavior",68,80
10.1145/2661334.2661360,ASSETS,2014,Verification of daily activities of older adults,"a simple, non-intrusive, low-cost approach","This paper presents an approach to verifying the activities of daily living of older adults at their home. We verify activities, instead of inferring them, because our monitoring approach is driven by routines, initially sketched by users in their environment. Monitoring is supported by a lightweight sensor infrastructure, comprising non-intrusive, low-cost, wireless devices. Verification is performed by applying a simple formula to sensor log data, for each activity of interest. The result value determines whether an activity has been performed. We have conducted an experimental study to validate our approach. To do so, four participants have been monitored during five days at their home, equipped with sensors. When applied to the log data, our formulas were able to automatically verify that a list of activities were performed. They produced the same interpretations, using Signal Detection Theory, as a third party, manually analyzing the log data.","Lo&#239;c Caroux, Charles Consel, Lucile Dupuy, H&#233;l&#232;ne Sauz&#233;on","activities of daily living, activity recognition, older adults, pervasive computing, routines, sensors, signal detection theory, verification",43,50
10.1145/3234695.3236354,ASSETS,2018,Assessing Virtual Assistant Capabilities with Italian Dysarthric Speech, ,"The usage of smartphone-based virtual assistants (e.g., Siri or Google Assistant) is growing, and their spread has generally a positive impact on device accessibility, e.g., for people with disabilities. However, people with dysarthria or other speech impairments may be unable to use these virtual assistants with proficiency. This paper investigates to which extent people with ALS-induced dysarthria can be understood and get consistent answers by three widely used smartphone-based assistants, namely Siri, Google Assistant, and Cortana. We focus on the recognition of Italian dysarthric speech, to study the behavior of the virtual assistants with this specific population for which no relevant studies are available. We collected and recorded suitable speech samples from people with dysarthria in a dedicated center of the Molinette hospital, in Turin, Italy. Starting from those recordings, the differences between such assistants, in terms of speech recognition and consistency in answer, are investigated and discussed. Results highlight different performance among the virtual assistants. For speech recognition, Google Assistant is the most promising, with around 25% of word error rate per sentence. Consistency in answer, instead, sees Siri and Google Assistant provide coherent answers around 60% of times.","Fabio Ballati, Fulvio Corno, Luigi De Russis","accessibility, automatic speech recognition, conversational assistant, dysarthria, speech impairment",93,101
10.1145/3308561.3353805,ASSETS,2019,Go That Way,Exploring Supplementary Physical Movements by a Stationary Robot When Providing Navigation Instructions,"We describe an exploration of how kiosk-type stationary robots might provide navigation instructions for blind people. Inspired by a technique used by Orientation &#38; Mobility experts in which a route is traced out on a person's palm, we developed five methods that supplement verbal instructions with physical movements. We explored the usability, strengths, and limitations of each of our methods in two exploratory studies with blind participants. One method, in which the robot used its entire arm to create path gestures while participants held its gripper, was preferred by 5 out of 8 blind participants and performed comparably on a recall task as a verbal-only instruction method. A closer approximation of the original palm method failed. We analyzed interview data to understand the reasons behind the failures and successes. We discuss the lessons learned from our studies about instruction methods, how robots in public settings can be useful for blind people, and the challenges of deploying such systems in public.","Xiang Zhi Tan, Elizabeth Carter, Samantha Reig, Aaron Steinfeld","assistive robots, blindness, human-robot interaction, navigation instructions",299,311
10.1145/3132525.3132531,ASSETS,2017,Speed-Dial,A Surrogate Mouse for Non-Visual Web Browsing,"Sighted people can browse the Web almost exclusively using a mouse. This is because web browsing mostly entails pointing and clicking on some element in the web page, and these two operations can be done almost instantaneously with a computer mouse. Unfortunately, people with vision impairments cannot use a mouse as it only provides visual feedback through a cursor. Instead, they are forced to go through a slow and tedious process of building a mental map of the web page, relying primarily on a screen reader's keyboard shortcuts and its serial audio readout of the textual content of the page, including metadata. This can often cause content and cognitive overload. This paper describes our Speed-Dial system which uses an off-the-shelf physical Dial as a surrogate for the mouse for non-visual web browsing. Speed-Dial interfaces the physical Dial with the semantic model of a web page, and provides an intuitive and rapid access to the entities and their content in the model, thereby bringing blind people's browsing experience closer to how sighted people perceive and interact with the Web. A user study with blind participants suggests that with Speed-Dial they can quickly move around the web page to select content of interest, akin to pointing and clicking with a mouse.","Syed Masum Billah, Vikas Ashok, Donald Porter, IV Ramakrishnan","microsoft surface dial, screen reader, semantic web browsing, tactile exploration, tactile interaction, visual impairment",110,119
10.1145/2513383.2513455,ASSETS,2013,Follow that sound,using sonification and corrective verbal feedback to teach touchscreen gestures,"While sighted users may learn to perform touchscreen gestures through observation (<i>e.g</i>., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures can be challenging. We propose and evaluate two techniques to teach touchscreen gestures to users with visual impairments: (1) <i>corrective verbal feedback</i> using text-to-speech and automatic analysis of the user's drawn gesture; (2) <i>gesture sonification</i> to generate sound based on finger touches, creating an audio representation of a gesture. To refine and evaluate the techniques, we conducted two controlled lab studies. The first study, with 12 sighted participants, compared parameters for sonifying gestures in an eyes-free scenario and identified pitch + stereo panning as the best combination. In the second study, 6 blind and low-vision participants completed gesture replication tasks with the two feedback techniques. Subjective data and preliminary performance findings indicate that the techniques offer complementary advantages.","Uran Oh, Shaun Kane, Leah Findlater","blindness, gestures, sonification, touchscreen, visual impairments",1,8
10.1145/3234695.3236356,ASSETS,2018,Modeling the Speed and Timing of American Sign Language to Generate Realistic Animations, ,"To enable more websites to provide content in the form of sign language, we investigate software to partially automate the synthesis of animations of American Sign Language (ASL), based on a human-authored message specification. We automatically select: where prosodic pauses should be inserted (based on the syntax or other features), the time-duration of these pauses, and the variations of the speed at which individual words are performed (e.g. slower at the end of phrases). Based on an analysis of a corpus of multi-sentence ASL recordings with motion-capture data, we trained machine-learning models, which were evaluated in a cross-validation study. The best model out-performed a prior state-of-the-art ASL timing model. In a study with native ASL signers evaluating animations generated from either our new model or from a simple baseline (uniform speed and no pauses), participants indicated a preference for speed and pausing in ASL animations from our model.","Sedeeq Al-khazraji, Larwan Berke, Sushant Kafle, Peter Yeung, Matt Huenerfauth","ASL, American sign language, animation, modeling, pauses, prosodic breaks, timing",259,270
10.1145/2384916.2384950,ASSETS,2012,Design and evaluation of classifier for identifying sign language videos in video sharing sites, ,"Video sharing sites provide an opportunity for the collection and use of sign language presentations about a wide range of topics. Currently, locating sign language videos (SL videos) in such sharing sites relies on the existence and accuracy of tags, titles or other metadata indicating the content is in sign language. In this paper, we describe the design and evaluation of a classifier for distinguishing between sign language videos and other videos. A test collection of SL videos and videos likely to be incorrectly recognized as SL videos (likely false positives) was created for evaluating alternative classifiers. Five video features thought to be potentially valuable for this task were developed based on common video analysis techniques. A comparison of the relative value of the five video features shows that a measure of the symmetry of movement relative to the face is the best feature for distinguishing sign language videos. Overall, an SVM classifier provided with all five features achieves 82% precision and 90% recall when tested on the challenging test collection. The performance would be considerably higher when applied to the more varied collections of large video sharing sites.","Caio Monteiro, Ricardo Gutierrez-Osuna, Frank Shipman","asl, metadata extraction, sign language, video analysis, video sharing",191,198
10.1145/3132525.3132558,ASSETS,2017,Improving Smartphone Accessibility with Personalizable Static Overlays, ,"The physical keypads that used to dominate our mobile devices provided additional support for non-visual interaction - the keys could be recognized tactually, the interfaces were simpler and consistent. When combined with a screen reader, these devices could be easily operated by blind people. The advent of smartphones, with their rich, feature-filled applications and interfaces, have brought forward additional challenges for blind users. Apps and features are no longer developed by a single entity leading to an overwhelming variety of interfaces. We present an approach that superimposes a virtual overlay to all other interfaces ensuring interface consistency by re-structuring how content is accessed in every screen. To explore the approach, we split the screen, dedicating half to a configurable set of static options mimicking always available physical buttons regardless of context; while the other enables the standard content navigation gestures with the ability to re-order content and apply filters. In a qualitative study with nine visually impaired participants, the virtual overlays were reported as simpler to use, while still providing full-fledged usage of the system and the third party applications, and were seen as effective and useful, particularly for novice users.","Andr&#233; Rodrigues, Andr&#233; Santos, Kyle Montague, Tiago Guerreiro","accessibility, blind, personalizable interface, smartphone",37,41
10.1145/1878803.1878836,ASSETS,2010,Vi-bowling,a tactile spatial exergame for individuals with visual impairments,"Lack of sight forms a significant barrier to participate in physical activity. Consequently, individuals with visual impairments are at greater risk for developing serious health problems, such as obesity. Exergames are video games that provide physical exercise. For individuals with visual impairments, exergames have the potential to reduce health disparities as they may be safer to play and can be played without the help of others. This paper presents VI Bowling, a tactile/audio exergame that can be played using an inexpensive motion-sensing controller. VI Bowling explores tactile dowsing: a novel technique for performing spatial sensorimotor challenges, which can be used for motor learning. VI Bowling was evaluated with six blind adults. All players enjoyed VI Bowling and the challenge tactile dowsing provided. Players could throw their ball with an average error of 9.76 degrees using tactile dowsing. Participants achieved an average active energy expenditure of 4.61 <i>kJ/Min</i> while playing VI Bowling, which is comparable to walking.","Tony Morelli, John Foley, Eelke Folmer","exergames, haptics, health, visual impairments",179,186
10.1145/2049536.2049540,ASSETS,2011,The design of human-powered access technology, ,"People with disabilities have always overcome accessibility problems by enlisting people in their community to help. The Internet has broadened the available community and made it easier to get on-demand assistance remotely. In particular, the past few years have seen the development of technology in both research and industry that uses human power to overcome technical problems too difficult to solve automatically. In this paper, we frame recent developments in human computation in the historical context of accessibility, and outline a framework for discussing new advances in human-powered access technology. Specifically, we present a set of 13 design principles for human-powered access technology motivated both by historical context and current technological developments. We then demonstrate the utility of these principles by using them to compare several existing human-powered access technologies. The power of identifying the 13 principles is that they will inspire new ways of thinking about human-powered access technologies.","Jeffrey Bigham, Richard Ladner, Yevgen Borodin","access technology, crowdsourcing, human computation",3,10
10.1145/3308561.3353786,ASSETS,2019,The Design Space of Nonvisual Word Completion, ,"Word completion interfaces are ubiquitously available in mobile virtual keyboards; however, there is no prior research on how to design these interfaces for screen reader users. In addressing this, we propose a design space for nonvisual representation of word completions. The design space covers seven categories aiming to identify challenges and opportunities for interaction design in an unexplored research topic. It is intended to guide the design of novel interaction techniques, serving as a framework for researchers and practitioners working on nonvisual word completion. To demonstrate its potential, we engaged blind users in an exploration of the design space, to create their own bespoke word completion solutions. Through this study we found that users create alternative interfaces that extended current screen readers' capabilities. Resulting interfaces are less conservative than mainstream solutions on notification frequency and cardinality. Customization decisions were based on perceived benefits/costs and varied depending on multiple factors such as users' perceived prediction accuracy, potential keystroke gains, and situational restrictions.","Hugo Nicolau, Andr&#233; Rodrigues, Andr&#233; Santos, Tiago Guerreiro, Kyle Montague, Jo&#227;o Guerreiro","blind, design space, mobile, screen reader, text entry, touchscreen, word completion, word prediction",249,261
10.1145/2661334.2661378,ASSETS,2014,Where's my bus stop?,supporting independence of blind transit riders with StopInfo,"Locating bus stops, particularly in unfamiliar areas, can present challenges to people who are blind or low vision. At the same time, new information technology such as smart phones and mobile devices have enabled them to undertake a much greater range of activities with increased independence. We focus on the intersection of these issues. We developed and deployed StopInfo, a system for public transit riders that provides very detailed information about bus stops with the goal of helping riders find and verify bus stop locations. We augmented internal information from a major transit agency in the Seattle area with information entered by the community, primarily as they waited at these stops. Additionally, we conducted a five week field study with six blind and low vision participants to gauge usage patterns and determine values related to independent travel. We found that StopInfo was received positively and is generally usable. Furthermore, the system supports tenets of independence; participants took public transit trips that they might not have attempted otherwise. Lastly, an audit of bus stops in three Seattle neighborhoods found that information from both the transit agency and the community was accurate.","Megan Campbell, Cynthia Bennett, Caitlin Bonnar, Alan Borning","accessible transit stops, community-sourcing, crowdsourcing, public transit, tools for blind and low vision riders",11,18
10.1145/2049536.2049565,ASSETS,2011,How voice augmentation supports elderly web users, ,"Online Web applications have become widespread and have made our daily life more convenient. However, older adults often find such applications inaccessible because of age-related changes to their physical and cognitive abilities. Two of the reasons that older adults may shy away from the Web are fears of the unknown and of the consequences of incorrect actions. We are extending a voice-based augmentation technique originally developed for blind users. We want to reduce the cognitive load on older adults by providing contextual support. An experiment was conducted to evaluate how voice augmentation can support elderly users in using Web applications. Ten older adults participated in our study and their subjective evaluations showed how the system gave them confidence in completing Web forms. We believe that voice augmentation may help address the users' concerns arising from their low confidence levels.","Daisuke Sato, Masatomo Kobayashi, Hironobu Takagi, Chieko Asakawa, Jiro Tanaka","older adults, voice-based augmentation, web accessibility",155,162
10.1145/3308561.3353779,ASSETS,2019,WatchOut,Obstacle Sonification for People with Visual Impairment or Blindness,"Independent mobility is one of the main challenges for blind or visually impaired (BVI) people. In particular, BVI people often need to identify and avoid nearby obstacles, for example a bicycle parked on the sidewalk. This is generally achieved with a combination of residual vision, hearing and haptic sensing using the white cane. However, in many cases, BVI people can only perceive obstacles at short distance (typically about 1m, i.e., the white cane detection range), in other situations obstacles are hard to detect (e.g., those elevated from the ground), while others should not be hit by the white cane (e.g., a standing person). Thus, some time and effort are required to identify the object in order to understand how to avoid it. A solution to these problems can be found in recent computer vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them to a BVI user. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main characteristics of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centric approach, involving two iterations of online questionnaires with BVI participants in order to define, improve and evaluate the sonification technique. WatchOut was implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable, and can guide the users to avoid more than 85% of the obstacles.","Giorgio Presti, Dragan Ahmetovic, Mattia Ducci, Cristian Bernareggi, Luca Ludovico, Adriano Barat&#232;, Federico Avanzini, Sergio Mascetti","navigation assistive technologies, obstacle avoidance, sonification, visual impairment",402,413
10.1145/2700648.2809867,ASSETS,2015,Usage of Subjective Scales in Accessibility Research, ,"Accessibility research studies often gather subjective responses to technology using Likert-type items, where participants respond to a prompt statement by selecting a position on a labeled response scale. We analyzed recent ASSETS papers, and found that participants in non-anonymous accessibility research studies gave more positive average ratings than those in typical usability studies, especially when responding to questions about a proposed innovation. We further explored potential positive response bias in an experimental study of two telephone information systems, one more usable than the other. We found that participants with visual impairment were less sensitive to usability problems than participants in a typical student sample, and that their subjective ratings didn't correlate as strongly with objective measures of performance. A deeper understanding of the mechanism behind this effect would help researchers to design better accessibility studies, and to interpret subjective ratings with more accuracy.","Shari Trewin, Diogo Marques, Tiago Guerreiro","accessibility, likert, response bias",59,67
10.1145/2513383.2517033,ASSETS,2013,Answering visual questions with conversational crowd assistants, ,"Blind people face a range of accessibility challenges in their everyday lives, from reading the text on a package of food to traveling independently in a new place. Answering general questions about one's visual surroundings remains well beyond the capabilities of fully automated systems, but recent systems are showing the potential of engaging on-demand human workers (the crowd) to answer visual questions. The input to such systems has generally been a single image, which can limit the interaction with a worker to one question; or video streams where systems have paired the end user with a single worker, limiting the benefits of the crowd. In this paper, we introduce Chorus:View, a system that assists users over the course of longer interactions by engaging workers in a continuous conversation with the user about a video stream from the user's mobile device. We demonstrate the benefit of using multiple crowd workers instead of just one in terms of both latency and accuracy, then conduct a study with 10 blind users that shows Chorus:View answers common visual questions more quickly and accurately than existing approaches. We conclude with a discussion of users' feedback and potential future work on interactive crowd support of blind users.","Walter Lasecki, Phyo Thiha, Yu Zhong, Erin Brady, Jeffrey Bigham","assistive technology, crowdsourcing, human computation",1,8
10.1145/3234695.3239331,ASSETS,2018,Who Should Have Access to my Pointing Data?,Privacy Tradeoffs of Adaptive Assistive Technologies,"Customizing assistive technologies based on user needs, abilities, and preferences is necessary for accessibility, especially for individuals whose abilities vary due to a diagnosis, medication, or other external factors. Adaptive Assistive Technologies (AATs) that can automatically monitor a user's current abilities and adapt functionality and appearance accordingly offer exciting solutions. However, there is an often-overlooked privacy tradeoff between usability and user privacy when designing such systems. We present a general privacy threat model analysis of AATs and contextualize it with findings from an interview study with older adults who experience pointing problems. We found that participants had positive attitude towards assistive technologies that gather their personal data but also had strong preferences for how their data should be used and who should have access to it. We identify a need to seriously consider privacy threats when designing assistive technologies to avoid exposing users to them.","Foad Hamidi, Kellie Poneres, Aaron Massey, Amy Hurst","adaptive systems, assistive technology, essential tremors, older adults, pointing, privacy, threat modeling",203,216
10.1145/2049536.2049550,ASSETS,2011,Leveraging large data sets for user requirements analysis, ,"In this paper, we show how a large demographic data set that includes only high-level information about health and disability can be used to specify user requirements for people with specific needs and impairments. As a case study, we consider adapting spoken dialogue systems (SDS) to the needs of older adults. Such interfaces are becoming increasingly prevalent in telecare and home care, where they will often be used by older adults. As our data set, we chose the English Longitudinal Survey of Ageing (ELSA), a large representative survey of the health, wellbeing, and socioeconomic status of English older adults. In an inclusion audit, we show that one in four older people surveyed by ELSA might benefit from SDS due to problems with dexterity, mobility, vision, or literacy. Next, we examine the technology that is available to our target users (technology audit) and estimate factors that might prevent older people from using SDS (exclusion audit). We conclude that while SDS are ideal for solutions that are delivered on the near ubiquitous landlines, they need to be accessible for people with mild to moderate hearing problems, and thus multimodal solutions should be based on the television, a technology even more widespread than landlines.","Maria Wolters, Vicki Hanson, Johanna Moore","hearing, inclusive design, requirements capture, spoken dialogue systems, telecare",67,74
10.1145/3308561.3353772,ASSETS,2019,Exploration of Automatic Speech Recognition for Deaf and Hard of Hearing Students in Higher Education Classes, ,"Automatic speech recognition (ASR) programs that generate real-time speech-to-text captions can be provided as supplemental access technologies for deaf and hard of hearing (DHH) students in higher education classes. As part of a pilot program, we implemented ASR as a supplemental access service in biology, statistics, and other courses at our university. To identify the benefits and limitations of ASR as an access technology, we surveyed 26 DHH students and interviewed 8 of these students about their experiences with ASR in their mainstream classes. Participants believed that ASR was beneficial despite the errors that ASR continued to generate; however, the accuracy and readability of ASR need to improve so that students can better access spoken information through ASR. This paper reviews points for researchers to consider when designing and providing ASR as a supplemental access service in educational settings.","Janine Butler, Brian Trager, Byron Behm","automatic speech recognition, deaf and hard of hearing, feedback, real-time captions",32,42
10.1145/3308561.3353802,ASSETS,2019,See-Thru: Towards Minimally Obstructive Eye-Controlled Wheelchair Interfaces, ,"Eye-tracking interfaces increase the communication bandwidth between humans and computers when using hands is not possible. For some, eyes are the only available input modality to control and interact with the various devices that enable their independence. The goal of this work is to develop and evaluate an eye-controlled wheelchair navigation interface that minimizes obstruction to the user's field of view by removing the conventional use of a computer screen as a feedback mechanism. We present See-Thru, an eye-tracking interface that provides feedback to the user without a screen while simultaneously providing a clear view of the path ahead. Our prototype is evaluated against a screen-based state of the art interface in a study with three navigation tasks completed by seven power wheelchair users. Our results show that a majority of the participants not only prefer using the See-Thru interface, but perform better at driving tasks when using it. This supports the notion that users favor minimally obstructive interfaces in navigational contexts.","Corten Singer, Bj&#246;rn Hartmann","eye gaze, eye tracking, field of view (fov), gaze control, navigation, power wheelchair, user interfaces",459,469
10.1145/3234695.3236344,ASSETS,2018,"""Siri Talks at You""",An Empirical Investigation of Voice-Activated Personal Assistant (VAPA) Usage by Individuals Who Are Blind,"Voice-activated personal assistants (VAPAs)--like Amazon Echo or Apple Siri--offer considerable promise to individuals who are blind due to widespread adoption of these non-visual interaction platforms. However, studies have yet to focus on the ways in which these technologies are used by individuals who are blind, along with whether barriers are encountered during the process of interaction. To address this gap, we interviewed fourteen legally-blind adults with experience of home and/or mobile-based VAPAs. While participants appreciated the access VAPAs provided to inaccessible applications and services, they faced challenges relating to the input, responses from VAPAs, and control of information presented. User behavior varied depending on the situation or context of the interaction. Implications for design are suggested to support inclusivity when interacting with VAPAs. These include accounting for privacy and situational factors in design, examining ways to support concerns over trust, and synchronizing presentation of visual and non-visual cues.","Ali Abdolrahmani, Ravi Kuber, Stacy Branham","blind individuals, non-visual interaction, usability challenges, vapa, voice activated personal assistant",249,258
10.1145/1878803.1878815,ASSETS,2010,Field evaluation of a collaborative memory aid for persons with amnesia and their family members, ,"The loss of memory can have a profound and disabling effect on individuals. People who acquire memory impairments are often unable to live independent lives because they cannot remember what they need to do. In many cases, they rely on family members who live with them to accomplish everyday activities, such as coordinating a doctor's appointment. To design technology for persons with amnesia and their families, we involved end users in the participatory design of a collaborative memory aid called Family-Link. We evaluated Family-Link by comparing it to a commercially available calendar application. We found that participants shared significantly more events when using Family-Link. Qualitative evidence also suggests that Family-Link increased participants' awareness of family members' schedules, enabled caregivers to track the person with amnesia leading to a greater a sense of security and reduced stress, and reduced the amount of caregiver coordination effort. The paper concludes with design implications.","Mike Wu, Ronald Baecker, Brian Richards","amnesia, collaboration, design, family, memory aid, user study",51,58
10.1145/2982142.2982175,ASSETS,2016,Tactile Accessibility,Does Anyone Need a Haptic Glove?,"Graphical user interfaces (GUIs) are widely used on smartphones, tablets, and laptops. While GUIs are convenient for sighted users, their accessibility for blind people, who use screen readers to interact with GUIs, remains to be problematic. Even the most screen-reader accessible GUIs are far less usable for blind people compared to sighted people, because the former group cannot benefit from the geometric layout of GUIs. As a result, blind people often have to listen through a lot of irrelevant content before they find what they are looking for. Haptic interfaces (those providing tactile feedback) have the potential to make GUI interfaces more accessible and usable for blind people. Alas, mainstream computer devices do not have haptic screens that would enable high-resolution tactile feedback, and specialized haptic devices are very limited and/or are exuberantly expensive and bulky.   In this paper, we describe a low-cost haptic-glove system, FeelX, which can potentially enable usable tactile interaction with GUIs. The vision of FeelX is to enable blind users to connect it to any computer or smartphone, and then interact with it by moving their hands on any flat surface such as the desk or table. To establish the practicality and the desirability of using haptic gloves, we evaluated the initial prototype of the glove in a user study with 20 blind participants. Throughout the study, we performed a comparative evaluation of several design options for the tactile interface. The participants were asked to identify simple geometric figures such as lines, rectangles, circles, and triangles that are the basic building blocks of any GUI interface. Although the FeelX prototype is far from being a usable product, the results of the study indicate that blind users want to use haptic gloves.","Andrii Soviak, Anatoliy Borodin, Vikas Ashok, Yevgen Borodin, Yury Puzis, I.V. Ramakrishnan","blindness, gui accessibility, haptic display, haptic glove, screen reader, tactile exploration, tactile interaction",101,109
10.1145/2661334.2661375,ASSETS,2014,Technology to reduce social isolation and loneliness, ,"Large numbers of individuals, many of them senior citizens, live in social isolation. This typically leads to loneliness, depression, and vulnerability, and subsequently to other negative health consequences. We report on research focused on understanding the communication needs of people in environments associated with social isolation and loneliness, and how technology facilitates social connection. Our work consists of successive iterations of field studies and technology prototype design, deployment, and analysis. Particular attention is paid to seniors in retirement communities and in long-term care settings (nursing homes). We present design implications for technology to enable seniors' social connections, the ""InTouch"" prototype that satisfies most of the implications, and a report on one older adult's experience of InTouch.","Ron Baecker, Kate Sellen, Sarah Crosskey, Veronique Boscart, Barbara Barbosa Neves","chronic pain, communications technology, field studies, loneliness, long-term care, prototypes, seniors, social isolation",27,34
10.1145/2661334.2661358,ASSETS,2014,Analyzing the intelligibility of real-time mobile sign language video transmitted below recommended standards, ,"Mobile sign language video communication has the potential to be more accessible and affordable if the current recommended video transmission standard of 25 frames per second at 100 kilobits per second (kbps) as prescribed in the International Telecommunication Standardization Sector (ITU-T) Q.26/16 were relaxed. To investigate sign language video intelligibility at lower settings, we conducted a laboratory study, where fluent ASL signers in pairs held real-time free-form conversations over an experimental smartphone app transmitting real-time video at 5 fps/25 kbps, 10 fps/50 kbps, 15 fps/75 kbps, and 30 fps/150 kbps, settings well below the ITU-T standard that save both bandwidth and battery life. The aim of the laboratory study was to investigate how fluent ASL signers adapt to the lower video transmission rates, and to identify a lower threshold at which intelligible real-time conversations could be held. We gathered both subjective and objective measures from participants and calculated battery life drain. As expected, reducing the frame rate/bit rate monotonically extended the battery life. We discovered all participants were successful in holding intelligible conversations across all frame rates/bit rates. Participants did perceive the lower quality of video transmitted at 5 fps/25 kbps and felt that they were signing more slowly to compensate; however, participants' rate of fingerspelling did not actually decrease. This and other findings support our recommendation that intelligible mobile sign language conversations can occur at frame rates as low as 10 fps/50 kbps while optimizing resource consumption, video intelligibility, and user preferences.","Jessica Tran, Ben Flowers, Eve Risken, Richard Ladner, Jacob Wobbrock","american sign language, bit rate, comprehension, deaf community., frame rate, intelligibility, laboratory study, video compression",177,184
10.1145/1878803.1878833,ASSETS,2010,Are synthesized video descriptions acceptable?, ,"We conducted a series of experiments to assess the feasibility of synthesized narrations to describe online videos. To reduce the cultural bias, we included adult blind or low-vision participants from Japan and the U.S. in the main study. Our research also includes a follow-up study we conducted in Japan to assess the effectiveness of synthesized video descriptions in realistic situations. The results showed that synthesized video descriptions were generally accepted in both countries. We also found that appropriate technology support allowed a novice describer to make effective video descriptions. Based on these results, we discuss the implications for developing a technology platform for describing online videos.","Masatomo Kobayashi, Trisha O'Connell, Bryan Gould, Hironobu Takagi, Chieko Asakawa","audio description, online videos, speech synthesis, text-to-speech (tts), video description, web accessibility",163,170
10.1145/2513383.2513447,ASSETS,2013,Good fonts for dyslexia, ,"Around 10% of the people have dyslexia, a neurological disability that impairs a person's ability to read and write. There is evidence that the presentation of the text has a significant effect on a text's accessibility for people with dyslexia. However, to the best of our knowledge, there are no experiments that objectively measure the impact of the font type on reading performance. In this paper, we present the first experiment that uses eye-tracking to measure the effect of font type on reading speed. Using a within-subject design, 48 subjects with dyslexia read 12 texts with 12 different fonts. <i>Sans serif, monospaced</i> and <i>roman</i> font styles significantly improved the reading performance over <i>serif, proportional</i> and <i>italic</i> fonts. On the basis of our results, we present a set of more accessible fonts for people with dyslexia.","Luz Rello, Ricardo Baeza-Yates","dyslexia, eye-tracking, font types, legibility, readability, text layout, text presentation, typography",1,8
10.1145/1878803.1878807,ASSETS,2010,Disability studies as a source of critical inquiry for the field of assistive technology, ,"Disability studies and assistive technology are two related fields that have long shared common goals - understanding the experience of disability and identifying and addressing relevant issues. Despite these common goals, there are some important differences in what professionals in these fields consider problems, perhaps related to the lack of connection between the fields. To help bridge this gap, we review some of the key literature in disability studies. We present case studies of two research projects in assistive technology and discuss how the field of disability studies influenced that work, led us to identify new or different problems relevant to the field of assistive technology, and helped us to think in new ways about the research process and its impact on the experiences of individuals who live with disability. We also discuss how the field of disability studies has influenced our teaching and highlight some of the key publications and publication venues from which our community may want to draw more deeply in the future.","Jennifer Mankoff, Gillian Hayes, Devva Kasnitz","assistive technology, disability studies",3,10
10.1145/2700648.2809855,ASSETS,2015,A Unifying Notification System To Scale Up Assistive Services, ,"Aging creates needs for assistive technology to support all activities of daily living (meal preparation, dressing, social participation, stove monitoring, etc.). These needs are mostly addressed by a silo-based approach that requires a new assistive service (e.g., a reminder system, a pill prompter) to be acquired for every activity to be supported. In practice, these services manifest their silo-based nature in their user interactions, and more specifically, in the heterogeneity of their notification system. This heterogeneity incurs a cognitive cost that prevents scaling up assistive services and compromises adoption by older adults. This paper presents an approach to scaling up the combination of technology-based, assistive services by proposing a unifying notification system. To do so, (1) we propose a decomposition of assistive services to expose their needs in notification; (2) we introduce a notification framework, allowing heterogeneous assistive services to homogeneously notify users; (3) we present how this notification framework is carried out in practice for an assisted living platform. We successfully applied our approach to a range of existing and new assistive services. We used our notification framework to implement an assistive platform that combines a variety of assistive services. This platform has been deployed and used 24/7 at the home of 15 older adults for up to 6 months. This study provides empirical evidence of the effectiveness and learnability of the notification system of our platform, irrespective of the cognitive and sensory resources of the user. Additional results show that our assisted living platform achieved high user acceptance and satisfaction.","Charles Consel, Lucile Dupuy, H&#233;l&#232;ne Sauz&#233;on","field study, human factors, measurement, notification systems, performance",77,87
10.1145/2982142.2982159,ASSETS,2016,Nothing to Hide,Aesthetic Customization of Hearing Aids and Cochlear Implants in an Online Community,"Hearing aids and cochlear implants can improve accessibility and quality of life for people with hearing impairments. However, use of these devices may cause concern amongst some users due to sociocultural issues such as unwanted attention and perceived stigma. While some individuals may respond to these concerns by attempting to conceal their devices, or even abandoning their devices, others have responded by making their devices more visible through aesthetic customization, and some have begun to share these customizations online. In this paper, we describe community interactions in an online forum dedicated to customized hearing aids and cochlear implants. We found that community members discussed customization tools and techniques, shared their customizations, and provided each other with encouragement and support. Community members customized their devices as a means of self-expression that demonstrated the wearer's fashion sense, revealed favorite sports teams and characters, and marked holidays and personal milestones. Our findings may inform the design of assistive technologies that better support personalization, customization, and self-expression.","Halley Profita, Abigale Stangl, Laura Matuszewska, Sigrunn Sky, Shaun Kane","cochlear implants, deafness, diy assistive technology, hearing aids, online communities, social acceptability",219,227
10.1145/2661334.2661362,ASSETS,2014,Motor-impaired touchscreen interactions in the wild, ,"Touchscreens are pervasive in mainstream technologies; they offer novel user interfaces and exciting gestural interactions. However, to interpret and distinguish between the vast ranges of gestural inputs, the devices require users to consistently perform interactions inline with the predefined location, movement and timing parameters of the gesture recognizers. For people with variable motor abilities, particularly hand tremors, performing these input gestures can be extremely challenging and impose limitations on the possible interactions the user can make with the device. In this paper, we examine touchscreen performance and interaction behaviors of motor-impaired users on mobile devices. The primary goal of this work is to measure and understand the variance of touchscreen interaction performances by people with motor-impairments. We conducted a four-week in-the-wild user study with nine participants using a mobile touchscreen device. A Sudoku stimulus application measured their interaction performance abilities during this time. Our results show that not only does interaction performance vary significantly between users, but also that an individual's interaction abilities are significantly different between device sessions. Finally, we propose and evaluate the effect of novel tap gesture recognizers to accommodate for individual variances in touchscreen interactions.","Kyle Montague, Hugo Nicolau, Vicki Hanson","in-the-wild, motor-impaired, touchscreen, user models",123,130
10.1145/2661334.2661376,ASSETS,2014,Design of and subjective response to on-body input for people with visual impairments, ,"For users with visual impairments, who do not necessarily need the visual display of a mobile device, non-visual on-body interaction (e.g., Imaginary Interfaces) could provide accessible input in a mobile context. Such interaction provides the potential advantages of an always-available input surface, and increased tactile and proprioceptive feedback compared to a smooth touchscreen. To investigate preferences for and design of accessible on-body interaction, we conducted a study with 12 visually impaired participants. Participants evaluated five locations for on-body input and compared on-phone to on-hand interaction with one versus two hands. Our findings show that the least preferred areas were the face/neck and the forearm, while locations on the hands were considered to be more discreet and natural. The findings also suggest that participants may prioritize social acceptability over ease of use and physical comfort when assessing the feasibility of input at different locations of the body. Finally, tradeoffs were seen in preferences for touchscreen versus on-body input, with on-body input considered useful for contexts where one hand is busy (e.g., holding a cane or dog leash). We provide implications for the design of accessible on-body input.","Uran Oh, Leah Findlater","design recommendations., eyes-free interaction, gestural interfaces, mobile, on-body input, visual impairments",115,122
10.1145/1878803.1878829,ASSETS,2010,Using accessible math textbooks with students who have learning disabilities, ,Math is a subject that most students in K-12 participate in every school day. This includes students with learning disabilities as they are equally accountable to meeting general math curriculum requirements. Project SMART provided digital versions of math textbooks modified to include MathML for use by eighth grade students with various learning disabilities. A goal of Project SMART was to determine whether these accessible digital textbooks improved student test performance as compared to control groups using the same texts in print format with a traditional oral accommodation. The study also examined the extent to which using accessible math impacted student perceptions about math abilities. Students and most teachers found the accessible digital textbooks preferable to the print versions. This was generally reflected in higher test scores as well as consistently positive responses from qualitative measures obtained from ongoing student and teacher surveys.,"Preston Lewis, Steve Noble, Neil Soiffer","accessibility, daisy, learning disabilities, math disabilities, mathml, print disabilities, visual impairments",139,146
10.1145/2049536.2049544,ASSETS,2011,Supporting spatial awareness and independent wayfinding for pedestrians with visual impairments, ,"Much of the information designed to help people navigate the built environment is conveyed through visual channels, which means it is not accessible to people with visual impairments. Due to this limitation, travelers with visual impairments often have difficulty navigating and discovering locations in unfamiliar environments, which reduces their sense of independence with respect to traveling by foot. In this paper, we examine how mobile location-based computing systems can be used to increase the feeling of independence in travelers with visual impairments. A set of formative interviews with people with visual impairments showed that increasing one's general spatial awareness is the key to greater independence. This insight guided the design of Talking Points 3 (TP3), a mobile location-aware system for people with visual impairments that seeks to increase the legibility of the environment for its users in order to facilitate navigating to desired locations, exploration, serendipitous discovery, and improvisation. We conducted studies with eight legally blind participants in three campus buildings in order to explore how and to what extent TP3 helps promote spatial awareness for its users. The results shed light on how TP3 helped users find destinations in unfamiliar environments, but also allowed them to discover new points of interest, improvise solutions to problems encountered, develop personalized strategies for navigating, and, in general, enjoy a greater sense of independence.","Rayoung Yang, Sangmi Park, Sonali Mishra, Zhenan Hong, Clint Newsom, Hyeon Joo, Erik Hofer, Mark Newman","accessibility, wayfinding",27,34
10.1145/2049536.2049541,ASSETS,2011,Empowering individuals with do-it-yourself assistive technology, ,"Assistive Technologies empower individuals to accomplish tasks they might not be able to do otherwise. Unfortunately, a large percentage of Assistive Technology devices that are purchased (35% or more) end up unused or abandoned [7,10], leaving many people with Assistive Technology that is inappropriate for their needs. Low acceptance rates of Assistive Technology occur for many reasons, but common factors include 1) lack of considering user opinion in selection, 2) ease in obtaining devices, 3) poor device performance, and 4) changes in user needs and priorities [7]. We are working to help more people gain access to the Assistive Technology they need by empowering non-engineers to ""Do-It-Yourself"" (DIY) and create, modify, or build. This paper illustrates that it is possible to custom-build Assistive Technology, and argues why empowering users to make their own Assistive Technology can improve the adoption process (and subsequently adoption rates). We discuss DIY experiences and impressions from individuals who have either built Assistive Technology before, or rely on it. We found that increased control over design elements, passion, and cost motivated individuals to make their own Assistive Technology instead of buying it. We discuss how a new generation of rapid prototyping tools and online communities can empower more individuals. We synthesize our findings into design recommendations to help promote future DIY-AT success.","Amy Hurst, Jasmine Tobias","assistive technology, do-it-yourself, empowerment, human-centered computing, online communities, personal-scale manufacturing, rapid prototyping",11,18
10.1145/3308561.3353775,ASSETS,2019,Leveraging Shared Control to Empower People with Tetraplegia to Participate in Extreme Sports, ,"Outdoor recreation improves quality of life for individuals with tetraplegia, however providing safe opportunities to engage in these sports has many challenges. We describe the iterative design and field evaluation of Tetra-Ski, a novel power-assisted ski chair. Users control Tetra-Ski with a joystick or Sip-and-Puff controller either independently or collaboratively with a tethered skier through our novel shared-control scheme. We also developed a training simulator to help prepare users prior to their skiing experience. A field study with eight participants and interviews with three trainers who used Tetra-Ski showed that Tetra-Ski is usable, enjoyable, and that the experience has a positive psychosocial effect on users. Furthermore, the shared-control scheme developed for Tetra-Ski proved crucial for supporting the unique abilities of different users, suggesting that a shared-control approach could enable broader access to less-dependent forms of outdoor recreation in the future.","Ahmad Alsaleem, Ross Imburgia, Mateo Godinez, Andrew Merryweather, Roger Altizer, Tamara Denning, Jeffery Rosenbluth, Stephen Trapp, Jason Wiese","adaptive skiing, shared control, ski chair, spinal cord injury/disorder, tetra-ski, tetraplegia",470,481
10.1145/2661334.2661364,ASSETS,2014,Including blind people in computing through access to graphs, ,"Our goal in creating the Graph SKetching tool, GSK, was to provide blind screen reader users with a means to create and access graphs as node-link diagrams and share them with sighted people in real-time. Through this effort, we hoped to better include blind people in computing and other STEM disciplines in which graphs are important. GSK proved very effective for one blind computer science student in courses that involved graphs and graph structures such as automata, decision trees, and resource-allocation diagrams. In order to determine how well GSK works for other blind people, we carried out a user study with ten blind participants. We report on the results of the user study, which demonstrates the efficacy of GSK for the examination, navigation, and creation of graphs by blind users. Based on the study results, we improved the efficiency of GSK for blind users. We plan more enhancements to help meet the need for accessible graph tools as articulated by the blind community.","Suzanne Balik, Sean Mealin, Matthias Stallmann, Robert Rodman, Michelle Glatz, Veronica Sigler","accessibility, gsk, universal design",91,98
10.1145/1878803.1878816,ASSETS,2010,In-situ study of blind individuals listening to audio-visual contents, ,"Videodescription (VD) or audio description is added to the sound track of audio-visual contents to make media such as film and television accessible to individuals with visual impairment. VD translates the relevant visual information into auditory information. In our previous users' testing, we found that the need of VD could be quite different depending on the visual disabilities of the participants. In order to better identify those differences, we conducted a study with ten legally blind individuals (with and without residual vision) to observe the type, quantity and frequency of the information needed by them. We learned that the degree of residual vision and the complexity of the content have a significant impact of the required level of VD. This suggests that a tool to render VD should offer a basic level of information, allow enough flexibility to provide more VD if needed, and answer on the fly demands for specific information. These specifications were implanted into an accessible video player.",Claude Chapdelaine,"accessibility, audio description, blind and visual impairment., multimedia",59,66
10.1145/2700648.2809854,ASSETS,2015,Transcribing Across the Senses,Community Efforts to Create 3D Printable Accessible Tactile Pictures for Young Children with Visual Impairments,"The design of 3D printable accessible tactile pictures (3DP-ATPs) for young children with visual impairments has the potential to greatly increase the supply of tactile materials that can be used to support emergent literacy skill development. Many caregivers and stakeholders invested in supporting young children with visual impairments have shown interest in using 3D printing to make accessible tactile materials. Unfortunately, the task of designing and producing 3DP-ATPs is far more complex than simply learning to use personal fabrication tools. This paper presents formative research conducted to investigate how six caregiver stakeholder-groups, with diverse skillsets and domain interests, attempt to create purposeful 3DP-ATPs with amateur-focused 3D modeling programs. We expose the experiences of these stakeholder groups as they attempt to design 3DP-ATG for the first time. We discuss how the participant groups practically and conceptually approach the task and focus their design work. Each group demonstrated different combinations of skillsets. In turn, we identify the common activities required of the design task as well how different participants are well suited and motivated to preform those activities. This study suggests that the emerging community of amateur 3DP-ATP designers may benefit from an online creativity support tool to help offset the challenges of designing purposeful 3DP-ATPs that are designed to meet individual children with VI's emergent literacy needs.","Abigale Stangl, Chia-Lo Hsu, Tom Yeh","3D model, 3D print, accessibility, emergent literacy, tactile graphics, tactile pictures, visual impairment",127,137
10.1145/3132525.3132543,ASSETS,2017,Understanding Fatigue and Stamina Management Opportunities and Challenges in Wheelchair Basketball, ,"Wearable fitness devices have demonstrated the capacity to improve overall fitness and athletic performance. Previous research has identified a need for these technologies to take into consideration a broader range of abilities to create more inclusive fitness communities. The adaptive sports community offers opportunities to explore technology use within a specialized community wherein physical activity is central to its identity. In this paper, we explore the current use and future potential of fitness technologies for stamina and fatigue management in wheelchair basketball, a team-based sport originally created for paraplegic athletes. We present findings from observations, seven interviews, and a survey (76 responses) with wheelchair basketball players and coaches. We present findings relating to their experience with and interest in automatic tracking of fatigue and stamina related metrics for themselves and their players.","Patrick Carrington, Denzel Ketter, Amy Hurst","adaptive sports, fitness, stamina and fatigue management, wearable, wheelchair basketball",130,139
10.1145/2384916.2384930,ASSETS,2012,A readability evaluation of real-time crowd captions in the classroom, ,"Deaf and hard of hearing individuals need accommodations that transform aural to visual information, such as captions that are generated in real-time to enhance their access to spoken information in lectures and other live events. The captions produced by professional captionists work well in general events such as community or legal meetings, but is often unsatisfactory in specialized content events such as higher education classrooms. In addition, it is hard to hire professional captionists, especially those that have experience in specialized content areas, as they are scarce and expensive. The captions produced by commercial automatic speech recognition (ASR) software are far cheaper, but is often perceived as unreadable due to ASR's sensitivity to accents, background noise and slow response time. We ran a study to evaluate the readability of captions generated by a new crowd captioning approach versus professional captionists and ASR. In this approach, captions are typed by classmates into a system that aligns and merges the multiple incomplete caption streams into a single, comprehensive real-time transcript. Our study asked 48 deaf and hearing readers to evaluate transcripts produced by a professional captionist, ASR and crowd captioning software respectively and found the readers preferred crowd captions over professional captions and ASR.","Raja Kushalnagar, Walter Lasecki, Jeffrey Bigham","design, experimentation, human factors",71,78
10.1145/2700648.2809842,ASSETS,2015,Getting Smartphones to Talkback,Understanding the Smartphone Adoption Process of Blind Users,"The advent of system-wide accessibility services on mainstream touch-based smartphones has been a major point of inclusion for blind and visually impaired people. Ever since, researchers aimed to improve the accessibility of specific tasks, such text-entry and gestural interaction. However, little work aimed to understand and improve the overall accessibility of these devices in real world settings. In this paper, we present an eight-week long study with five novice blind participants where we seek to understand major concerns, expectations, challenges, barriers, and experiences with smartphones. The study included pre-adoption and weekly interviews, weekly controlled task assessments, and in-the wild system-wide usage. Our results show that mastering these devices is an arduous and long task, confirming the users' initial concerns. We report on accessibility barriers experienced throughout the study, which could not be encountered in task-based laboratorial settings. Finally, we discuss how smartphones are being integrated in everyday activities and highlight the need for better adoption support tools.","Andr&#233; Rodrigues, Kyle Montague, Hugo Nicolau, Tiago Guerreiro","adoption, blind, challenges, novice, smartphone, touchscreen",23,32
10.1145/2982142.2982160,ASSETS,2016,Using Dynamic Audio Feedback to Support Peripersonal Reaching in Young Visually Impaired People, ,"Blind children engage with their immediate environment much less than sighted children, particularly through self-initiated movement or exploration. Research has suggested that providing dynamic feedback about the environment and the child's actions within/against it may help to encourage reaching activity and support spatial cognitive learning. This paper investigated whether the accuracy of peripersonal reaching (space within arm's reach) can be improved by the use of dynamic sound from both the objects to reach for and the reaching hand itself (via a worn speaker). We ran two studies that tested the efficacy of static and dynamic audio feedback designs with blind and visually impaired young people, to identify optimal feedback designs. Study 1 was with young adults aged 18 to 22 and Study 2 involved children aged 12 to 17. The results showed that dynamic audio feedback helps to build spatial connections between the objects and the reaching hand and participants were able to reach more accurately, compared to unchanging feedback.","Graham Wilson, Stephen Brewster","reaching, sound perception, visual impairment",209,218
10.1145/2384916.2384941,ASSETS,2012,Crowdsourcing subjective fashion advice using VizWiz,challenges and opportunities,"Fashion is a language. How we dress signals to others who we are and how we want to be perceived. However, this language is primarily visual, making it inaccessible to people with vision impairments. Someone who is low-vision or completely blind cannot see what others are wearing or readily know what constitutes the norms and extremes of fashion, but most everyone they encounter can see (and judge) their fashion choices. We describe our findings of a diary study with people with vision impairments that revealed the many accessibility barriers fashion presents, and how an online survey revealed that clothing decisions are often made collaboratively, regardless of visual ability. Based on these findings, we identified a need for a collaborative and real-time environment for fashion advice. We have tested the feasibility of providing this advice through crowdsourcing using VizWiz, a mobile phone application where participants receive nearly real-time answers to visual questions. Our pilot study results show that this application has the potential to address a great need within the blind community, but remaining challenges include improving photo capture and assembling a set of crowd workers with the requisite expertise. More broadly our research highlights the feasibility of using crowdsourcing for subjective, opinion-based advice.","Michele Burton, Erin Brady, Robin Brewer, Callie Neylan, Jeffrey Bigham, Amy Hurst","blind users, crowdsourcing, fashion",135,142
10.1145/2384916.2384923,ASSETS,2012,Considerations for technology that support physical activity by older adults, ,"Barriers to physical activity prevent older adults from meeting recommended physical activity levels necessary for maintaining quality of life. As technology becomes more advanced, we have the opportunity and the responsibility to address concerns faced by the aging population. We seek opportunities for technology to empower older adults to overcome barriers on their own by interviewing and learning from older adults who have successfully overcome these barriers. In this paper, we present a set of needs that technology can address, and considerations for designing technology interventions that support physical activity by older adults.","Chloe Fan, Jodi Forlizzi, Anind Dey","barriers, older adults, physical activity, technology interventions",33,40
10.1145/3234695.3236340,ASSETS,2018,MANA,Designing and Validating a User-Centered Mobility Analysis System,"In this paper, we demonstrate a new IMU-based wearable system (dubbed MANA or Mobility ANAlytics) for measuring gait in a clinical setting. The design process and choices that were made to ensure that the technology was invisible and accessible are described. We collect a rich and diverse dataset of walking data from sixty participants, including forty people with Parkinson's Disease (PD). The system is then validated in a clinical setting with this dataset. We present novel and innovative algorithms to measure common gait parameters. The system is able to estimate these gait parameters with high accuracy, with a mean absolute error of 4.0 cm for stride length and 2.6 cm for step length, outperforming all state-of-the-art methods that included data from people with PD.","Boyd Anderson, Shenggao Zhu, Ke Yang, Jian Wang, Hugh Anderson, Chao Xu Tay, Vincent Tan, Ye Wang","Parkinson's disease, inertial measurement units, mobility analysis, user-centered design, wearable sensors",321,332
10.1145/2982142.2982161,ASSETS,2016,An Evaluation of SingleTapBraille Keyboard,A Text Entry Method that Utilizes Braille Patterns on Touchscreen Devices,"This paper provides an evaluation of the SingleTapBraille keyboard, designed to assist people with no or low vision in using touchscreen smartphones. This application allows blind users to input characters based on braille patterns. To assess SingleTapBraille, this study compares its performance with that of the commonly used QWERTY keyboard. We conducted an evaluation study with 7 blind participants to examine the performance of both keyboards on Android platforms. Overall, participants were able to quickly adjust to SingleTapBraille and type on touchscreen devices using their knowledge of Braille patterns within fifteen to twenty minutes of introduction to the system. The SingleTapBraille keyboard was better than the QWERTY keyboard in terms of both speed and accuracy, indicating that SingleTapBraille represents an improvement over existing alternatives in making touchscreen keyboards more accessible for blind users. Based on the evaluation results and the feedback of our participants, we discuss the strengths and weaknesses of previous keyboards that have been used by participants, as well as those of SingleTapBraille. In doing so, we consider possible design improvements for the future development of accessible keyboards for blind users.","Maraim Alnfiai, Srinivas Sampalli","accessibility, blindness, braille patterns, single-touch interaction, smartphone devices, tapping, gestures., text entry, touchscreens",161,169
10.1145/2982142.2982158,ASSETS,2016,How Designing for People With and Without Disabilities Shapes Student Design Thinking, ,"Despite practices addressing disability in design and advocating user-centered design (UCD) approaches, popular mainstream technologies remain largely inaccessible for people with disabilities. We conducted a design course study investigating how student designers regard disability and explored how designing for both disabled and non-disabled users encouraged students to think about accessibility throughout the design process. Students focused on a design project while learning UCD concepts and techniques, working with people with and without disabilities throughout the project. We found that designing for both disabled and non-disabled users surfaced challenges and tensions in finding solutions to satisfy both groups, influencing students' attitudes toward accessible design. In addressing these tensions, non-functional aspects of accessible design emerged as important complements to functional aspects for users with and without disabilities.","Kristen Shinohara, Cynthia Bennett, Jacob Wobbrock","accessibility, assistive technology, design, design thinking",229,237
10.1145/3308561.3353781,ASSETS,2019,Evaluating the Benefit of Highlighting Key Words in Captions for People who are Deaf or Hard of Hearing, ,"Recent research has investigated automatic methods for identifying how important each word in a text is for the overall message, in the context of people who are Deaf and Hard of Hearing (DHH) viewing video with captions. We examine whether DHH users report benefits from visual highlighting of important words in video captions. In formative interview and prototype studies, users indicated a preference for underlining of 5%-15% of words in a caption text to indicate that they are important, and they expressed an interest for such text markup in the context of educational lecture videos. In a subsequent user study, 30 DHH participants viewed lecture videos in two forms: with and without such visual markup. Users indicated that the videos with captions containing highlighted words were easier to read and follow, with lower perceived task-load ratings, compared to the videos without highlighting. This study motivates future research on caption highlighting in online educational videos, and it provides a foundation for how to evaluate the efficacy of such systems with users.","Sushant Kafle, Peter Yeung, Matt Huenerfauth","caption highlighting, captioning system, deaf and hard of hearing, feedback, text highlighting, user study",43,55
10.1145/2661334.2661372,ASSETS,2014,Accessibility in context,understanding the truly mobile experience of smartphone users with motor impairments,"Lab-based studies on touchscreen use by people with motor impairments have identified both positive and negative impacts on accessibility. Little work, however, has moved beyond the lab to investigate the truly mobile experiences of users with motor impairments. We conducted two studies to investigate how smartphones are being used on a daily basis, what activities they enable, and what contextual challenges users are encountering. The first study was a small online survey with 16 respondents. The second study was much more in depth, including an initial interview, two weeks of diary entries, and a 3-hour contextual session that included neighborhood activities. Four expert smartphone users participated in the second study and we used a case study approach for analysis. Our findings highlight the ways in which smartphones are enabling everyday activities for people with motor impairments, particularly in overcoming physical accessibility challenges in the real world and supporting writing and reading. We also identified important situational impairments, such as the inability to retrieve the phone while in transit, and confirmed many lab-based findings in the real-world setting. We present design implications and directions for future work.","Maia Naftali, Leah Findlater","accessibility, assistive devices, case study., contextual interviews, mobile, smartphones",209,216
10.1145/1878803.1878842,ASSETS,2010,Stroke therapy through motion-based games,a case study,"In the United States alone, more than five million people are living with long term motor impairments caused by a stroke. Video game-based therapies show promise in helping people recover lost range of motion and motor control. While researchers have demonstrated the potential utility of game-based rehabilitation through controlled studies, relatively little work has explored longer-term home-based use of therapeutic games. We conducted a six-week home study with a 62 year old woman who was seventeen years post-stroke. She played therapeutic games for approximately one hour a day, five days a week. Over the six weeks, she recovered significant motor abilities, which is unexpected given the time since her stroke. Through observations and interviews, we present lessons learned about the barriers and opportunities that arise from long-term home-based use of therapeutic games.","Gazihan Alankus, Rachel Proffitt, Caitlin Kelleher, Jack Engsberg","case study, stroke rehabilitation, video games",219,226
10.1145/3234695.3236353,ASSETS,2018,Exploring the Data Tracking and Sharing Preferences of Wheelchair Athletes, ,"Sports are increasingly data-driven. Athletes use a variety of physical activity monitors to capture their movements, improve performance, and achieve excellence. To understand how wheelchair athletes want to use and share their activity data, we conducted a study using a prototype wheelchair fitness tracking device, which served as a probe to facilitate discussions. We interviewed 15 wheelchair basketball players about the use of performance data in the context of wheelchair basketball, and we discuss several implications for using and sharing automatically-tracked data. We find that the wheelchair basketball community is less concerned about the privacy of their data, and, in contrast to health data, athletes are motivated by competition. We conclude with a set of design opportunities that leverage digitized performance metrics within wheelchair basketball, which could apply to the broader wheelchair and adaptive athletics community.","Patrick Carrington, Gierad Laput, Jeffrey Bigham","activity recognition, wheelchair, wheelchair basketball",242,248
10.1145/3308561.3353776,ASSETS,2019,Closing the Gap,Designing for the Last-Few-Meters Wayfinding Problem for People with Visual Impairments,"Despite the major role of Global Positioning Systems (GPS) as a navigation tool for people with visual impairments (VI), a crucial missing aspect of point-to-point navigation with these systems is the last-few-meters wayfinding problem. Due to GPS inaccuracy and inadequate map data, systems often bring a user to the vicinity of a destination but not to the exact location, causing challenges such as difficulty locating building entrances or a specific storefront from a series of stores. In this paper, we study this problem space in two parts: (1) A formative study (N=22) to understand challenges, current resolution techniques, and user needs; and (2) A design probe study (N=13) using a novel, vision-based system called Landmark AI to understand how technology can better address aspects of this problem. Based on these investigations, we articulate a design space for systems addressing this challenge, along with implications for future systems to support precise navigation for people with VI.","Manaswi Saha, Alexander Fiannaca, Melanie Kneisel, Edward Cutrell, Meredith Morris","accessibility, blindness, landmarks, wayfinding",222,235
10.1145/3132525.3132545,ASSETS,2017,Virtual Navigation for Blind People,Building Sequential Representations of the Real-World,"When preparing to visit new locations, sighted people often look at maps to build an a priori mental representation of the environment as a sequence of step-by-step actions and points of interest (POIs), e.g., turn right after the coffee shop. Based on this observation, we would like to understand if building the same type of sequential representation, prior to navigating in a new location, is helpful for people with visual impairments (VI). In particular, our goal is to understand how the simultaneous interplay between turn-by-turn navigation instructions and the relevant POIs in the route can aid the creation of a memorable sequential representation of the world. To this end, we present two smartphone-based virtual navigation interfaces: VirtualLeap, which allows the user to jump through a sequence of street intersection labels, turn-by-turn instructions and POIs along the route; and VirtualWalk, which simulates variable speed step-by-step walking using audio effects, whilst conveying similar route information. In a user study with 14 VI participants, most were able to create and maintain an accurate mental representation of both the sequential structure of the route and the approximate locations of the POIs. While both virtual navigation modalities resulted in similar spatial understanding, results suggests that each method is useful in different interaction contexts.","Jo&#227;o Guerreiro, Dragan Ahmetovic, Kris Kitani, Chieko Asakawa","assistive technology, blind navigation, cognitive mapping, orientation and mobility, virtual navigation",280,289
10.1145/3308561.3353783,ASSETS,2019,GestureCalc,An Eyes-Free Calculator for Touch Screens,"A digital calculator is one of the most frequently used touch screen applications. However, keypad-based character input in existing calculator applications requires precise, targeted key presses that are time-consuming and error-prone for many screen readers users. We introduce GestureCalc, a digital calculator that uses target-free gestures for arithmetic tasks. It allows eyes-free target-less input of digits and operations through taps and directional swipes with one to three fingers, guided by minimal audio feedback. We conducted a mixed methods longitudinal study with eight screen reader users and found that they entered characters with GestureCalc 40.5% faster on average than with a typical touch screen calculator. Participants made more mistakes but also corrected more errors with GestureCalc, resulting in 52.2% fewer erroneous calculations than the baseline. Over the three sessions in the study, participants were able to learn the GestureCalc gestures and efficiently perform short calculations. From our interviews after the second session, participants recognized the effort in learning a new gesture set, yet reported confidence in their ability to become fluent in practice.","Bindita Chaudhuri, Leah Perlmutter, Justin Petelka, Philip Garrison, James Fogarty, Jacob Wobbrock, Richard Ladner","digital calculator, eyes-free entry, gesture input, mobile devices, touch screen",112,123
10.1145/3132525.3132539,ASSETS,2017,CymaSense,A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum,"Music Therapy has been shown to be an effective intervention for clients with Autism Spectrum Condition (ASC), a lifelong neurodevelopmental condition that can affect people in a number of ways. This paper presents a study evaluating the use of a multimodal 3D interactive tool, CymaSense, within a series of music therapy sessions. Eight adults with ASC participated in an 8-week period using a single case experimental design approach. The study used qualitative and quantitative methodological tools for analysis within and beyond the therapy sessions. The results indicate an increase in communicative behaviours for both verbal and non-verbal participants.","John McGowan, Gr&#233;gory Lepl&#226;tre, Iain McGregor","assistive technologies, autism spectrum condition (asc), cymatics, interactive audio-visual, multi-sensory environments, music therapy",62,71
10.1145/2982142.2982178,ASSETS,2016,Supporting Orientation of People with Visual Impairment,Analysis of Large Scale Usage Data,"In the field of assistive technology, large scale user studies are hindered by the fact that potential participants are geographically sparse and longitudinal studies are often time consuming. In this contribution, we rely on remote usage data to perform large scale and long duration behavior analysis on users of iMove, a mobile app that supports the orientation of people with visual impairments. Exploratory analysis highlights popular functions, common configuration settings, and usage patterns among iMove users. The study shows stark differences between users accessing the app through VoiceOver and other users, who tend to use the app more scarcely and sporadically.Analysis through clustering of VoiceOver iMove user interactions discovers four distinct user groups: 1) users interested in surrounding points of interest, 2) users keeping the app active for long sessions while in movement, 3) users interacting in short bursts to inquire about current location, and 4) users querying in bursts about surrounding points of interest and addresses. Our analysis provides insights into iMove's user base and can inform decisions for tailoring the app to diverse user groups, developing future improvements of the software, or guiding the design process of similar assistive tools.","Hernisa Kacorri, Sergio Mascetti, Andrea Gerino, Dragan Ahmetovic, Hironobu Takagi, Chieko Asakawa","interaction stream analysis, outdoor navigation, user behavior models, visual impairment",151,159
10.1145/2700648.2809861,ASSETS,2015,Typing Performance of Blind Users,"An Analysis of Touch Behaviors, Learning Effect, and In-Situ Usage","Non-visual text-entry for people with visual impairments has focused mostly on the comparison of input techniques reporting on performance measures, such as accuracy and speed. While researchers have been able to establish that non-visual input is slow and error prone, there is little understanding on how to improve it. To develop a richer characterization of typing performance, we conducted a longitudinal study with five novice blind users. For eight weeks, we collected in-situ usage data and conducted weekly laboratory assessment sessions. This paper presents a thorough analysis of typing performance that goes beyond traditional aggregated measures of text-entry and reports on character-level errors and touch measures. Our findings show that users improve over time, even though it is at a slow rate (0.3 WPM per week). Substitutions are the most common type of error and have a significant impact on entry rates. In addition to text input data, we analyzed touch behaviors, looking at touch contact points, exploration movements, and lift positions. We provide insights on why and how performance improvements and errors occur. Finally, we derive some implications that should inform the design of future virtual keyboards for non-visual input.","Hugo Nicolau, Kyle Montague, Tiago Guerreiro, Andr&#233; Rodrigues, Vicki Hanson","behavior, blind, input, novice, performance, text-entry, touch",273,280
10.1145/2700648.2809856,ASSETS,2015,Towards Efficacy-Centered Game Design Patterns For Brain Injury Rehabilitation,A Data-Driven Approach,"Games are often used in brain injury (BI) therapy sessions to help motivate patients to engage in rehabilitation activities. In this paper, we explore game design patterns as a mechanism to help game designers understand needs in BI therapy. Design patterns, originating from the work of Christopher Alexander, aim to provide a common language to support the creative work of designers by documenting solutions that have successful addressed recurring design problems. Through analyzing data we gathered on the use of commercial games in BI therapy, we generated a list of 14 'efficacy-centered game design patterns' that focused on game design considerations when addressing therapeutic goals in BI rehabilitation. We argue that our patterns can serve as a common language to support the design of BI rehabilitation games; additionally, our data-driven approach sets up a paradigm for generating game design patterns in related areas.","Jinghui Cheng, Cynthia Putnam, Doris Rusch","brain injury, game design, game design patterns, gaming therapy",291,299
10.1145/2982142.2982165,ASSETS,2016,The Cost of Turning Heads,A Comparison of a Head-Worn Display to a Smartphone for Supporting Persons with Aphasia in Conversation,"Current symbol-based dictionaries providing vocabulary support for persons with the language disorder, aphasia, are housed on smartphones or other portable devices. To employ the support on these external devices requires the user to divert their attention away from their conversation partner, to the neglect of conversation dynamics like eye contact or verbal inflection. A prior study investigated head-worn displays (HWDs) as an alternative form factor for supporting glanceable, unobtrusive, and always-available conversation support, but it did not directly compare the HWD to a control condition. To address this limitation, we compared vocabulary support on a HWD to equivalent support on a smartphone in terms of overall experience, perceived focus, and conversational success. Lastly, we elicited critical discussion of how each device might be better designed for conversation support. Our work contributes (1) evidence that a HWD can support more efficient communication, (2) preliminary results that a HWD can provide a better overall experience using assistive vocabulary, and (3) a characterization of the design features persons with aphasia value in portable conversation support technologies. Our findings should motivate further work on head-worn conversation support for persons with aphasia.","Kristin Williams, Karyn Moffatt, Jonggi Hong, Yasmeen Faroqi-Shah, Leah Findlater","aac, accessibility., aphasia, conversational support, head-worn display, wearable computing",111,120
10.1145/3308561.3353774,ASSETS,2019,"Sign Language Recognition, Generation, and Translation",An Interdisciplinary Perspective,"Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.","Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa Verhoef, Christian Vogler, Meredith Ringel Morris","and generation, asl, recognition, sign language, translation",16,31
10.1145/2384916.2384927,ASSETS,2012,iSCAN,a phoneme-based predictive communication aid for nonspeaking individuals,"The high incidence of literacy deficits among people with severe speech impairments (SSI) has been well documented. Without literacy skills, people with SSI are unable to effectively use orthographic-based communication systems to generate novel linguistic items in spontaneous conversation. To address this problem, phoneme-based communication systems have been proposed which enable users to create spoken output from phoneme sequences. In this paper, we investigate whether prediction techniques can be employed to improve the usability of such systems. We have developed iSCAN, a phoneme-based predictive communication system, which offers phoneme prediction and phoneme-based word prediction. A pilot study with 16 able-bodied participants showed that our predictive methods led to a 108.4% increase in phoneme entry speed and a 79.0% reduction in phoneme error rate. The benefits of the predictive methods were also demonstrated in a case study with a cerebral palsied participant. Moreover, results of a comparative evaluation conducted with the same participant after 16 sessions using iSCAN indicated that our system outperformed an orthographic-based predictive communication device that the participant has used for over 4 years.","Ha Trinh, Annalu Waller, Keith Vertanen, Per Ola Kristensson, Vicki Hanson","augmentative and alternative communication, phoneme prediction, phoneme-based communication, word prediction",57,64
10.1145/2513383.2513445,ASSETS,2013,Uncovering information needs for independent spatial learning for users who are visually impaired, ,"Sighted individuals often develop significant knowledge about their environment through what they can visually observe. In contrast, individuals who are visually impaired mostly acquire such knowledge about their environment through information that is explicitly related to them. This paper examines the practices that visually impaired individuals use to learn about their environments and the associated challenges. In the first of our two studies, we uncover four types of information needed to master and navigate the environment. We detail how individuals' context impacts their ability to learn this information, and outline requirements for independent spatial learning. In a second study, we explore how individuals learn about places and activities in their environment. Our findings show that users not only learn information to satisfy their immediate needs, but also to enable future opportunities -- something existing technologies do not fully support. From these findings, we discuss future research and design opportunities to assist the visually impaired in independent spatial learning.","Nikola Banovic, Rachel Franz, Khai Truong, Jennifer Mankoff, Anind Dey","assistive technology, navigation, orientation & mobility, spatial learning, visually impaired, wayfinding",1,8
10.1145/3308561.3353791,ASSETS,2019,Effect of Automatic Sign Recognition Performance on the Usability of Video-Based Search Interfaces for Sign Language Dictionaries, ,"Researchers have investigated various methods to help users search for the meaning of an unfamiliar word in American Sign Language (ASL). Some are based on sign-recognition technology, e.g. a user performs a word into a webcam and obtains a list of possible matches in the dictionary. However, developers of such technology report the performance of their systems inconsistently, and prior research has not examined the relationship between the performance of search technology and users' subjective judgements for this task. We conducted two studies using a Wizard-of-Oz prototype of a webcam-based ASL dictionary search system to investigate the relationship between the performance of such a system and user judgements. We found that in addition to the position of the desired word in a list of results, which is what is often reported in literature; the similarity of the other words in the results list also affected users' judgements of the system. We also found that metrics that incorporate the precision of the overall list correlated better with users' judgements than did metrics currently reported in prior ASL dictionary research.","Oliver Alonzo, Abraham Glasser, Matt Huenerfauth","american sign language (asl), dictionary, search",56,67
10.1145/3132525.3132555,ASSETS,2017,In-context Q&#38;A to Support Blind People Using Smartphones, ,"Blind people face many barriers using smartphones. Still, previous research has been mostly restricted to non-visual gestural interaction, paying little attention to the deeper daily challenges of blind users. To bridge this gap, we conducted a series of workshops with 42 blind participants, uncovering application challenges across all levels of expertise, most of which could only be surpassed through a support network. We propose Hint Me!, a human-powered service that allows blind users to get in-app assistance by posing questions or browsing previously answered questions on a shared knowledge-base. We evaluated the perceived usefulness and acceptance of this approach with six blind people. Participants valued the ability to learn independently and anticipated a series of usages: labeling, layout and feature descriptions, bug workarounds, and learning to accomplish tasks. Creating or browsing questions depends on aspects like privacy, knowledge of respondents and response time, revealing the benefits of a hybrid approach.","Andr&#233; Rodrigues, Kyle Montague, Hugo Nicolau, Jo&#227;o Guerreiro, Tiago Guerreiro","assistance, blind, human computation, smartphone",32,36
10.1145/2384916.2384926,ASSETS,2012,What we talk about,designing a context-aware communication tool for people with aphasia,"Many people with aphasia experience difficulty recalling words extemporaneously, but can recognize those words when given an image, text, or audio prompt. Augmented and alternative communication (AAC) systems can help address this problem by enabling people with aphasia to browse and select from a list of vocabulary words. However, these systems can be difficult to navigate, especially when they contain large amounts of content. In this paper, we describe the design of TalkAbout, a context-aware, adaptive AAC system that provides users with a word list that is adapted to their current location and conversation partner. We describe the design and development of TalkAbout, which we conducted in collaboration with 5 adults with aphasia. We then present guidelines for developing and evaluating context-aware technology for people with aphasia.","Shaun Kane, Barbara Linam-Church, Kyle Althoff, Denise McCall","accessibility, aphasia, augmented and alternative communication, context-aware computing, participatory design",49,56
10.1145/2661334.2661355,ASSETS,2014,How companies engage customers around accessibility on social media, ,"Social media offers a targeted way for mainstream technology companies to communicate with people with disabilities about the accessibility problems that they face. While companies have started to engage with users on social media about accessibility, they differ greatly in terms of their approach and how well they support the ways in which their users want to engage. In this paper, we describe current use patterns of six corporate accessibility teams and their users on Twitter, and present an analysis of these interactions. We find that while many users want to interact directly with companies about accessibility, companies prefer to redirect them to other channels and use Twitter for broadcast messages promoting their accessibility work instead. Our analysis demonstrates that users want to use social media to become part of the process of improving accessibility of mainstream technology, and suggests the extent to which a company is able to leverage this input depends greatly on how they choose to present themselves and interact on social media.","Erin Brady, Jeffrey Bigham","accessibility, corporations, social media, twitter",51,58
10.1145/3132525.3132544,ASSETS,2017,Introducing People with ASD to Crowd Work, ,"Adults with Autism Spectrum Disorders (ASD) are unemployed at a high rate, in part because the constraints and expectations of traditional employment can be difficult for them. In this paper, we report on our work in introducing people with ASD to remote work on a crowdsourcing platform and a prototype tool we developed by working with participants. We conducted a six-week long user-centered design study with three participants with ASD. The early stage of the study focused on assessing the abilities of our participants to search and work on micro-tasks available on the crowdsourcing market. Based on our preliminary findings, we designed, developed, and evaluated a prototype tool to facilitate image transcription tasks that are increasingly popular on crowd labor markets. Our findings suggest that people with ASD have varying levels of ability to work on micro-tasks, but are likely to be able to work on tasks like image transcription. The tool we introduce, Assistive Task Queue (ATQ), facilitated our participants' completion of image transcription tasks by removing ambiguity in finding the next task to work on and in simplifying tasks into discrete steps. ATQ may serve as a general platform for finding and delivering appropriate tasks to workers with autism.","Kotaro Hara, Jeffrey Bigham","autism spectrum disorder, crowdsourcing",42,51
10.1145/1878803.1878821,ASSETS,2010,A tactile windowing system for blind users, ,"Today's window systems present the information in a graphical and thereby a spatial manner making the text-only access of a standard Braille device insufficient to enable blind users an equivalent exploration of the data. In this paper we present the planar Braille Window System (BWS) designed for a tactile display consisting of a pin-matrix of 120 columns and 60 rows. The system is composed of six separate regions enabling the user to receive different types of information simultaneously. The content of the main region containing Braille windows can be shown in various manners (text-or graphics-based) through four different views. The interaction within our Braille Window System is implemented not only by keyboard shortcuts but also by the use of multitouch gestures. Therefore the user is able to interact directly on the touch-sensitive display. A study conducted with eight blind users has confirmed the concept of Braille windows, regions and views. Especially the gestural input for exploring details of the content offers new possibilities in interacting within a GUI.","Denise Prescher, Gerhard Weber, Martin Spindler","blind user, braille, gesture interaction, planar tactile display, screen reader, tactile graphics",91,98
10.1145/3132525.3132542,ASSETS,2017,Evaluating the Usability of Automatically Generated Captions for People who are Deaf or Hard of Hearing, ,"The accuracy of Automated Speech Recognition (ASR) technology has improved, but it is still imperfect in many settings. Researchers who evaluate ASR performance often focus on improving the Word Error Rate (WER) metric, but WER has been found to have little correlation with human-subject performance on many applications. We propose a new captioning-focused evaluation metric that better predicts the impact of ASR recognition errors on the usability of automatically generated captions for people who are Deaf or Hard of Hearing (DHH). Through a user study with 30 DHH users, we compared our new metric with the traditional WER metric on a caption usability evaluation task. In a side-by-side comparison of pairs of ASR text output (with identical WER), the texts preferred by our new metric were preferred by DHH participants. Further, our metric had significantly higher correlation with DHH participants' subjective scores on the usability of a caption, as compared to the correlation between WER metric and participant subjective scores. This new metric could be used to select ASR systems for captioning applications, and it may be a better metric for ASR researchers to consider when optimizing ASR systems.","Sushant Kafle, Matt Huenerfauth","accessibility for people who are deaf or hard-of-hearing, automatic speech recognition, caption usability evaluation, real-time captioning system",165,174
10.1145/3132525.3132535,ASSETS,2017,NavCog3,An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment,"Navigating in unfamiliar environments is challenging for most people, especially for individuals with visual impairments. While many personal navigation tools have been proposed to enable in- dependent indoor navigation, they have insufficient accuracy (e.g., 5-10 m), do not provide semantic features about surroundings (e.g., doorways, shops, etc.), and may require specialized devices to function. Moreover, the deployment of many systems is often only evaluated in constrained scenarios, which may not precisely reflect the performance in the real world. Therefore, we have de- signed and implemented NavCog3, a smartphone-based indoor navigation assistant that has been evaluated in a 21,000 m2 shop- ping mall. In addition to turn-by-turn instructions, it provides in- formation on landmarks (e.g., tactile paving) and points of interests nearby. We first conducted a controlled study with 10 visually im- paired users to assess localization accuracy and the perceived use- fulness of semantic features. To understand the usability of the app in a real-world setting, we then conducted another study with 43 participants with visual impairments where they could freely nav- igate in the shopping mall using NavCog3. Our findings suggest that NavCog3 can open a new opportunity for users with visual im- pairments to independently find and visit large and complex places with confidence.","Daisuke Sato, Uran Oh, Kakuya Naito, Hironobu Takagi, Kris Kitani, Chieko Asakawa","indoor navigation, points of interest, user evaluation, visual impairments, voice- based interaction",270,279
10.1145/3132525.3132557,ASSETS,2017,AMI,An Adaptable Music Interface to Support the Varying Needs of People with Dementia,"Dementia is a progressive, degenerative syndrome that erodes cognition, long term memory, and the ability to maintain social relationships. Anxiety is common among those with dementia, and ranges from momentary and mild, to chronic and severe. Listening to familiar music from childhood or early adulthood has been shown to provide therapeutic and positive quality of life effects for individuals with dementia, but most modern interfaces are unfamiliar and difficult to use which may add frustration and stress that music is intended to relieve. To enable individuals with dementia to control playback of music, we present AMI, a tangible music player that can be reconfigured and adapted to meet the changing needs and preferences of individuals. AMI provides a set of input components (e.g., buttons, switches, knobs) with varying physical properties which can be easily interchanged by a non-technical user (such as a caregiver). This work contributes the system design, results of user tests with the target population, as well as a set of design principles that can be used in the development of future interfaces.","P. Seymour, Justin Matejka, Geoff Foulds, Ihor Petelycky, Fraser Anderson","accessibility, dementia, music and audio, personalization, tangible input",150,154
10.1145/3132525.3132532,ASSETS,2017,Opinions and Preferences of Blind and Low Vision Consumers Regarding Self-Driving Vehicles,Results of Focus Group Discussions,"Fully autonomous vehicles, commonly referred to as self-driving vehicles, are an emerging technology that may hold tremendous mobility potential for individuals who are blind or visually impaired who have been previously disadvantaged by an inability to operate conventional motor vehicles. This study explores the opinions of 38 participants who are blind and low vision, through the use of focus group methodology, regarding this emerging self-driving vehicle technology. Participants were overwhelmingly optimistic about the potential for independence and mobility that self-driving vehicles may provide but were concerned that the needs of individuals with visual impairments were not being adequately considered in the development of the technology. Participants also raised questions about how the technology would satisfy their need for situational awareness, how the technology would enable blind or visually impaired operators to verify their arrival at their desired location and a host of issues related to parking, vehicle location and roadside assistance. Participants also expressed a preference for smartphone and speech input capabilities as a primary means of system interaction. These findings suggest that at a minimum more needs to be done to engage individuals with visual impairments in the development of self-driving vehicle technology and to increase awareness of manufacturer efforts.","Julian Brinkley, Brianna Posadas, Julia Woodward, Juan Gilbert","advanced driver assistance systems, blindness, low vision, accessibility, self-driving vehicles",290,299
10.1145/3308561.3353810,ASSETS,2019,Identifying Comfort Areas in 3D Space for Persons with Upper Extremity Mobility Impairments Using Virtual Reality, ,"We present a method to extract workspace comfort areas for ergonomic placement of assistive technologies for persons with upper extremity mobility impairments Currently, areas of comfort are determined using multiple physical prototypes over several iterations which is expensive and time consuming. Our method utilizes a virtual reality exergame to obtain user-specific end effector motion data which is then combined with kernel density estimation to identify areas of frequent motion. Levels of comfort were confirmed by calculating shoulder joint forces necessary to reach these frequented areas and validated through a user study. Identifying areas of comfort in the workspace allows for optimal positioning and training of input devices for numerous applications.","Shanmugam Muruga Palaniappan, Ting Zhang, Bradley Duerstock","comfort areas, ergonomics, joint forces, mixed reality, mobility impairments, virtual reality",495,499
10.1145/2384916.2384936,ASSETS,2012,Exploration and avoidance of surrounding obstacles for the visually impaired, ,Proximity-based interaction through a long cane is essential for the blind and the visually impaired. We designed and implemented an obstacle detector consisting of a 3D Time-of-Flight (TOF) camera and a planar tactile display to extend the interaction range and provide rich non-visual information about the environment. Users choose a better path after acquiring the spatial layout of obstacles than with a white cane alone. A user study with 6 blind people was analyzed and showed extra time is needed to ensure safe walking while reading the layout. Both hanging and ground-based obstacles were circumvented. Tactile mapping information has been designed for representation of precise spatial information around a blind user.,"Limin Zeng, Denise Prescher, Gerhard Weber","3D TOF camera, haptic user interface, obstacle avoidance, tactile symbol",111,118
10.1145/3308561.3353794,ASSETS,2019,Ageing is Not a Disease,Pitfalls for the Acceptance of Self-Management Health Systems Supporting Healthy Ageing,"Recently, a shift from curative to preventive care is promoted, by which patients are expected to become active and use diverse forms of self-management health systems (SMHS), i.e., integrated solutions that present data from multiple sensors and/or self-reports, possibly enhanced with risk assessment and decision support, to perform health-related actions. This is promoted as contributing to living longer and healthier, assisting ageing-in-place. Hence, older adults have also become a potential target of SMHS. While studies are performed on uses and attitudes of specific patient groups towards SMHS, studies that focus on older, including the oldest, adults are scarce. Therefore, we report on a qualitative study with 20 older adults (mean age = 80). Through thematic analysis, we identified four themes (i.e., enforced use of technology; need for support in technology use; equivocal stance towards sharing data; hypothetical value of technology for healthy ageing) to provide a deeper understanding of older adults' attitudes and engagement with information- and communication technologies (ICT) in general and SMHS in particular. We also present four pitfalls, unified by a central concept ""Ageing is not a disease'', along with design considerations for future SMHS.","Ine D'Haeseleer, Kathrin Gerling, Dominique Schreurs, Bart Vanrumste, Vero Vanden Abeele","attitudes, older adults, self-management health systems, thematic analysis",286,298
10.1145/2700648.2809848,ASSETS,2015,Towards Large Scale Evaluation of Novel Sonification Techniques for Non Visual Shape Exploration, ,"There are several situations in which a person with visual impairment or blindness needs to extract information from an image. Examples include everyday activities, like reading a map, as well as educational activities, like exercises to develop visuospatial skills. In this contribution we propose a set of 6 sonification techniques to recognize simple shapes on touchscreen devices. The effectiveness of these sonification techniques is evaluated though Invisible Puzzle, a mobile application that makes it possible to conduct non-supervised evaluation sessions. Invisible Puzzle adopts a gamification approach and is a preliminary step in the development of a complete game that will make it possible to conduct a large scale evaluation with hundreds or thousands of blind users. With Invisible Puzzle we conducted 131 tests with sighted subjects and 18 tests with subjects with blindness. All subjects involved in the process successfully completed the evaluation session, with high engagement, hence showing the effectiveness of the evaluation procedure. Results give interesting insights on the differences among the sonification techniques and, most importantly, show that, after a short training, subjects are able to identify many different shapes.","Andrea Gerino, Lorenzo Picinali, Cristian Bernareggi, Nicol&#242; Alabastro, Sergio Mascetti","accessibility, gamification, image recognition, remote evaluation, sonification, touch-screen devices, visual impairment or blindness",13,21
10.1145/1878803.1878827,ASSETS,2010,Multiple view perspectives,improving inclusiveness and video compression in mainstream classroom recordings,"Multiple View Perspectives (MVP) enables deaf and hard of hearing students to view and record multiple video views of a classroom presentation using a stand-alone solution. We show that deaf and hard of hearing students prefer multiple, focused videos over a single, high-quality video and that a compacted layout of only the most important views is preferred. We also show that this approach empowers deaf and hard of hearing students by virtue of its low cost, flexibility, and ease of use in the classroom.","Raja Kushalnagar, Anna Cavender, Jehan-Fran&#231;ois P&#226;ris","accessible technology, deaf and hard of hearing users",123,130
10.1145/3132525.3132559,ASSETS,2017,Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites, ,"Sign language is the primary medium of communication for many people who are deaf or hard of hearing. Members of this community access online sign language (SL) content posted on video sharing sites to stay informed. Unfortunately, locating SL videos can be difficult since the text-based search on video sharing sites is based on metadata rather than on the video content. Low cost or real-time video classification techniques would be invaluable for improving access to this content. Our prior work developed a technique to identify SL content based on video features alone but is computationally expensive. Here we describe and evaluate three optimization strategies that have the potential to reduce the computation time without overly impacting precision and recall. Two optimizations reduce the cost of face-detection, whereas the third focuses on analyzing shorter segments of the video. Our results identify a combination of these techniques that yields a 96% reduction in computation time while losing only 1% in F1 score. To further reduce computation, we additionally explore a keyframe-based approach that achieves comparable recall but lower precision than the above techniques, making it appropriate as an early filter in a staged classifier.","Frank Shipman, Satyakiran Duggina, Caio Monteiro, Ricardo Gutierrez-Osuna","computer vision, sign language detection, signal processing",185,189
10.1145/1878803.1878808,ASSETS,2010,"A general education course on universal access, disability, technology and society", ,"This paper reports on a General Education course called ""Universal Access: Disability, Technology and Society"" that enables students from all majors to learn more about disability and the issues that surround it, as well as how Assistive Technology facilitates effective participation of those with disabilities in society. Guest lectures, meant to give the students different perspectives on disability, are integral part of the course. Guest lecturers include experts in disability studies, professionals working with people with disabilities, and persons with disability. To gain practical knowledge, the students carry out group projects or volunteering activities that involves people with disabilities. Since its first introduction in 2006, the course had always filled to capacity. A survey with 75 students conducted in Winter 2010 revealed that students felt that their knowledge about universal access and disabilities had improved significantly, and that they had become aware of accessibility in everyday life.","Sri Kurniawan, Sonia Arteaga, Roberto Manduchi","disability awareness, general education, undergraduate education, universal access",11,18
10.1145/3234695.3236364,ASSETS,2018,Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis, ,"We conduct the first large-scale analysis of the accessibility of mobile apps, examining what unique insights this can provide into the state of mobile app accessibility. We analyzed 5,753 free Android apps for label-based accessibility barriers in three classes of image-based buttons: Clickable Images, Image Buttons, and Floating Action Buttons. An epidemiology-inspired framework was used to structure the investigation. The population of free Android apps was assessed for label-based inaccessible button diseases. Three determinants of the disease were considered: missing labels, duplicate labels, and uninformative labels. The prevalence, or frequency of occurrences of barriers, was examined in apps and in classes of image-based buttons. In the app analysis, 35.9% of analyzed apps had 90% or more of their assessed image-based buttons labeled, 45.9% had less than 10% of assessed image-based buttons labeled, and the remaining apps were relatively uniformly distributed along the proportion of elements that were labeled. In the class analysis, 92.0% of Floating Action Buttons were found to have missing labels, compared to 54.7% of Image Buttons and 86.3% of Clickable Images. We discuss how these accessibility barriers are addressed in existing treatments, including accessibility development guidelines.","Anne Ross, Xiaoyi Zhang, James Fogarty, Jacob Wobbrock","accessibility, image-based buttons, large-scale analysis, mobile app",119,130
10.1145/3308561.3353787,ASSETS,2019,Turning Heads&#58; Designing Engaging Immersive Video Experiences to Support People with Intellectual Disability when Learning Everyday Living Skills, ,"As head mounted displays and 360&#176; video cameras are becoming affordable, they offer opportunities to personalise immersive learning experiences to local contexts and individuals. In this paper, we present lessons learnt from a participatory design process focused on understanding engagement and preferences of users with intellectual disability viewing 360&#176; videos. Over 4 iterations involving re-designs informed by interviews and observations of two to four participants with intellectual disability, we have established that: participants are more comfortable with using the technology if they are first introduced to a familiar scene before seeing anything new, they prefer to be 'accompanied' by an in-video facilitator, and participants engage more with the immersive visual environment when prompted to look around from within the video. We have also established a number of guidelines for filming with a 360&#176; camera with regards to movement and viewpoint.","Laurianne Sitbon, Ross Brown, Lauren Fell","360 video, intellectual disability, lifeskills training",171,182
10.1145/3308561.3353771,ASSETS,2019,CaBot&#58; Designing and Evaluating an Autonomous Navigation Robot for Blind People, ,"Navigation robots have the potential to overcome some of the limitations of traditional navigation aids for blind people, specially in unfamiliar environments. In this paper, we present the design of CaBot (Carry-on roBot), an autonomous suitcase-shaped navigation robot that is able to guide blind users to a destination while avoiding obstacles on their path. We conducted a user study where ten blind users evaluated specific functionalities of CaBot, such as a vibro-tactile handle to convey directional feedback; experimented to find their comfortable walking speed; and performed navigation tasks to provide feedback about their overall experience. We found that CaBot's performance highly exceeded users' expectations, who often compared it to navigating with a guide dog or sighted guide. Users' high confidence, sense of safety, and trust on CaBot poses autonomous navigation robots as a promising solution to increase the mobility and independence of blind people, in particular in unfamiliar environments.","Jo&#227;o Guerreiro, Daisuke Sato, Saki Asakawa, Huixu Dong, Kris Kitani, Chieko Asakawa","blind navigation, guide robot, human-robot interaction, mobility, obstacle avoidance",68,82
10.1145/2513383.2513435,ASSETS,2013,Bypassing lists,accelerating screen-reader fact-finding with guided tours,"Navigating back and forth from a list of links (index) to its target pages is common on the web, but tethers screen-reader users to unnecessary cognitive and mechanical steps. This problem worsens when indexes lack information scent: cues that enable users to select a link with confidence during fact-finding. This paper investigates how blind users who navigate the web with screen-readers can bypass a scentless index with <i>guided tours</i>: a much simpler browsing pattern that linearly concatenates items of a collection. In a controlled study (N=11) at the Indiana School for the Blind and Visually Impaired (ISBVI), guided tours lowered user's cognitive effort and significantly decreased time-on-task and number of pages visited when compared to an index with poor information scent. Our findings suggest that designers can supplement indexes with guided tours to benefit screen-reader users in a variety of web navigation contexts.","Tao Yang, Prathik Gadde, Robert Morse, Davide Bolchini","blind users, fact-finding, guided tour, index, information scent, screen-reader users, web navigation",1,8
10.1145/3132525.3132553,ASSETS,2017,Digital Strategies for Supporting Strengths- and Interests-based Learning with Children with Autism, ,"Technologies to support children with autism tend to use predefined content to enhance specific skills, such as verbal communication or emotion recognition. Few mobilise the child's own (often very specific) interests, strengths and capabilities. Digital technologies offer opportunities for children to personalise learning with their own content, following their own interests and enabling their self-expression. This project sought to engage children to record and express their own interests within their contexts of support - the home and the classroom. The vehicle for self-expression was an audio-visual calendaring app called MeCalendar. Implementation was kept open-ended to allow teachers to use it in ways that best fit with their existing embedded practices. In this paper we report on how the prototype has been appropriated in two classrooms by teachers in an autism-specific school setting with children aged 6 to 7. Our contribution is an understanding of how technologies for self-expression led to enhanced verbal communication, positive reinforcement through video modelling, engagement in class tasks and enhanced social interaction. Children appropriated the design in unimagined ways, leading them to self-scaffold and to catalyse their confidence in social interaction and self-expression. Teachers played an integral role in appropriating the design in the classroom, specifically through their in-depth knowledge of each child and their individual needs, strengths and interests.","Cara Wilson, Margot Brereton, Bernd Ploderer, Laurianne Sitbon, Beth Saggers","appropriation, autism, child-computer interaction, classroom",52,61
10.1145/2513383.2513432,ASSETS,2013,A web-based intelligibility evaluation of sign language video transmitted at low frame rates and bitrates, ,"Mobile sign language video conversations can become unintelligible due to high video transmission rates causing network congestion and delayed video. In an effort to understand how much sign language video quality can be sacrificed, we evaluated the perceived lower limits of intelligible sign language video transmitted at four low frame rates (1, 5, 10, and 15 frames per second [fps]) and four low fixed bitrates (15, 30, 60, and 120 kilobits per second [kbps]). We discovered an ""intelligibility ceiling effect,"" where increasing the frame rate above 10 fps decreased perceived intelligibility, and increasing the bitrate above 60 kbps produced diminishing returns. Additional findings suggest that relaxing the recommended international video transmission rate, 25 fps at 100 kbps or higher, would still provide intelligible content while considering network resources and bandwidth consumption. As part of this work, we developed the <i>Human Signal Intelligibility Model</i>, a new conceptual model useful for informing evaluations of video intelligibility.","Jessica Tran, Rafael Rodriguez, Eve Riskin, Jacob Wobbrock","American Sign Language, bitrate, communication model, comprehension, deaf community, frame rate, intelligibility, video compression, web-survey",1,8
10.1145/2513383.2517030,ASSETS,2013,IncluCity,using contextual cues to raise awareness on environmental accessibility,"Awareness campaigns aiming to highlight the accessibility challenges affecting people with disabilities face an important challenge. They often describe the environmental features that pose accessibility barriers out of context, and as a result public cannot relate to the problems at hand. In this paper we demonstrate that contextual cues can enhance people's perception and understanding of accessibility. We describe a two-week study where our participants submitted reports of inaccessible spots all over the city through a web application. Using a 2x2 factorial design we contrast the impact of two types of contextual cues, visual cues (i.e., displaying a picture of the inaccessible spot) and location cues (i.e., ability to zoom-in the exact location). We measure participants' perceptions of accessibility and how they are challenged to consider their own limitations and barriers that may also affect themselves in certain circumstances. Our results suggest that visual cues led to a bigger sense of urgency while also improving participants' attitude towards disability.","Jorge Goncalves, Vassilis Kostakos, Simo Hosio, Evangelos Karapanos, Olga Lyra","accessibility, civic engagement, contextual cues, disability, inclusion",1,8
10.1145/2513383.2513451,ASSETS,2013,What health topics older adults want to track,a participatory design study,"Older adults are increasingly savvy consumers of smartphone-based health solutions and information. These technologies may enable older adults to age-in-place more successfully. However, many app creators fail to do needs assessments of their end-users. To rectify this issue, we involved older adults (aged 65+) in the beginning stages of designing a mobile health and wellness application. We conducted a participatory design study, where 5 groups of older adults created 5 designs. Four groups identified at least 1 health metric not currently offered in either the iPhone app store or the Google Play store. At the end of the sessions we administered a questionnaire to determine what health topics participants would like to track via smartphone or tablet. The designs included 13 health topics that were not on the questionnaire. Seventeen of eighteen participants expressed interest in tracking health metrics using a smartphone/tablet despite having little experience with these devices. This shows that older adults have unique ideas that are not being considered by current technology designers. We conclude with recommendations for future development, and propose continuing to involve to older adults in participatory design.","Jennifer Davidson, Carlos Jensen","health, mHealth, older adults, participatory design, smartphone applications",1,8
10.1145/2513383.2513436,ASSETS,2013,Wheelchair-based game design for older adults, ,"Few leisure activities are accessible to institutionalized older adults using wheelchairs; in consequence, they experience lower levels of perceived health than able-bodied peers. Video games have been shown to be an engaging leisure activity for older adults. In our work, we address the design of wheelchair-accessible motion-based games. We present KINECT<sup>Wheels</sup>, a toolkit designed to integrate wheelchair movements into motion-based games, and Cupcake Heaven, a wheelchair-based video game designed for older adults using wheelchairs. Results of two studies show that KINECT<sup>Wheels</sup> can be applied to make motion-based games wheelchair-accessible, and that wheelchair-based games engage older adults. Through the application of the wheelchair as an enabling technology in play, our work has the potential of encouraging older adults to develop a positive relationship with their wheelchair.","Kathrin Gerling, Regan Mandryk, Michael Kalyn","accessibility, entertainment, games, older adults, wheelchairs",1,8
10.1145/3132525.3132540,ASSETS,2017,Design and Psychometric Evaluation of an American Sign Language Translation of the System Usability Scale, ,"In usability studies, designers and researchers frequently use subjective questions to evaluate participants' impression of the usability of some product. The System Usability Scale (SUS) is a popular standardized questionnaire consisting of ten English statements about the usability of a product, to which participants indicate their agreement on a five-point scale. Many deaf adults in the U.S. have lower levels of English reading literacy, but there are currently no standardized questionnaires similar to SUS for Deaf and Hard-of-Hearing (DHH) users who are fluent in American Sign Language (ASL). To facilitate the inclusion of such users in studies, we created an ASL translation of SUS following accepted methods of survey translation: using a bilingual team including native ASL signers who are members of the Deaf community, along with back-translation evaluation to determine whether the meaning of the original was preserved. To validate whether key psychometric properties were preserved during translation, we deployed the ASL instrument in a study with 30 DHH participants. By comparing the results to users? responses to another measurement instrument, along with scores from 10 additional DHH participants responding to the original English SUS, we verified the criterion validity and internal reliability of the new ""ASL-SUS."" We are disseminating the translated instrument to promote the inclusion of DHH users in HCI research studies or in usability testing of consumer products.","Matt Huenerfauth, Kasmira Patel, Larwan Berke","american sign language, asl-sus, criterion validity, internal reliability, sus, system usability scale, translation",175,184
10.1145/3308561.3353792,ASSETS,2019,Making Memes Accessible, ,"Images on social media platforms are inaccessible to people with vision impairments due to a lack of descriptions that can be read by screen readers. Providing accurate alternative text for all visual content on social media is not yet feasible, but certain subsets of images, such as internet memes, offer affordances for automatic or semi-automatic generation of alternative text. We present two methods for making memes accessible semi-automatically through (1) the generation of rich alternative text descriptions and (2) the creation of audio macro memes. Meme authors create alternative text templates or audio meme templates, and insert placeholders instead of the meme text. When a meme with the same image is encountered again, it is automatically recognized from a database of meme templates. Text is then extracted and either inserted into the alternative text template or rendered in the audio template using text-to-speech. In our evaluation of meme formats with 10 Twitter users with vision impairments, we found that most users preferred alternative text memes because the description of the visual content conveys the emotional tone of the character. As the preexisting templates can be automatically matched to memes using the same visual image, this combined approach can make a large subset of images on the web accessible, while preserving the emotion and tone inherent in the image memes.","Cole Gleason, Amy Pavel, Xingyu Liu, Patrick Carrington, Lydia Chilton, Jeffrey Bigham","alternative text, audio, blind, image description, low vision, meme, social media",367,376
10.1145/3308561.3353788,ASSETS,2019,Leveraging Augmented Reality to Create Apps for People with Visual Disabilities,A Case Study in Indoor Navigation,"The introduction of augmented reality technology to iOS and Android enables, for the first time, mainstream smartphones to estimate their own motion in 3D space with high accuracy. For assistive technology researchers, this development presents a potential opportunity. In this spirit, we present our work leveraging these technologies to create a smartphone app to empower people who are visually impaired to more easily navigate indoor environments. Our app, Clew, allows users to record routes and then load them, at any time, providing automatic guidance (using haptic, speech, and sound feedback) along the route. We present our user-centered design process, Clew's system architecture and technical details, and both small and large-scale evaluations of the app. We discuss opportunities, pitfalls, and design guidelines for utilizing augmented reality for orientation and mobility apps. Our work expands the capabilities of technology for orientation and mobility that can be distributed on a mass scale.","Chris Yoon, Ryan Louie, Jeremy Ryan, MinhKhang Vu, Hyegi Bang, William Derksen, Paul Ruvolo","assistive tech, augmented reality, orientation and mobility",210,221
10.1145/3234695.3236357,ASSETS,2018,Multimodal Deep Learning using Images and Text for Information Graphic Classification, ,"Information graphics, e.g. line or bar graphs, are often displayed in documents and popular media to support an intended message, but for a growing number of people, they are missing the point. The World Health Organization estimates that the number of people with vision impairment could triple in the next thirty years due to population growth and aging. If a graphic is not described, explained in the text, or missing alt tags and other metadata (as is often the case in popular media), the intended message is lost or not adequately conveyed. In this work, we describe a multimodal deep learning approach that supports the communication of the intended message. The multimodal model uses both the pixel data and text data in a single neural network to classify the information graphic into an intention category that has previously been validated as useful for people who are blind or who are visually impaired. Furthermore, we collect a new dataset of information graphics and present qualitative and quantitative results that show our multimodal model exceeds the performance of any one modality alone, and even surpasses the capabilities of the average human annotator.","Edward Kim, Kathleen McCoy","assistive technology, classification, deep learning, information graphic, multimodal machine learning",143,148
10.1145/2049536.2049553,ASSETS,2011,ACES,"aphasia emulation, realism, and the turing test","To an outsider it may appear as though an individual with aphasia has poor cognitive function. However, the problem resides in the individual's receptive and expressive language, and not in their ability to think. This misperception, paired with a lack of empathy, can have a direct impact on quality of life and medical care. Hailpern's 2011 paper on ACES demonstrated a novel system that enabled users (e.g., caregivers, therapists, family) to experience first hand the communication-distorting effects of aphasia. While their paper illustrated the impact of ACES on empathy, it did not validate the underlying distortion emulation. This paper provides a validation of ACES' distortions through a Turing Test experiment with participants from the Speech and Hearing Science community. It illustrates that text samples generated with ACES distortions are generally not distinguishable from text samples originating from individuals with aphasia. This paper explores ACES distortions through a `How Human' is it test, in which participants explicitly rate how human- or computer-like distortions appear to be.","Joshua Hailpern, Marina Danilevsky, Karrie Karahalios","aphasia, assistive technology, disabilities, empathy, emulation software, language, speech, turing test",83,90
10.1145/2513383.2513443,ASSETS,2013,Real time object scanning using a mobile phone and cloud-based visual search engine, ,"Computer vision and human-powered services can provide blind people access to visual information in the world around them, but their efficacy is dependent on high-quality photo inputs. Blind people often have difficulty capturing the information necessary for these applications to work because they cannot see what they are taking a picture of. In this paper, we present <i>Scan Search</i>, a mobile application that offers a new way for blind people to take high-quality photos to support recognition tasks. To support realtime scanning of objects, we developed a key frame extraction algorithm that automatically retrieves high-quality frames from continuous camera video stream of mobile phones. Those key frames are streamed to a cloud-based recognition engine that identifies the most significant object inside the picture. This way, blind users can scan for objects of interest and hear potential results in real time. We also present a study exploring the tradeoffs in how many photos are sent, and conduct a user study with 8 blind participants that compares <i>Scan Search</i> with a standard photo-snapping interface. Our results show that <i>Scan Search</i> allows users to capture objects of interest more efficiently and is preferred by users to the standard interface.","Yu Zhong, Pierre Garrigues, Jeffrey Bigham","accessibility, blind user, mobile, real time object scanning",1,8
10.1145/2513383.2513450,ASSETS,2013,Architecture of an automated therapy tool for childhood apraxia of speech, ,"We present a multi-tier system for the remote administration of speech therapy to children with apraxia of speech. The system uses a client-server architecture model and facilitates task-oriented remote therapeutic training in both in-home and clinical settings. Namely, the system allows a speech therapist to remotely assign speech production exercises to each child through a web interface, and the child to practice these exercises on a mobile device. The mobile app records the child's utterances and streams them to a back-end server for automated scoring by a speech-analysis engine. The therapist can then review the individual recordings and the automated scores through a web interface, provide feedback to the child, and adapt the training program as needed. We validated the system through a pilot study with children diagnosed with apraxia of speech, and their parents and speech therapists. Here we describe the overall client-server architecture, middleware tools used to build the system, the speech-analysis tools for automatic scoring of recorded utterances, and results from the pilot study. Our results support the feasibility of the system as a complement to traditional face-to-face therapy through the use of mobile tools and automated speech analysis algorithms.","Avinash Parnandi, Virendra Karappa, Youngpyo Son, Mostafa Shahin, Jacqueline McKechnie, Kirrie Ballard, Beena Ahmed, Ricardo Gutierrez-Osuna","automated speech analysis, childhood apraxia of speech, speech therapy",1,8
10.1145/2982142.2982176,ASSETS,2016,Gesture-Based Interactive Audio Guide on Tactile Reliefs, ,"For blind and visually impaired people, tactile reliefs offer many benefits over the more classic raised line drawings or tactile diagrams, as depth, 3D shape and surface textures are directly perceivable. However, without proper guidance some reliefs are still difficult to explore autonomously. In this work, we present a gesture-controlled interactive audio guide (IAG) based on recent low-cost depth cameras that operates directly on relief surfaces. The interactively explorable, location-dependent verbal descriptions promise rapid tactile accessibility to 2.5D spatial information in a home or education setting, to on-line resources, or as a kiosk installation at public places. We present a working prototype, discuss design decisions and present the results of two evaluation sessions with a total of 20 visually impaired test users.","Andreas Reichinger, Anton Fuhrmann, Stefan Maierhofer, Werner Purgathofer","blind users, evaluation, finger tracking, gesture detection, interactive audio guide, tactile reliefs",91,100
10.1145/3308561.3353777,ASSETS,2019,Automatic Generation and Evaluation of Usable and Secure Audio reCAPTCHA, ,"CAPTCHAs are challenge-response tests to differentiate humans from automated agents, with tasks that are easy for humans but difficult for computers. The most common CAPTCHAs require humans to decipher characters from an image and are unsuitable for visually impaired people. As an alternative, audio CAPTCHA was proposed, which require deciphering spoken digits/letters. However, current audio CAPTCHAs suffer from low usability and are insecure against Automatic Speech Recognition (ASR) attacks. In this work, we propose reCAPGen, a system that uses ASR for generating secure CAPTCHAs. We evaluated four audio CAPTCHA schemes with 60 sighted and 19 visually impaired participants. We found that our proposed Last Two Words scheme was the most usable with success rate of &#62;78.2% and low response time of &#60;14.5s. Furthermore, solving our audio CAPTCHAs can transcribe unknown words with &#62;82% accuracy.","Mohit Jain, Rohun Tripathi, Ishita Bhansali, Pratyush Kumar","blind, captcha, evaluation, mturk, visually impaired",355,366
10.1145/2700648.2809852,ASSETS,2015,Accessible Texts for Autism,An Eye-Tracking Study,"People with Autism Spectrum Disorder (ASD) are known to experience difficulties in reading comprehension, as well as to have unusual attention patterns, which makes the development of user-centred tools for this population a challenging task. This paper presents the first study to use eye-tracking technology with ASD participants in order to evaluate text documents. Its aim is two-fold. First, it evaluates the use of images in texts and provides evidence of a significant difference in the attention patterns of participants with and without autism. Sets of two types of images, photographs and symbols, are compared to establish which ones are more useful to include in simple documents. Second, the study evaluates human-produced easy-read documents, as a gold standard for accessible documents, on 20 adults with autism. The results provide an understanding of the perceived level of difficulty of easy-read documents according to this population, as well as the preferences of autistic individuals in text presentation. The results are synthesized as set of guidelines for creating accessible text for autism.","Victoria Yaneva, Irina Temnikova, Ruslan Mitkov","autism, easy-read, eye tracking, readability, reading comprehension, text simplification",49,57
10.1145/3308561.3353790,ASSETS,2019,3D Printed Maps and Icons for Inclusion,Testing in the Wild by People who are Blind or have Low Vision,"The difficulty and consequent fear of travel is one of the most disabling consequences of blindness and severe vision impairment, affecting confidence and quality of life. Traditional tactile graphics are vital in the Orientation and Mobility training process, however 3D printing may have the capacity to enable production of more meaningful and inclusive maps. This study explored the use of 3D printed maps on site at a public event to examine their suitability and to identify guidelines for the design of future 3D maps. An iterative design process was used in the production of the 3D maps, with feedback from visitors who are blind or have low vision informing the recommendations for their design and use. For example, it was found that many representational 3D icons could be recognised by touch without the need for a key and that such a map helped form mental models of the event space. Complex maps, however, require time to explore and should be made available before an event or at the entrance in a comfortable position. The maps were found to support the orientation and mobility process, and importantly to also promote a positive message about inclusion and accessibility.","Leona Holloway, Kim Marriott, Matthew Butler, Samuel Reinders","3d printing, blind, low vision, maps, orientation &#38; mobility, vision impairment",183,195
10.1145/2049536.2049560,ASSETS,2011,Annotation-based video enrichment for blind people,a pilot study on the use of earcons and speech synthesis,Our approach to address the question of online video accessibility for people with sensory disabilities is based on video annotations that are rendered as video enrichments during the playing of the video. We present an exploratory work that focuses on video accessibility for blind people with audio enrichments composed of speech synthesis and earcons (i.e. nonverbal audio messages). Our main results are that earcons can be used together with speech synthesis to enhance understanding of videos; that earcons should be accompanied with explanations; and that a potential side effect of earcons is related to video rhythm perception.,"Beno&#238;t Encelle, Magali Ollagnier-Beldame, St&#233;phanie Pouchot, Yannick Pri&#233;","accessibility for blind people, audio notification, video accessibility, video annotation, video enrichment",123,130
10.1145/3308561.3353780,ASSETS,2019,Perception and Adoption of Mobile Accessibility Features by Older Adults Experiencing Ability Changes, ,"To investigate how older adults perceive ability changes (e.g., sensory, physical, cognitive) and how attitudes toward those changes affect perception and adoption of built-in mobile accessibility features (such as those found on Apple iOS and Google Android smartphones and tablets), we conducted an interview study with 14 older adults and six of their family members. Accessibility features were difficult for participants to find and configure, which were issues compounded by a reluctance to use trial-and-error. At 4-6 weeks after the interview, however, some participants had adopted new accessibility features that we had showed them, suggesting a willingness to adopt once features are made visible. The older adults who did already use accessibility features had experienced a disability earlier in life, suggesting that those experiencing progressive ability changes later in life might not be as aware of accessibility features, or might not have the know-how to adapt technologies to their changing needs. Our findings provide support for creating technologies that can detect older adults' abilities and recommend or enact interface changes to match.","Rachel Franz, Jacob Wobbrock, Yi Cheng, Leah Findlater","ability-based design, accessibility, changing abilities, interviews, mobile devices, older adults, smartphones",267,278
10.1145/2661334.2661373,ASSETS,2014,A computer-based method to improve the spelling of children with dyslexia, ,"In this paper we present a method which aims to improve the spelling of children with dyslexia through playful and targeted exercises. In contrast to previous approaches, our method does not use correct words or positive examples to follow, but presents the child a misspelled word as an exercise to solve. We created these training exercises on the basis of the linguistic knowledge extracted from the errors found in texts written by children with dyslexia. To test the effectiveness of this method in Spanish, we integrated the exercises in a game for iPad, DysEggxia (Piruletras in Spanish), and carried out a within-subject experiment. During eight weeks, 48 children played either DysEggxia or Word Search, which is another word game. We conducted tests and questionnaires at the beginning of the study, after four weeks when the games were switched, and at the end of the study. The children who played DysEggxia for four weeks in a row had significantly less writing errors in the tests that after playing Word Search for the same time. This provides evidence that error-based exercises presented in a tablet help children with dyslexia improve their spelling skills.","Luz Rello, Clara Bayarri, Yolanda Otal, Martin Pielot","dyslexia, literacy, serious game, spelling, written errors",153,160
10.1145/1878803.1878817,ASSETS,2010,Understanding the challenges and opportunities for richer descriptions of stereotypical behaviors of children with asd,a concept exploration and validation,"Individuals with Autism Spectrum Disorder (ASD) often engage in stereotypical behaviors. In some individuals these behaviors occur with very high frequency and can be disruptive and at times self-injurious. We propose a system that can tacitly collect contextual data related to the individual's physiological state and their external environment, and map it to occurrences of stereotypies. A user study was conducted with children with ASD, parents, and caregivers to explore and validate this concept. A prototype of the system, developed through participatory design, was used in the study as a probe to elicit the information needs of these stakeholders, and provide a better understanding of the nuances involved in supporting those needs. Here we present the findings of this study, and four design recommendations; promoting ecological integration, addressing privacy concerns, supporting inference, and enabling customization.","Fnu Nazneen, Fatima Boujarwah, Shone Sadler, Amha Mogus, Gregory Abowd, Rosa Arriaga","autism spectrum disorder, stereotypical repetitive behavior",67,74
10.1145/2384916.2384928,ASSETS,2012,Detecting linguistic HCI markers in an online aphasia support group, ,"Aphasia is an acquired language disorder resulting from trauma or injury to language areas of the brain. Despite extensive research on the impact of aphasia on traditional forms of communication, little is known about the impact of aphasia on computer-mediated communication (CMC). In this study we asked whether the well-documented language deficits associated with aphasia can be detected in online writing of people with aphasia. We analyzed 150 messages (14,754 words) posted to an online aphasia support forum, by six people with aphasia and by four controls. Significant linguistic differences between people with aphasia and controls were detected, suggesting five putative linguistic HCI markers for aphasia. These findings suggest that interdisciplinary research on communication disorders and CMC has both applied and theoretical implications.","Yoram Kalman, Kathleen Geraghty, Cynthia Thompson, Darren Gergle","aphasia, computer-mediated communication, hci markers, human factors, online support groups, unobtrusive monitoring, user modeling",65,70
10.1145/2384916.2384934,ASSETS,2012,Helping visually impaired users properly aim a camera, ,"We evaluate three interaction modes to assist visually impaired users during the camera aiming process: speech, tone, and silent feedback. Our main assumption is that users are able to spatially localize what they want to photograph, and roughly aim the camera in the appropriate direction. Thus, small camera motions are sufficient for obtaining a good composition. Results in the context of documenting accessibility barriers related to public transportation show that audio feedback is valuable. Visually impaired users were not affected by audio feedback in terms of social comfort. Furthermore, we observed trends in favor of speech over tone, including higher ratings for ease of use. This study reinforces earlier work that suggests users who are blind or low vision find assisted photography appealing and useful.","Marynel V&#225;zquez, Aaron Steinfeld","accessibility, photography, transit, visually impaired",95,102
10.1145/3132525.3132548,ASSETS,2017,FluxMarker,Enhancing Tactile Graphics with Dynamic Tactile Markers,"For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.","Ryo Suzuki, Abigale Stangl, Mark Gross, Tom Yeh","dynamic tactile markers, interactive tactile graphics, tangible interfaces, visual impairment",190,199
10.1145/2982142.2982171,ASSETS,2016,A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users, ,"Sounds provide informative signals about the world around us. In situations where non-auditory cues are inaccessible, it can be useful for deaf and hard-of-hearing people to be notified about sounds. Through a survey, we explored which sounds are of interest to deaf and hard-of-hearing people, and which means of notification are appropriate. Motivated by these findings, we designed a mobile phone app that alerts deaf and hard-of-hearing people to sounds they care about. The app uses training examples of personally relevant sounds recorded by the user to learn a model of those sounds. It then screens the incoming audio stream from the phone's microphone for those sounds. When it detects a sound, it alerts the user by vibrating and providing a pop-up notification. To evaluate the interface design independent of sound detection errors, we ran a Wizard-of-Oz user study, and found that the app design successfully facilitated deaf and hard-of-hearing users recording training examples. We also explored the viability of a basic machine learning algorithm for sound detection.","Danielle Bragg, Nicholas Huynh, Richard Ladner","accessibility, deaf, hard-of-hearing, sound detection",3,13
10.1145/3234695.3236342,ASSETS,2018,Understanding Authentication Method Use on Mobile Devices by People with Vision Impairment, ,"Passwords help people avoid unauthorized access to their personal devices but are not without challenges, like memorability and shoulder surfing attacks. Little is known about how people with vision impairment assure their digital security in mobile contexts. We conducted an online survey to understand their strategies to remember passwords, their perceptions of authentication methods and their self-assessed ability to keep their digital information safe. We collected answers from 325 people who are blind or have low vision from 12 countries and found: most use familiar names and numbers to create memorable passwords, the majority consider fingerprint to be the most secure and accessible user authentication method and PINs the least secure user authentication method. This paper presents our survey results and provides insights for designing better authentication methods for people with vision impairment.","Daniella Briotto Faustino, Audrey Girouard","blind, low vision, mobile devices, password, smartphones, user authentication methods, vision impaired",217,228
10.1145/3308561.3353795,ASSETS,2019,Evaluating Instructor Strategy and Student Learning Through Digital Accessibility Course Enhancements, ,"University students graduating and entering into technology design and development fields are underprepared to support digital accessibility due to a lack of awareness and training. Teach Access is a consortium of 10 industry partners, 5 advocacy groups, and 20 university partners working to address this issue. In an attempt to bridge the gap between what is taught to students and the increasing demand from industry, the initiative described here was aimed at awarding instructor grants to support the development of accessibility modules in tech-related courses. In our study we surveyed student attitudes toward accessibility pre- and post-instruction of these modules, as well as, instructor strategy. We found that across all courses, student confidence in accessibility-related concepts increased. The largest increases were found in student confidence in defining the Americans with Disabilities Act (ADA) and the Web Content Accessibility Guidelines (WCAG). Our work makes the following contributions: 1) A detailed description of how accessibility was integrated into 18 different university and college courses 2) Instructional delivery methods found to be effective by participating instructors 3) Insights for resource materials development.","Claire Kearney-Volpe, Devorah Kletenik, Kate Sonka, Deborah Sturm, Amy Hurst","accessibility, higher education",377,388
10.1145/3234695.3236350,ASSETS,2018,Why Is Gesture Typing Promising for Older Adults?,Comparing Gesture and Tap Typing Behavior of Older with Young Adults,"Gesture typing has been a widely adopted text entry method on touchscreen devices. We have conducted a study to understand whether older adults could gesture type, how they type, what are the strengths and weaknesses of gesture typing, and how to further improve it. By logging stroke-level interaction data and leveraging the existing modeling tools, we compared the gesture and tap typing behavior of older adults with young adults. Our major finding is promising and encouraging. Gesture typing outperformed the typical tap typing for older adults, and was very easy for them to learn. The gesture typing input speed was 15.28% higher than that of tap typing for 14 older adults who had none gesture typing experience in the past. One of the main reasons was that older adults adopted the word-level inputting strategy in gesture typing, while often used the letter-level correction strategy in tap typing. Compared with young adults, older adults exhibited little degradation in gesture accuracy. Our study also led to implications on how to further improve gesture typing for older adults.","Yu-Hao Lin, Suwen Zhu, Yu-Jung Ko, Wenzhe Cui, Xiaojun Bi","gesture typing, older adults, text entry",271,281
10.1145/2384916.2384945,ASSETS,2012,PassChords,secure multi-touch authentication for blind people,"Blind mobile device users face security risks such as inaccessible authentication methods, and aural and visual eavesdropping. We interviewed 13 blind smartphone users and found that most participants were unaware of or not concerned about potential security threats. Not a single participant used optional authentication methods such as a password-protected screen lock. We addressed the high risk of unauthorized user access by developing <i>PassChords</i>, a non-visual authentication method for touch surfaces that is robust to aural and visual eavesdropping. A user enters a PassChord by tapping several times on a touch surface with one or more fingers. The set of fingers used in each tap defines the password. We give preliminary evidence that a four-tap PassChord has about the same entropy, a measure of password strength, as a four-digit personal identification number (PIN) used in the iPhone's Passcode Lock. We conducted a study with 16 blind participants that showed that PassChords were nearly three times as fast as iPhone's Passcode Lock with VoiceOver, suggesting that PassChords are a viable accessible authentication method for touch screens.","Shiri Azenkot, Kyle Rector, Richard Ladner, Jacob Wobbrock","blind, mobile devices, privacy, security, touch screens",159,166